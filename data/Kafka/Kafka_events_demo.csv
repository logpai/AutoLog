Log,LoggingLevel,Method
Successfully produced messages to {} with base offset {}.,trace,"<org.apache.kafka.clients.producer.internals.ProducerBatch: boolean done(long,long,java.lang.RuntimeException,java.util.function.Function)>"
Failed to produce messages to {} with base offset {}.,trace,"<org.apache.kafka.clients.producer.internals.ProducerBatch: boolean done(long,long,java.lang.RuntimeException,java.util.function.Function)>"
ProduceResponse returned {} for {} after batch with base offset {} had already been {}.,debug,"<org.apache.kafka.clients.producer.internals.ProducerBatch: boolean done(long,long,java.lang.RuntimeException,java.util.function.Function)>"
Ignored state transition {} -> {} for {} batch with base offset {},debug,"<org.apache.kafka.clients.producer.internals.ProducerBatch: boolean done(long,long,java.lang.RuntimeException,java.util.function.Function)>"
Executing onJoinComplete with generation {} and memberId {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void onJoinComplete(int,java.lang.String,java.lang.String,java.nio.ByteBuffer)>"
Updating assignment with\n\tAssigned partitions:                       {}\n\tCurrent owned partitions:                  {}\n\tAdded partitions (assigned - owned):       {}\n\tRevoked partitions (owned - assigned):     {}\n,info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void onJoinComplete(int,java.lang.String,java.lang.String,java.nio.ByteBuffer)>"
LOG_APPEND_TIME,<init>,<org.apache.kafka.common.record.TimestampType: void <clinit>()>
getClaim - {}: {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.ValidatorAccessTokenValidator: java.lang.Object getClaim(org.apache.kafka.common.security.oauthbearer.secured.ValidatorAccessTokenValidator$ClaimSupplier,java.lang.String)>"
Joining group with current subscription: {},debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: org.apache.kafka.common.message.JoinGroupRequestData$JoinGroupRequestProtocolCollection metadata()>
Received TxnOffsetCommit response for consumer group {}: {},debug,<org.apache.kafka.clients.producer.internals.TransactionManager$TxnOffsetCommitHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Skipping fetching records for assigned partition {} because it is paused,debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch collectFetch()>
Sending FindCoordinator request to broker {},debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendFindCoordinatorRequest(org.apache.kafka.common.Node)>
Metrics scheduler closed,info,<org.apache.kafka.common.metrics.Metrics: void close()>
Closing reporter {},info,<org.apache.kafka.common.metrics.Metrics: void close()>
"Error when closing , <*>, ",error,<org.apache.kafka.common.metrics.Metrics: void close()>
Metrics reporters closed,info,<org.apache.kafka.common.metrics.Metrics: void close()>
"{} partitions have leader brokers without a matching listener, including {}",warn,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: void handleSuccessfulResponse(org.apache.kafka.common.requests.RequestHeader,long,org.apache.kafka.common.requests.MetadataResponse)>"
Error while fetching metadata with correlation id {} : {},warn,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: void handleSuccessfulResponse(org.apache.kafka.common.requests.RequestHeader,long,org.apache.kafka.common.requests.MetadataResponse)>"
Ignoring empty metadata response with correlation id {}.,trace,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: void handleSuccessfulResponse(org.apache.kafka.common.requests.RequestHeader,long,org.apache.kafka.common.requests.MetadataResponse)>"
Found expiring credential with principal \'{}\'.,debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerRefreshingLogin$1: org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredential expiringCredential()>
close started,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void close()>
close completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void close()>
close completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void close()>
"Coordinator discovery failed, refreshing metadata",debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: boolean ensureCoordinatorReady(org.apache.kafka.common.utils.Timer)>
FindCoordinator request hit fatal exception,info,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: boolean ensureCoordinatorReady(org.apache.kafka.common.utils.Timer)>
\'logDir\' field is too long to be serialized,<init>,"<org.apache.kafka.common.message.DescribeLogDirsResponseData$DescribeLogDirsResult: void addSize(org.apache.kafka.common.protocol.MessageSizeAccumulator,org.apache.kafka.common.protocol.ObjectSerializationCache,short)>"
parseAccessToken - responseBody: {},debug,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String parseAccessToken(java.lang.String)>
Partition {} keeps lastOffset at {},trace,"<org.apache.kafka.clients.producer.internals.TransactionManager: void updateLastAckedOffset(org.apache.kafka.common.requests.ProduceResponse$PartitionResponse,org.apache.kafka.clients.producer.internals.ProducerBatch)>"
Successfully added partition for consumer group {} to transaction,debug,<org.apache.kafka.clients.producer.internals.TransactionManager$AddOffsetsToTxnHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Broker no longer low on memory - unmuting incoming sockets,trace,<org.apache.kafka.common.network.Selector: void poll(long)>
{}: entering performPendingMetricsOperations,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void performPendingMetricsOperations()>
{}: removing metric {},trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void performPendingMetricsOperations()>
{}: adding metric {},trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void performPendingMetricsOperations()>
{}: leaving performPendingMetricsOperations,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void performPendingMetricsOperations()>
Polling for fetches with timeout {},trace,<org.apache.kafka.clients.consumer.KafkaConsumer: org.apache.kafka.clients.consumer.internals.Fetch pollForFetches(org.apache.kafka.common.utils.Timer)>
Committing offsets: {},debug,"<org.apache.kafka.clients.consumer.KafkaConsumer: void commitAsync(java.util.Map,org.apache.kafka.clients.consumer.OffsetCommitCallback)>"
message,warn,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void warn(java.lang.String,java.lang.Object)>"
"Unsupported extensions will be ignored, supported {}, provided {}",debug,<org.apache.kafka.common.security.scram.internals.ScramSaslServer: byte[] evaluateResponse(byte[])>
Notifying assignor about the new {},info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokeOnAssignment(org.apache.kafka.clients.consumer.ConsumerPartitionAssignor,org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$Assignment)>"
Updating last stable offset for partition {} to {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher$2: void onSuccess(org.apache.kafka.clients.consumer.internals.Fetcher$ListOffsetResult)>
Updating high watermark for partition {} to {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher$2: void onSuccess(org.apache.kafka.clients.consumer.internals.Fetcher$ListOffsetResult)>
init started,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void init()>
JWKS validation key refresh thread started with a refresh interval of {} ms,info,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void init()>
init completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void init()>
init completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void init()>
Client supplied realm: {} ,trace,<org.apache.kafka.common.security.authenticator.SaslServerCallbackHandler: void handleRealmCallback(javax.security.sasl.RealmCallback)>
Failed to create channel due to ,info,"<org.apache.kafka.common.network.SslChannelBuilder: org.apache.kafka.common.network.KafkaChannel buildChannel(java.lang.String,java.nio.channels.SelectionKey,int,org.apache.kafka.common.memory.MemoryPool,org.apache.kafka.common.network.ChannelMetadataRegistry)>"
Ignoring fetched records for partition {} since it no longer has valid position,debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Discarding stale fetch response for partition {} since its offset {} does not match the expected offset {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Preparing to read {} bytes of data for partition {} with offset {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Updating high watermark for partition {} to {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Updating log start offset for partition {} to {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Updating last stable offset for partition {} to {},trace,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Error in fetch for partition {}: {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Received unknown topic or partition error in fetch for partition {},warn,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Received unknown topic ID error in fetch for partition {},warn,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Received inconsistent topic ID error in fetch for partition {},warn,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Discarding stale fetch response for partition {} since the fetched offset {} does not match the current offset {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Unset the preferred read replica {} for partition {} since we got {} when fetching {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Not authorized to read from partition {}.,warn,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Received unknown leader epoch error in fetch for partition {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Unknown server error while fetching offset {} for topic-partition {},warn,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch initializeCompletedFetch(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch)>
Error executing interceptor onAcknowledgement callback,warn,"<org.apache.kafka.clients.producer.internals.ProducerInterceptors: void onSendError(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.common.TopicPartition,java.lang.Exception)>"
Entering KafkaClient#poll(timeout={}),trace,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void processRequests()>
KafkaClient#poll retrieved {} response(s),trace,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void processRequests()>
Authentication exception while processing transactional request,trace,<org.apache.kafka.clients.producer.internals.Sender: void runOnce()>
`OffsetDelete` request for group id {} failed due to error {}.,debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetDelete` request for group id {} failed because the coordinator is still in the process of loading state. Will retry.,debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetDelete` request for group id {} returned error {}. Will attempt to find the coordinator again and retry.,debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetDelete` request for group id {} failed due to unexpected error {}.,error,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
"SecurityPropertyModification: key:{}, value:{}",fine,"<jdk.internal.event.EventHelper: void logSecurityPropertyEvent(java.lang.String,java.lang.String)>"
released buffer of size {} and identity {},trace,<org.apache.kafka.common.memory.GarbageCollectedMemoryPool: void bufferToBeReleased(java.nio.ByteBuffer)>
Connection to node {} is throttled for {} ms until timestamp {},trace,"<org.apache.kafka.clients.NetworkClient: void maybeThrottle(org.apache.kafka.common.requests.AbstractResponse,short,java.lang.String,long)>"
Still waiting for metadata,warn,"<org.apache.kafka.clients.consumer.KafkaConsumer: org.apache.kafka.clients.consumer.ConsumerRecords poll(org.apache.kafka.common.utils.Timer,boolean)>"
Returning empty records from `poll()` since the consumer\'s position has advanced for at least one topic partition,trace,"<org.apache.kafka.clients.consumer.KafkaConsumer: org.apache.kafka.clients.consumer.ConsumerRecords poll(org.apache.kafka.common.utils.Timer,boolean)>"
Caught fenced group.instance.id {} error in heartbeat thread,error,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread$1: void onFailure(java.lang.RuntimeException)>
last_contained_log_timestamp,<init>,<org.apache.kafka.common.message.SnapshotHeaderRecord: void <clinit>()>
Metadata update failed due to authentication error,warn,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void updateFailed(java.lang.Throwable)>
Metadata update failed,info,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void updateFailed(java.lang.Throwable)>
ALTER_REPLICA_LOG_DIRS,<init>,<org.apache.kafka.common.protocol.ApiKeys: void <clinit>()>
DESCRIBE_LOG_DIRS,<init>,<org.apache.kafka.common.protocol.ApiKeys: void <clinit>()>
Principal={}: Interrupted while waiting for re-login thread to shutdown.,warn,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: void close()>
"connections.max.idle.ms, ",<init>,"<org.apache.kafka.clients.producer.KafkaProducer: org.apache.kafka.clients.producer.internals.Sender newSender(org.apache.kafka.common.utils.LogContext,org.apache.kafka.clients.KafkaClient,org.apache.kafka.clients.producer.internals.ProducerMetadata)>"
handshakeWrapAfterFailure status {} doWrite {},trace,<org.apache.kafka.common.network.SslTransportLayer: boolean handshakeWrapAfterFailure(boolean)>
Failed to wrap and flush all bytes before closing channel,debug,<org.apache.kafka.common.network.SslTransportLayer: boolean handshakeWrapAfterFailure(boolean)>
Interrupted while reading the error stream,warn,<org.apache.kafka.common.utils.Shell: void runCommand()>
Error while closing the input stream,warn,<org.apache.kafka.common.utils.Shell: void runCommand()>
Error while closing the error stream,warn,<org.apache.kafka.common.utils.Shell: void runCommand()>
Error while closing the input stream,warn,<org.apache.kafka.common.utils.Shell: void runCommand()>
Error while closing the error stream,warn,<org.apache.kafka.common.utils.Shell: void runCommand()>
Metadata request for partition {} returned partition-level error {}. Will retry,debug,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map)>"
Received unexpected error for partition {} in `Metadata` response,error,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map)>"
Failed to close {} with type {},warn,"<org.apache.kafka.common.utils.Utils: void closeQuietly(java.lang.AutoCloseable,java.lang.String)>"
Node disconnected before response could be received for request {}. Will attempt retry,debug,"<org.apache.kafka.clients.admin.internals.AdminApiDriver: void onFailure(long,org.apache.kafka.clients.admin.internals.AdminApiDriver$RequestSpec,java.lang.Throwable)>"
Resuming partitions {},debug,<org.apache.kafka.clients.consumer.KafkaConsumer: void resume(java.util.Collection)>
released buffer of size {},trace,<org.apache.kafka.common.memory.SimpleMemoryPool: void bufferToBeReleased(java.nio.ByteBuffer)>
Flushing accumulated records in producer.,trace,<org.apache.kafka.clients.producer.KafkaProducer: void flush()>
Starting Kafka producer I/O thread.,debug,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Uncaught error in kafka producer I/O thread: ,error,<org.apache.kafka.clients.producer.internals.Sender: void run()>
"Beginning shutdown of Kafka producer I/O thread, sending remaining records.",debug,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Uncaught error in kafka producer I/O thread: ,error,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Aborting incomplete transaction due to shutdown,info,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Uncaught error in kafka producer I/O thread: ,error,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Aborting incomplete transactional requests due to forced shutdown,debug,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Aborting incomplete batches due to forced shutdown,debug,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Failed to close network client,error,<org.apache.kafka.clients.producer.internals.Sender: void run()>
Shutdown of Kafka producer I/O thread has completed.,debug,<org.apache.kafka.clients.producer.internals.Sender: void run()>
,<init>,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin$LoginContextFactory: javax.security.auth.login.LoginContext createLoginContext(org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin)>
,<init>,"<org.apache.kafka.clients.admin.KafkaAdminClient: org.apache.kafka.clients.admin.DescribeConsumerGroupsResult describeConsumerGroups(java.util.Collection,org.apache.kafka.clients.admin.DescribeConsumerGroupsOptions)>"
Timed out {} pending calls.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void timeoutPendingCalls(org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor)>
Client requested connection close from node {},info,<org.apache.kafka.clients.NetworkClient: void close(java.lang.String)>
topics,<init>,<org.apache.kafka.common.message.DescribeLogDirsRequestData: void <clinit>()>
topics,<init>,<org.apache.kafka.common.message.DescribeLogDirsRequestData: void <clinit>()>
A cycle of length {} was found: {},error,<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor$PartitionMovements: boolean hasCycles(java.util.Set)>
,<init>,<org.apache.kafka.clients.admin.DescribeLogDirsResult: org.apache.kafka.common.requests.DescribeLogDirsResponse$ReplicaInfo lambda$null$2(java.util.Map$Entry)>
Revoke previously assigned partitions {},info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsRevoked(java.util.SortedSet)>
The pause flag in partitions {} will be removed due to revocation.,info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsRevoked(java.util.SortedSet)>
User provided listener {} failed on invocation of onPartitionsRevoked for partitions {},error,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsRevoked(java.util.SortedSet)>
The configuration \'{}\' was supplied but isn\'t a known config.,warn,<org.apache.kafka.common.config.AbstractConfig: void logUnused()>
Topic metadata fetch included errors: {},debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map getTopicMetadata(org.apache.kafka.common.requests.MetadataRequest$Builder,org.apache.kafka.common.utils.Timer)>"
Instantiated a transactional producer.,info,"<org.apache.kafka.clients.producer.KafkaProducer: org.apache.kafka.clients.producer.internals.TransactionManager configureTransactionState(org.apache.kafka.clients.producer.ProducerConfig,org.apache.kafka.common.utils.LogContext)>"
Instantiated an idempotent producer.,info,"<org.apache.kafka.clients.producer.KafkaProducer: org.apache.kafka.clients.producer.internals.TransactionManager configureTransactionState(org.apache.kafka.clients.producer.ProducerConfig,org.apache.kafka.common.utils.LogContext)>"
Initiating logout for {},info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: void reLogin()>
"Initiating re-login for {}, logout() still needs to be called on a previous login = {}",info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: void reLogin()>
No Expiring Credential after a supposedly-successful re-login,error,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: void reLogin()>
Principal={}: It is an expiring credential after re-login as expected,debug,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: void reLogin()>
Server response mentioned unknown topic partition {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$28: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Setting SASL/{} client state to {},debug,<org.apache.kafka.common.security.scram.internals.ScramSaslClient: void setState(org.apache.kafka.common.security.scram.internals.ScramSaslClient$State)>
Not returning fetched records for partition {} since it is no longer assigned,debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Not returning fetched records for assigned partition {} since it is no longer fetchable,debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Returning {} fetched records at offset {} for assigned partition {},trace,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Updating fetch position from {} to {} for partition {} and returning {} records from `poll()`,trace,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Ignoring fetched records for {} at offset {} since the current position is {},debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Draining fetched records for partition {},trace,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.Fetch fetchRecords(org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch,int)>"
Set SASL server state to {} during {},debug,"<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: void setSaslState(org.apache.kafka.common.security.authenticator.SaslServerAuthenticator$SaslState,org.apache.kafka.common.errors.AuthenticationException)>"
Aborting incomplete transaction,info,<org.apache.kafka.clients.producer.KafkaProducer: void abortTransaction()>
Tracking closing connection {} to process outstanding requests,debug,"<org.apache.kafka.common.network.Selector: void close(org.apache.kafka.common.network.KafkaChannel,org.apache.kafka.common.network.Selector$CloseMode)>"
Record batch from {} with last offset {} exceeded max record batch size {} after cleaning (new size is {}). Consumers with version earlier than ... may need to increase their fetch sizes.,warn,"<org.apache.kafka.common.record.MemoryRecords: org.apache.kafka.common.record.MemoryRecords$FilterResult filterTo(org.apache.kafka.common.TopicPartition,java.lang.Iterable,org.apache.kafka.common.record.MemoryRecords$RecordFilter,java.nio.ByteBuffer,int,org.apache.kafka.common.utils.BufferSupplier)>"
Server response mentioned unknown topic {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$1: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Creating SaslServer for {} with mechanism {},debug,"<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: javax.security.sasl.SaslServer createSaslKerberosServer(org.apache.kafka.common.security.auth.AuthenticateCallbackHandler,java.util.Map,javax.security.auth.Subject)>"
Asynchronous auto-commit of offsets {} failed due to retriable error: {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void lambda$autoCommitOffsetsAsync$2(java.util.Map,java.lang.Exception)>"
Asynchronous auto-commit of offsets {} failed: {},warn,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void lambda$autoCommitOffsetsAsync$2(java.util.Map,java.lang.Exception)>"
Completed asynchronous auto-commit of offsets {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void lambda$autoCommitOffsetsAsync$2(java.util.Map,java.lang.Exception)>"
Refreshing JWKs from {} as no suitable verification key for JWS w/ header {} was found in {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: java.security.Key resolveKey(org.jose4j.jws.JsonWebSignature,java.util.List)>"
Principal={}: TGT refresh thread started.,info,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: No TGT found: will try again at {},warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
"The TGT cannot be renewed beyond the next expiry date: {}.This process will not be able to authenticate new SASL connections after that time (for example, it will not be able to authenticate a new connection with a Kafka Broker).  Ask your system administrator to either increase the \'renew until\' time by doing : \'modprinc -maxrenewlife {} \' within kadmin, or instead, to generate a keytab for {}. Because the TGT\'s expiry cannot be further extended by refreshing, exiting refresh thread now.",warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Refreshing now because expiry is before next scheduled refresh time.,info,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: TGT refresh thread time adjusted from {} to {} since the former is sooner than the minimum refresh interval ({} seconds) from now.,warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Next refresh: {} is later than expiry {}. This may indicate a clock skew problem.Check that this host and the KDC hosts\' clocks are in sync. Exiting refresh thread.,error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: TGT refresh sleeping until: {},info,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: TGT renewal thread has been interrupted and will exit.,warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: NextRefresh: {} is in the past: exiting refresh thread. Check clock sync between this host and KDC - (KDC\'s clock is likely ahead of this host). Manual intervention will be required for this client to successfully authenticate. Exiting refresh thread.,error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Running ticket cache refresh command: {} {},debug,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
"Principal={}: Error when trying to renew with TicketCache, but will retry ",warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
"Principal={}: Interrupted while renewing TGT, exiting Login thread",error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Could not renew TGT due to problem running shell command: \'{} {}\'. Exiting refresh thread.,warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
"Principal={}: Error when trying to re-Login, but will retry ",warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Interrupted during login retry after LoginException:,error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Could not refresh TGT.,error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Principal={}: Failed to refresh TGT: refresh thread exiting now.,error,<org.apache.kafka.common.security.kerberos.KerberosLogin: void lambda$login$0()>
Begin adding offsets {} for consumer group {} to transaction,debug,"<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionalRequestResult sendOffsetsToTransaction(java.util.Map,org.apache.kafka.clients.consumer.ConsumerGroupMetadata)>"
Beginning re-authentication: {},debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: void reauthenticate(org.apache.kafka.common.network.ReauthenticationContext)>
Setting SASL/{} client state to {},debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClient: void setState(org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClient$State)>
Resolved host {} as {},debug,"<org.apache.kafka.clients.ClientUtils: java.util.List resolve(java.lang.String,org.apache.kafka.clients.HostResolver)>"
Resetting sequence number of batch with current sequence {} for partition {} to {},info,"<org.apache.kafka.clients.producer.internals.ProducerBatch: void resetProducerState(org.apache.kafka.common.utils.ProducerIdAndEpoch,int,boolean)>"
"Process any available bytes from peer, netReadBuffer {} netWriterBuffer {} handshakeStatus {} readable? {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void handshake()>
FindCoordinator request failed due to {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler: void onFailure(java.lang.RuntimeException,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
The AdminClient is not accepting new calls. Timing out {}.,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void call(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
Received FindCoordinator response {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Group coordinator lookup failed: Invalid response containing more than a single coordinator,error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Discovered group coordinator {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Group coordinator lookup failed: {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$FindCoordinatorResponseHandler: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Received end transaction marker value version {}. Parsing as version {},debug,"<org.apache.kafka.common.record.EndTransactionMarker: org.apache.kafka.common.record.EndTransactionMarker deserializeValue(org.apache.kafka.common.record.ControlRecordType,java.nio.ByteBuffer)>"
Assigned producerId {} and producerEpoch {} to batch with base sequence {} being sent to partition {},debug,"<org.apache.kafka.clients.producer.internals.RecordAccumulator: java.util.List drainBatchesForOneNode(org.apache.kafka.common.Cluster,org.apache.kafka.common.Node,int,long)>"
still waiting to ensure active group,warn,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void ensureActiveGroup()>
{} released,trace,<org.apache.kafka.common.security.authenticator.LoginManager: void release()>
"{}: Attempted to increment {}, but the GaugeSuite was closed.",warn,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void increment(java.lang.Object)>
"{}: Attempted to increment {}, but there are already {} entries.",debug,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void increment(java.lang.Object)>
"{}: Removing the metric {}, which has a value of .",trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void increment(java.lang.Object)>
{}: Adding a new metric {}.,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void increment(java.lang.Object)>
"Registered signal handlers for , <*>, ",info,<org.apache.kafka.common.utils.LoggingSignalHandler: void register()>
message,info,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void info(java.lang.String,java.lang.Object)>"
Subscribed to pattern: \'{}\',info,"<org.apache.kafka.clients.consumer.KafkaConsumer: void subscribe(java.util.regex.Pattern,org.apache.kafka.clients.consumer.ConsumerRebalanceListener)>"
"Unexpected exception during send, closing connection {} and rethrowing exception {}",error,<org.apache.kafka.common.network.Selector: void send(org.apache.kafka.common.network.NetworkSend)>
Server response mentioned unknown topic ID {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$3: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
"<*>,  TLSHandshake: {}:{}, {}, {}, {}, ",fine,"<jdk.internal.event.EventHelper: void logTLSHandshakeEvent(java.time.Instant,java.lang.String,int,java.lang.String,java.lang.String,long)>"
Fetching committed offsets for partitions: {},debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendOffsetFetchRequest(java.util.Set)>
"Error when removing metric from , <*>, ",error,<org.apache.kafka.common.metrics.Metrics: org.apache.kafka.common.metrics.KafkaMetric removeMetric(org.apache.kafka.common.MetricName)>
Removed metric named {},trace,<org.apache.kafka.common.metrics.Metrics: org.apache.kafka.common.metrics.KafkaMetric removeMetric(org.apache.kafka.common.MetricName)>
"With the COOPERATIVE protocol, owned partitions cannot be reassigned to other members; however the assignor has reassigned partitions {} which are still owned by some members",error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void validateCooperativeAssignment(java.util.Map,java.util.Map)>"
handleOutput - responseCode: {},debug,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String handleOutput(java.net.HttpURLConnection)>
handleOutput - preparing to read response body from {},debug,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String handleOutput(java.net.HttpURLConnection)>
handleOutput - error retrieving data,warn,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String handleOutput(java.net.HttpURLConnection)>
"handleOutput - responseCode: {}, response: {}",debug,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String handleOutput(java.net.HttpURLConnection)>
"handleOutput - error response code: {}, error response body: {}",warn,<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: java.lang.String handleOutput(java.net.HttpURLConnection)>
Request joining group due to: {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void requestRejoin(java.lang.String,java.lang.String)>"
msg,trace,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void trace(java.lang.String,java.lang.Throwable)>"
message,trace,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void trace(java.lang.String,java.lang.Object,java.lang.Object)>"
{}: created new gauge suite with maxEntries = {}.,trace,"<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void <init>(org.slf4j.Logger,java.lang.String,org.apache.kafka.common.metrics.Metrics,java.util.function.Function,int)>"
message,warn,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void warn(java.lang.String,java.lang.Object[])>"
BROKER_LOGGER,<init>,<org.apache.kafka.common.config.ConfigResource$Type: void <clinit>()>
Node {} was unable to process the fetch request with {}: {}.,info,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent a empty full fetch response to indicate that this client should be throttled for {} ms.,debug,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent an invalid full fetch response with {},info,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent a full fetch response{},debug,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent a full fetch response that created a new incremental fetch session {}{},debug,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent an invalid incremental fetch response with {},info,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent an incremental fetch response closing session {}{},debug,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
Node {} sent an incremental fetch response with throttleTimeMs = {} for session {}{},debug,"<org.apache.kafka.clients.FetchSessionHandler: boolean handleResponse(org.apache.kafka.common.requests.FetchResponse,short)>"
"Received value {} which is greater than max recordable value {}, will be pinned to the max value",debug,"<org.apache.kafka.common.metrics.stats.Percentiles: void update(org.apache.kafka.common.metrics.stats.SampledStat$Sample,org.apache.kafka.common.metrics.MetricConfig,double,long)>"
"Received value {} which is less than min recordable value {}, will be pinned to the min value",debug,"<org.apache.kafka.common.metrics.stats.Percentiles: void update(org.apache.kafka.common.metrics.stats.SampledStat$Sample,org.apache.kafka.common.metrics.MetricConfig,double,long)>"
message,debug,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void debug(java.lang.String,java.lang.Object,java.lang.Object)>"
Setting offset for partition {} to the committed offset {},info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean refreshCommittedOffsetsIfNeeded(org.apache.kafka.common.utils.Timer)>
Ignoring the returned {} since its partition {} is no longer assigned,info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean refreshCommittedOffsetsIfNeeded(org.apache.kafka.common.utils.Timer)>
Bootstrap broker {} disconnected,warn,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: void handleServerDisconnect(long,java.lang.String,java.util.Optional)>"
Reclaimed buffer of size {} and identity {} that was not properly release()ed. This is a bug.,error,<org.apache.kafka.common.memory.GarbageCollectedMemoryPool$GarbageCollectionListener: void run()>
interrupted,debug,<org.apache.kafka.common.memory.GarbageCollectedMemoryPool$GarbageCollectionListener: void run()>
GC listener shutting down,info,<org.apache.kafka.common.memory.GarbageCollectedMemoryPool$GarbageCollectionListener: void run()>
log_end_offset,<init>,<org.apache.kafka.common.message.DescribeQuorumResponseData$ReplicaState: void <clinit>()>
"Found multiple consumers {} and {} claiming the same TopicPartition {} in the same generation {}, this will be invalidated and removed from their previous assignment.",error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean allSubscriptionsEqual(java.util.Set,java.util.Map,java.util.Map,java.util.Set)>"
InitProducerId request for transactionalId `{}` failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.FenceProducersHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors)>"
InitProducerId request for transactionalId `{}` returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.FenceProducersHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors)>"
Could not read properties from file {},error,"<org.apache.kafka.common.config.provider.FileConfigProvider: org.apache.kafka.common.config.ConfigData get(java.lang.String,java.util.Set)>"
Requesting metadata update for partition {} due to error {},debug,"<org.apache.kafka.clients.Metadata: org.apache.kafka.clients.MetadataCache handleMetadataResponse(org.apache.kafka.common.requests.MetadataResponse,boolean,long)>"
Requesting metadata update for topic {} due to error {},debug,"<org.apache.kafka.clients.Metadata: org.apache.kafka.clients.MetadataCache handleMetadataResponse(org.apache.kafka.common.requests.MetadataResponse,boolean,long)>"
SSLHandshake handshakeWrap {},trace,<org.apache.kafka.common.network.SslTransportLayer: javax.net.ssl.SSLEngineResult handshakeWrap(boolean)>
Starting the Kafka producer,trace,"<org.apache.kafka.clients.producer.KafkaProducer: void <init>(org.apache.kafka.clients.producer.ProducerConfig,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.clients.producer.internals.ProducerMetadata,org.apache.kafka.clients.KafkaClient,org.apache.kafka.clients.producer.internals.ProducerInterceptors,org.apache.kafka.common.utils.Time)>"
"retry.backoff.ms, ",<init>,"<org.apache.kafka.clients.producer.KafkaProducer: void <init>(org.apache.kafka.clients.producer.ProducerConfig,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.clients.producer.internals.ProducerMetadata,org.apache.kafka.clients.KafkaClient,org.apache.kafka.clients.producer.internals.ProducerInterceptors,org.apache.kafka.common.utils.Time)>"
Kafka producer started,debug,"<org.apache.kafka.clients.producer.KafkaProducer: void <init>(org.apache.kafka.clients.producer.ProducerConfig,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.common.serialization.Serializer,org.apache.kafka.clients.producer.internals.ProducerMetadata,org.apache.kafka.clients.KafkaClient,org.apache.kafka.clients.producer.internals.ProducerInterceptors,org.apache.kafka.common.utils.Time)>"
"topics, ",<init>,"<org.apache.kafka.common.message.AlterReplicaLogDirsRequestDataJsonConverter$AlterReplicaLogDirJsonConverter: org.apache.kafka.common.message.AlterReplicaLogDirsRequestData$AlterReplicaLogDir read(com.fasterxml.jackson.databind.JsonNode,short)>"
FindCoordinator request for key {} returned topic-level error {}. Will retry,debug,"<org.apache.kafka.clients.admin.internals.CoordinatorStrategy: void handleError(org.apache.kafka.common.protocol.Errors,org.apache.kafka.clients.admin.internals.CoordinatorKey,int,java.util.Map,java.util.Map)>"
Principal={}: No Expiring credential found: will try again at {},warn,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
Principal={}: Current clock: {} is later than expiry {}. This may indicate a clock skew problem. Check that this host\'s and remote host\'s clocks are in sync. Exiting refresh thread.,error,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
Principal={}: Expiring credential already expired at {}: will try to refresh again at {},warn,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
"Principal={}: Expiring credential refresh thread exiting because the expiring credential\'s current expiration time ({}) exceeds the latest possible refresh time ({}). This process will not be able to authenticate new SASL connections after that time (for example, it will not be able to authenticate a new connection with a Kafka Broker).",warn,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
Principal={}: Expiring credential valid from {} to {},info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
"Principal={}: Expiring credential expires at {}, so buffer times of {} and {} seconds at the front and back, respectively, cannot be accommodated.  We will refresh at {}.",warn,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
"Principal={}: Proposed refresh time of {} extends into the desired buffer time of {} seconds before expiration, so refresh it at the desired buffer begin point, at {}",info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
Principal={}: Expiring credential re-login thread time adjusted from {} to {} since the former is sooner than the minimum refresh interval ({} seconds from now).,info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: java.lang.Long refreshMs(long)>
Max retries {} for {} reached,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void enqueue(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
Queueing {} with a timeout {} ms from now.,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void enqueue(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
The AdminClient thread has exited. Timing out {}.,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void enqueue(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
"Renegotiation requested, but it is not supported, channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {} handshakeStatus {}",error,<org.apache.kafka.common.network.SslTransportLayer: int read(java.nio.ByteBuffer)>
Sending heartbeat request with {}ms remaining on timer,trace,<org.apache.kafka.clients.consumer.internals.Heartbeat: void sentHeartbeat(long)>
SSL handshake completed successfully with peerHost \'{}\' peerPort {} peerPrincipal \'{}\' cipherSuite \'{}\',debug,<org.apache.kafka.common.network.SslTransportLayer: void handshakeFinished()>
"SSLHandshake FINISHED channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {} ",trace,<org.apache.kafka.common.network.SslTransportLayer: void handshakeFinished()>
Resetting sequence number of batch with current sequence {} for partition {} to {},info,"<org.apache.kafka.clients.producer.internals.TransactionManager: void lambda$adjustSequencesDueToFailedBatch$4(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.clients.producer.internals.ProducerBatch)>"
"{}: Attempted to decrement {}, but the gauge suite was closed.",warn,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void decrement(java.lang.Object)>
"{}: Attempted to decrement {}, but no such metric was registered.",debug,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void decrement(java.lang.Object)>
{}: Removed a reference to {}.  {} reference(s) remaining.,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void decrement(java.lang.Object)>
message,warn,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void warn(java.lang.String,java.lang.Object,java.lang.Object)>"
Failed to create channel due to ,warn,"<org.apache.kafka.common.network.PlaintextChannelBuilder: org.apache.kafka.common.network.KafkaChannel buildChannel(java.lang.String,java.nio.channels.SelectionKey,int,org.apache.kafka.common.memory.MemoryPool,org.apache.kafka.common.network.ChannelMetadataRegistry)>"
Group coordinator lookup failed: Invalid response containing more than a single coordinator,error,<org.apache.kafka.clients.producer.internals.TransactionManager$FindCoordinatorHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Discovered {} coordinator {},info,<org.apache.kafka.clients.producer.internals.TransactionManager$FindCoordinatorHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Login module control flag is not available in the JAAS config,<init>,<org.apache.kafka.common.security.JaasConfig: javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag loginModuleControlFlag(java.lang.String)>
"Found {} OAuth Bearer tokens in Subject\'s private credentials; the oldest expires at {}, will use the newest, which expires at {}",warn,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClientCallbackHandler: void handleCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback)>
Could not read properties from file {},error,<org.apache.kafka.common.config.provider.FileConfigProvider: org.apache.kafka.common.config.ConfigData get(java.lang.String)>
Transition from state {} to error state {},debug,"<org.apache.kafka.clients.producer.internals.TransactionManager: void transitionTo(org.apache.kafka.clients.producer.internals.TransactionManager$State,java.lang.RuntimeException)>"
Transition from state {} to {},debug,"<org.apache.kafka.clients.producer.internals.TransactionManager: void transitionTo(org.apache.kafka.clients.producer.internals.TransactionManager$State,java.lang.RuntimeException)>"
Kafka admin client initialized,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient: void <init>(org.apache.kafka.clients.admin.AdminClientConfig,java.lang.String,org.apache.kafka.common.utils.Time,org.apache.kafka.clients.admin.internals.AdminMetadataManager,org.apache.kafka.common.metrics.Metrics,org.apache.kafka.clients.KafkaClient,org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessorFactory,org.apache.kafka.common.utils.LogContext)>"
Disconnected from {}. Will retry.,debug,<org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler: void onComplete(org.apache.kafka.clients.ClientResponse)>
Received transactional response {} for request {},trace,<org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler: void onComplete(org.apache.kafka.clients.ClientResponse)>
Marking partition {} unresolved with next sequence number {},debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void markSequenceUnresolved(org.apache.kafka.clients.producer.internals.ProducerBatch)>
"No inflight batches remaining for {}, last ack\'d sequence for partition is {}, next sequence is {}. Going to bump epoch and reset sequence numbers.",info,<org.apache.kafka.clients.producer.internals.TransactionManager: void maybeResolveSequences()>
"Server config {} should be prefixed with SASL mechanism name, ignoring config",warn,"<org.apache.kafka.common.security.JaasContext: org.apache.kafka.common.security.JaasContext loadServerContext(org.apache.kafka.common.network.ListenerName,java.lang.String,java.util.Map)>"
Raising WakeupException in response to user wakeup,debug,<org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient: void maybeTriggerWakeup()>
log_start_offset,<init>,<org.apache.kafka.common.message.FetchRequestData$FetchPartition: void <clinit>()>
log_start_offset,<init>,<org.apache.kafka.common.message.FetchRequestData$FetchPartition: void <clinit>()>
log_start_offset,<init>,<org.apache.kafka.common.message.FetchRequestData$FetchPartition: void <clinit>()>
"Consumer instanceId=, <*>, , clientId=, <*>, , groupId=, <*>,  , ",<init>,"<org.apache.kafka.clients.consumer.KafkaConsumer: void <init>(org.apache.kafka.clients.consumer.ConsumerConfig,org.apache.kafka.common.serialization.Deserializer,org.apache.kafka.common.serialization.Deserializer)>"
"Consumer clientId=, <*>, , groupId=, <*>,  , ",<init>,"<org.apache.kafka.clients.consumer.KafkaConsumer: void <init>(org.apache.kafka.clients.consumer.ConsumerConfig,org.apache.kafka.common.serialization.Deserializer,org.apache.kafka.common.serialization.Deserializer)>"
Initializing the Kafka consumer,debug,"<org.apache.kafka.clients.consumer.KafkaConsumer: void <init>(org.apache.kafka.clients.consumer.ConsumerConfig,org.apache.kafka.common.serialization.Deserializer,org.apache.kafka.common.serialization.Deserializer)>"
"connections.max.idle.ms, ",<init>,"<org.apache.kafka.clients.consumer.KafkaConsumer: void <init>(org.apache.kafka.clients.consumer.ConsumerConfig,org.apache.kafka.common.serialization.Deserializer,org.apache.kafka.common.serialization.Deserializer)>"
Kafka consumer initialized,debug,"<org.apache.kafka.clients.consumer.KafkaConsumer: void <init>(org.apache.kafka.clients.consumer.ConsumerConfig,org.apache.kafka.common.serialization.Deserializer,org.apache.kafka.common.serialization.Deserializer)>"
"Token not provided, this login cannot be used to establish client connections",debug,<org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler: void handleTokenCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback)>
Extensions provided in login context without a token,<init>,<org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler: void handleTokenCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback)>
Retrieved token with principal {},info,<org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler: void handleTokenCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback)>
Error sending fetch request {} to node {}:,info,<org.apache.kafka.clients.FetchSessionHandler: void handleError(java.lang.Throwable)>
Disconnecting from {} due to timeout while awaiting {},info,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void timeoutCallsInFlight(org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor)>
Timed out {} call(s) in flight.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void timeoutCallsInFlight(org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor)>
Disabling heartbeat thread,debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread: void disable()>
"Metadata request for {} returned no error, but the leader is unknown. Will retry",debug,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: org.apache.kafka.clients.admin.internals.AdminApiLookupStrategy$LookupResult handleResponse(java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
Error registering AppInfo mbean,warn,"<org.apache.kafka.common.utils.AppInfoParser: void registerAppInfo(java.lang.String,java.lang.String,org.apache.kafka.common.metrics.Metrics,long)>"
"topics, ",<init>,"<org.apache.kafka.common.message.DescribeLogDirsRequestDataJsonConverter: org.apache.kafka.common.message.DescribeLogDirsRequestData read(com.fasterxml.jackson.databind.JsonNode,short)>"
null,<init>,<org.apache.kafka.clients.admin.KafkaAdminClient$15: org.apache.kafka.common.requests.DescribeLogDirsRequest$Builder createRequest(int)>
Support for using the empty group id by consumers is deprecated and will be removed in the next major release.,warn,<org.apache.kafka.clients.consumer.KafkaConsumer: void lambda$new$0(java.lang.String)>
Error executing interceptor onAcknowledgement callback,warn,"<org.apache.kafka.clients.producer.internals.ProducerInterceptors: void onAcknowledgement(org.apache.kafka.clients.producer.RecordMetadata,java.lang.Exception)>"
Seeking to offset {} for partition {} with epoch {},info,"<org.apache.kafka.clients.consumer.KafkaConsumer: void seek(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.OffsetAndMetadata)>"
Seeking to offset {} for partition {},info,"<org.apache.kafka.clients.consumer.KafkaConsumer: void seek(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.OffsetAndMetadata)>"
Error executing interceptor onCommit callback,warn,<org.apache.kafka.clients.consumer.internals.ConsumerInterceptors: void onCommit(java.util.Map)>
Down-converted records for partition {} with length={},debug,"<org.apache.kafka.common.record.LazyDownConversionRecordsSend: long writeTo(org.apache.kafka.common.network.TransferableChannel,long,int)>"
Found TGT with client principal \'{}\' and server principal \'{}\'.,debug,<org.apache.kafka.common.security.kerberos.KerberosLogin: javax.security.auth.kerberos.KerberosTicket getTGT()>
Metadata response contained no brokers. Will backoff and retry,debug,"<org.apache.kafka.clients.admin.internals.AllBrokersStrategy: org.apache.kafka.clients.admin.internals.AdminApiLookupStrategy$LookupResult handleResponse(java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
Discovered all brokers {} to send requests to,debug,"<org.apache.kafka.clients.admin.internals.AllBrokersStrategy: org.apache.kafka.clients.admin.internals.AdminApiLookupStrategy$LookupResult handleResponse(java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
`LeaveGroup` request for group id {} failed due to error {},debug,"<org.apache.kafka.clients.admin.internals.RemoveMembersFromConsumerGroupHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`LeaveGroup` request for group id {} failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.RemoveMembersFromConsumerGroupHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`LeaveGroup` request for group id {} returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.RemoveMembersFromConsumerGroupHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`LeaveGroup` request for group id {} failed due to unexpected error {},error,"<org.apache.kafka.clients.admin.internals.RemoveMembersFromConsumerGroupHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
Leader for partition {} is unknown for fetching offset {},debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map groupListOffsetRequests(java.util.Map,java.util.Set)>"
Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires,debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map groupListOffsetRequests(java.util.Map,java.util.Set)>"
Already logged in without a token,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean login()>
Login has already been committed without a token,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean login()>
"Logged in without a token, this login cannot be used to establish client connections",debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean login()>
Login succeeded; invoke commit() to commit it; current committed token count={},debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean login()>
"Read from closing channel failed, ignoring exception",trace,<org.apache.kafka.common.network.Selector: boolean maybeReadFromClosingChannel(org.apache.kafka.common.network.KafkaChannel)>
Login was not configured properly,<init>,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerRefreshingLogin: javax.security.auth.login.LoginContext login()>
"SyncGroup failed due to inconsistent Protocol Type, received {} but expected {}",error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Received successful SyncGroup response: {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"SyncGroup failed due to inconsistent Protocol Name, received {} but expected {}",error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Successfully synced group in generation {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Generation data was cleared by heartbeat thread to {} and state is now {} before receiving SyncGroup response, marking this rebalance as failed and retry",info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
SyncGroup failed: The group instance id {} has been fenced by another instance. Sent generation was {},error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
SyncGroup failed: {} Need to re-join the group. Sent generation was {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
SyncGroup failed: {} Marking coordinator unknown. Sent generation was {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler: void handle(org.apache.kafka.common.requests.SyncGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}",debug,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
Should never happen: re-authentication latency for a re-authenticated channel was null; continuing...,warn,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
Successfully {}authenticated with {},debug,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
Connection with {} disconnected,debug,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
Failed {}authentication with {} ({}),info,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
Unexpected error from {}; closing connection,warn,"<org.apache.kafka.common.network.Selector: void pollSelectionKeys(java.util.Set,boolean,long)>"
The OAuth login configuration encountered an error when closing the AccessTokenRetriever,warn,<org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler: void close()>
Found least loaded node {} connected with no in-flight requests,trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Removing node {} from least loaded node selection since it is neither ready for sending or connecting,trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Found least loaded node {} with {} inflight requests,trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Found least loaded connecting node {},trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Found least loaded node {} with no active connection,trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Least loaded node selection failed to find an available node,trace,<org.apache.kafka.clients.NetworkClient: org.apache.kafka.common.Node leastLoadedNode(long)>
Initiating API versions fetch from node {}.,debug,<org.apache.kafka.clients.NetworkClient: void handleInitiateApiVersionRequests(long)>
Successfully authenticate User={},debug,"<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer: byte[] process(java.lang.String,java.lang.String,org.apache.kafka.common.security.auth.SaslExtensions)>"
Error while loading kafka-version.properties: {},warn,<org.apache.kafka.common.utils.AppInfoParser: void <clinit>()>
Successfully validated token with principal {}: {},info,<org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredValidatorCallbackHandler: void handleCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerValidatorCallback)>
Group coordinator {} is unavailable or invalid due to cause: {}.isDisconnected: {}. Rediscovery will be attempted.,info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void markCoordinatorUnknown(boolean,java.lang.String)>"
Requesting disconnect from last known coordinator {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void markCoordinatorUnknown(boolean,java.lang.String)>"
Consumer has been disconnected from the group coordinator for {}ms,warn,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void markCoordinatorUnknown(boolean,java.lang.String)>"
LeaveGroup response with {} returned successfully: {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$LeaveGroupResponseHandler: void handle(org.apache.kafka.common.requests.LeaveGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
LeaveGroup request with {} failed with error: {},error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$LeaveGroupResponseHandler: void handle(org.apache.kafka.common.requests.LeaveGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Detected that all consumers were subscribed to same set of topics, invoking the optimized assignment algorithm",debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map assign(java.util.Map,java.util.Map)>"
"Detected that not all consumers were subscribed to same set of topics, falling back to the general case assignment algorithm",debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map assign(java.util.Map,java.util.Map)>"
"AdminClient clientId=, clientId,  , ",<init>,<org.apache.kafka.clients.admin.KafkaAdminClient: org.apache.kafka.common.utils.LogContext createLogContext(java.lang.String)>
(Re-)joining group,info,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendJoinGroupRequest()>
Sending JoinGroup ({}) to coordinator {},debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendJoinGroupRequest()>
"Cancelled in-flight {} request with correlation id {} due to node {} being disconnected (elapsed time since creation: {}ms, elapsed time since send: {}ms, request timeout: {}ms): {}",debug,"<org.apache.kafka.clients.NetworkClient: void cancelInFlightRequests(java.lang.String,long,java.util.Collection)>"
"Cancelled in-flight {} request with correlation id {} due to node {} being disconnected (elapsed time since creation: {}ms, elapsed time since send: {}ms, request timeout: {}ms)",info,"<org.apache.kafka.clients.NetworkClient: void cancelInFlightRequests(java.lang.String,long,java.util.Collection)>"
Exception closing connection to node {}:,error,"<org.apache.kafka.common.network.Selector: void doClose(org.apache.kafka.common.network.KafkaChannel,boolean)>"
"performing general assign. partitionsPerTopic: {}, subscriptions: {}, currentAssignment: {}",debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map generalAssign(java.util.Map,java.util.Map,java.util.Map)>"
unassigned Partitions: {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map generalAssign(java.util.Map,java.util.Map,java.util.Map)>"
Final assignment of partitions to consumers: \n{},info,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map generalAssign(java.util.Map,java.util.Map,java.util.Map)>"
"Detected truncated partitions: , truncations, ",<init>,<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.LogTruncationException buildLogTruncationException(java.util.List)>
Couldn\'t resolve server {} from {} as DNS resolution of the canonical hostname {} failed for {},warn,"<org.apache.kafka.clients.ClientUtils: java.util.List parseAndValidateAddresses(java.util.List,org.apache.kafka.clients.ClientDnsLookup)>"
Couldn\'t resolve server {} from {} as DNS resolution failed for {},warn,"<org.apache.kafka.clients.ClientUtils: java.util.List parseAndValidateAddresses(java.util.List,org.apache.kafka.clients.ClientDnsLookup)>"
Discarding error in ListOffsetResponse because another error is pending,error,<org.apache.kafka.clients.consumer.internals.Fetcher$3: void onFailure(java.lang.RuntimeException)>
Starting creation of new VerificationKeyResolver from {},debug,<org.apache.kafka.common.security.oauthbearer.secured.JwksFileVerificationKeyResolver: void init()>
Error reading the error stream,warn,<org.apache.kafka.common.utils.Shell$1: void run()>
Sending Heartbeat request with generation {} and member id {} to coordinator {},debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendHeartbeatRequest()>
Pausing partitions {},debug,<org.apache.kafka.clients.consumer.KafkaConsumer: void pause(java.util.Collection)>
Failed to close {} with type {},error,"<org.apache.kafka.common.utils.Utils: void closeQuietly(java.lang.AutoCloseable,java.lang.String,java.util.concurrent.atomic.AtomicReference)>"
"<*>, <*>",add,"<org.apache.kafka.clients.admin.KafkaAdminClient: org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult describeReplicaLogDirs(java.util.Collection,org.apache.kafka.clients.admin.DescribeReplicaLogDirsOptions)>"
,error,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyExtensions()>
An internal error occurred while retrieving SASL extensions from callback handler,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyExtensions()>
CallbackHandler {} does not support SASL extensions. No extensions will be added,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyExtensions()>
SASL Extensions cannot be null. Check whether your callback handler is explicitly setting them as null.,error,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyExtensions()>
Extensions cannot be null.,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyExtensions()>
DYNAMIC_BROKER_LOGGER_CONFIG,<init>,<org.apache.kafka.common.requests.DescribeConfigsResponse$ConfigSource: void <clinit>()>
Failed to construct list due to ,error,<org.apache.kafka.common.serialization.ListDeserializer: java.util.List createListInstance(int)>
Unexpected error code: {}.,warn,<org.apache.kafka.common.protocol.Errors: org.apache.kafka.common.protocol.Errors forCode(short)>
Requesting metadata update.,debug,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void requestUpdate()>
"Heartbeat failed, reset the timer to {}ms remaining",trace,<org.apache.kafka.clients.consumer.internals.Heartbeat: void failHeartbeat()>
Server response from broker {} mentioned unknown partition {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$16: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
,<init>,<org.apache.kafka.clients.admin.KafkaAdminClient$16: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
"Extensions callback is not supported by client callback handler {}, no extensions will be added",debug,<org.apache.kafka.common.security.scram.internals.ScramSaslClient: byte[] evaluateChallenge(byte[])>
ProducerId set to {} with epoch {},info,<org.apache.kafka.clients.producer.internals.TransactionManager: void setProducerIdAndEpoch(org.apache.kafka.common.utils.ProducerIdAndEpoch)>
Login must be done first,<init>,<org.apache.kafka.common.security.kerberos.KerberosLogin: void reLogin()>
Initiating logout for {},info,<org.apache.kafka.common.security.kerberos.KerberosLogin: void reLogin()>
Initiating re-login for {},info,<org.apache.kafka.common.security.kerberos.KerberosLogin: void reLogin()>
Cancelled request with header {} due to node {} being disconnected,trace,"<org.apache.kafka.clients.producer.internals.Sender: void handleProduceResponse(org.apache.kafka.clients.ClientResponse,java.util.Map,long)>"
Cancelled request {} due to a version mismatch with node {},warn,"<org.apache.kafka.clients.producer.internals.Sender: void handleProduceResponse(org.apache.kafka.clients.ClientResponse,java.util.Map,long)>"
Received produce response from node {} with correlation id {},trace,"<org.apache.kafka.clients.producer.internals.Sender: void handleProduceResponse(org.apache.kafka.clients.ClientResponse,java.util.Map,long)>"
"throttleTimeMs, <*>",<init>,"<org.apache.kafka.common.requests.DescribeLogDirsRequest: org.apache.kafka.common.requests.AbstractResponse getErrorResponse(int,java.lang.Throwable)>"
"Could not load config provider class , <*>, ",error,"<org.apache.kafka.common.config.AbstractConfig: java.util.Map instantiateConfigProviders(java.util.Map,java.util.Map)>"
Sending ListOffsetRequest {} to broker {},debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.clients.consumer.internals.RequestFuture sendListOffsetRequest(org.apache.kafka.common.Node,java.util.Map,boolean)>"
message,error,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void error(java.lang.String,java.lang.Object[])>"
"target,  was configured, as well as the deprecated alias(es) , <*>, .  Using the value of , target, ",error,"<org.apache.kafka.common.utils.ConfigUtils: void lambda$translateDeprecatedConfigs$4(java.util.Map,java.util.Map,java.lang.String,java.util.List)>"
"The configuration keys , <*>,  are deprecated and may be removed in the future.  Additionally, this configuration is ambigous because these configuration keys are all aliases for , target, .  Please update your configuration to have only , target,  set., ",error,"<org.apache.kafka.common.utils.ConfigUtils: void lambda$translateDeprecatedConfigs$4(java.util.Map,java.util.Map,java.lang.String,java.util.List)>"
"Configuration key , <*>,  is deprecated and may be removed in the future.  Please update your configuration to use , target,  instead., ",warn,"<org.apache.kafka.common.utils.ConfigUtils: void lambda$translateDeprecatedConfigs$4(java.util.Map,java.util.Map,java.lang.String,java.util.List)>"
Unexpected error during I/O,error,"<org.apache.kafka.clients.NetworkClient: java.util.List poll(long,long)>"
Uncaught exception in thread \'{}\':,error,"<org.apache.kafka.common.utils.KafkaThread: void lambda$configureThread$0(java.lang.String,java.lang.Thread,java.lang.Throwable)>"
Subscribed to topic(s): {},info,"<org.apache.kafka.clients.consumer.KafkaConsumer: void subscribe(java.util.Collection,org.apache.kafka.clients.consumer.ConsumerRebalanceListener)>"
Attempting to close NetworkClient that has already been closed.,warn,<org.apache.kafka.clients.NetworkClient: void close()>
"Error executing interceptor onSend callback for topic: {}, partition: {}",warn,<org.apache.kafka.clients.producer.internals.ProducerInterceptors: org.apache.kafka.clients.producer.ProducerRecord onSend(org.apache.kafka.clients.producer.ProducerRecord)>
Error executing interceptor onSend callback,warn,<org.apache.kafka.clients.producer.internals.ProducerInterceptors: org.apache.kafka.clients.producer.ProducerRecord onSend(org.apache.kafka.clients.producer.ProducerRecord)>
non-nullable field logDir was serialized as null,<init>,"<org.apache.kafka.common.message.DescribeLogDirsResponseData$DescribeLogDirsResult: void read(org.apache.kafka.common.protocol.Readable,short)>"
Member {} sending LeaveGroup request to coordinator {} due to {},info,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture maybeLeaveGroup(java.lang.String)>
Failed to send SSL Close message,debug,<org.apache.kafka.common.network.SslTransportLayer: void close()>
"connections.max.idle.ms, ",<init>,"<org.apache.kafka.clients.admin.KafkaAdminClient: org.apache.kafka.clients.admin.KafkaAdminClient createInternal(org.apache.kafka.clients.admin.AdminClientConfig,org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessorFactory,org.apache.kafka.clients.HostResolver)>"
Principal={}: It is not a Kerberos ticket,debug,<org.apache.kafka.common.security.kerberos.KerberosLogin: javax.security.auth.login.LoginContext login()>
Principal={}: It is a Kerberos ticket,debug,<org.apache.kafka.common.security.kerberos.KerberosLogin: javax.security.auth.login.LoginContext login()>
Set SASL client state to {},debug,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator: void setSaslState(org.apache.kafka.common.security.authenticator.SaslClientAuthenticator$SaslState)>
"Performing constrained assign with partitionsPerTopic: {}, consumerToOwnedPartitions: {}.",debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"Found partition {} still claimed as owned by consumer {}, despite being claimed by multiple consumers already in the same generation. Removing it from the ownedPartitions",error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"After reassigning previously owned partitions, unfilled members: {}, unassigned partitions: {}, current assignment: {}",debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
No more unfilled consumers to be assigned. The remaining unassigned partitions are: {},error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"Filled the last member up to maxQuota but still had partitions remaining to assign, will continue but this indicates a bug in the assignment.",error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"Current number of members with more than the minQuota partitions: {}, is less than the expected number of members with more than the minQuota partitions: {}, and no more partitions to be assigned to the remaining unfilled consumers: {}",error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"Consumer: {} should have {} partitions, but got {} partitions, and no more partitions to be assigned. The remaining unfilled consumers are: {}",error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
"skip over this unfilled member: {} because we\'ve reached the expected number of members with more than the minQuota partitions, and this member already have minQuota partitions",trace,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
Final assignment of partitions to consumers: \n{},info,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: java.util.Map constrainedAssign(java.util.Map,java.util.Map,java.util.Set)>"
Connection to node {} ({}) failed authentication due to: {},error,"<org.apache.kafka.clients.NetworkClient: void processDisconnection(java.util.List,java.lang.String,long,org.apache.kafka.common.network.ChannelState)>"
"Connection to node {} ({}) terminated during authentication. This may happen due to any of the following reasons: () Authentication failed due to invalid credentials with brokers older than .., () Firewall blocking Kafka TLS traffic (eg it may only allow HTTPS traffic), () Transient network issue.",warn,"<org.apache.kafka.clients.NetworkClient: void processDisconnection(java.util.List,java.lang.String,long,org.apache.kafka.common.network.ChannelState)>"
Connection to node {} ({}) could not be established. Broker may not be available.,warn,"<org.apache.kafka.clients.NetworkClient: void processDisconnection(java.util.List,java.lang.String,long,org.apache.kafka.common.network.ChannelState)>"
Failing OffsetCommit request since the consumer is not part of an active group,info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendOffsetCommitRequest(java.util.Map)>
Sending OffsetCommit request with {} to coordinator {},trace,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture sendOffsetCommitRequest(java.util.Map)>
Could not cast response body,error,"<org.apache.kafka.clients.consumer.internals.AsyncClient$1: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Received {} {} from broker {},trace,"<org.apache.kafka.clients.consumer.internals.AsyncClient$1: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
OAuth JWKS refresh is already in progress; ignoring concurrent refresh,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void refresh()>
OAuth JWKS refresh of {} starting,info,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void refresh()>
OAuth JWKS refresh of {} complete,info,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void refresh()>
OAuth JWKS refresh of {} encountered an error; not updating local JWKS cache,warn,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void refresh()>
Principal={}: Error while waiting for Login thread to shutdown.,warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: void close()>
Received {} response from node {} for request with header {}: {},debug,"<org.apache.kafka.clients.NetworkClient: void handleCompletedReceives(java.util.List,long)>"
Expected more than one potential consumer for partition \'{}\',error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean performReassignments(java.util.List,java.util.Map,java.util.Map,java.util.TreeSet,java.util.Map,java.util.Map,java.util.Map,java.util.Map,int)>"
Expected partition \'{}\' to be assigned to a consumer,error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean performReassignments(java.util.List,java.util.Map,java.util.Map,java.util.TreeSet,java.util.Map,java.util.Map,java.util.Map,java.util.Map,int)>"
Hostname for node {} changed from {} to {}.,info,"<org.apache.kafka.clients.ClusterConnectionStates: void connecting(java.lang.String,long,java.lang.String)>"
Overriding the default value for {} ({}) with the explicitly configured request timeout {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient: int configureDefaultApiTimeoutMs(org.apache.kafka.clients.admin.AdminClientConfig)>
Aborting batch for partition {},trace,<org.apache.kafka.clients.producer.internals.ProducerBatch: void abort(java.lang.RuntimeException)>
Immediately connected to node {},debug,"<org.apache.kafka.common.network.Selector: void connect(java.lang.String,java.net.InetSocketAddress,int,int)>"
Not leader error in `DescribeProducers` response for partition {} for brokerId {} set in options,error,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
Not leader error in `DescribeProducers` response for partition {}. Will retry later.,debug,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
Unknown topic/partition error in `DescribeProducers` response for partition {}. Will retry later.,debug,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
Invalid topic in `DescribeProducers` response for partition {},error,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
Authorization failed in `DescribeProducers` response for partition {},error,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
Unexpected error in `DescribeProducers` response for partition {},error,"<org.apache.kafka.clients.admin.internals.DescribeProducersHandler: void handlePartitionError(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.ApiError,java.util.Map,java.util.List)>"
`DeleteConsumerGroups` request for group id {} failed due to error {},debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DeleteConsumerGroups` request for group id {} failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DeleteConsumerGroups` request for group id {} returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DeleteConsumerGroups` request for group id {} failed due to unexpected error {},error,"<org.apache.kafka.clients.admin.internals.DeleteConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
The consumer {} is assigned more partitions than the maximum possible.,error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean canParticipateInReassignment(java.lang.String,java.util.Map,java.util.Map,java.util.Map,java.util.Map,int)>"
message,debug,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void debug(java.lang.String,java.lang.Object[])>"
Server response mentioned unknown feature {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$34: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Offset fetch failed: {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetFetchResponseHandler: void handle(org.apache.kafka.common.requests.OffsetFetchResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Failed to fetch offset for partition {}: {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetFetchResponseHandler: void handle(org.apache.kafka.common.requests.OffsetFetchResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Found no committed offset for partition {},info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetFetchResponseHandler: void handle(org.apache.kafka.common.requests.OffsetFetchResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"The following partitions still have unstable offsets which are not cleared on the broker side: {}, this could be either transactional offsets waiting for completion, or normal offsets waiting for replication after appending to local log",info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetFetchResponseHandler: void handle(org.apache.kafka.common.requests.OffsetFetchResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Requesting metadata update due to unknown leader topics from the batched records: {},debug,<org.apache.kafka.clients.producer.internals.Sender: long sendProducerData(long)>
Expired {} batches in accumulator,trace,<org.apache.kafka.clients.producer.internals.Sender: long sendProducerData(long)>
Nodes with data ready to send: {},trace,<org.apache.kafka.clients.producer.internals.Sender: long sendProducerData(long)>
App info {} for {} unregistered,info,"<org.apache.kafka.common.utils.AppInfoParser: void unregisterAppInfo(java.lang.String,java.lang.String,org.apache.kafka.common.metrics.Metrics)>"
Error unregistering AppInfo mbean,warn,"<org.apache.kafka.common.utils.AppInfoParser: void unregisterAppInfo(java.lang.String,java.lang.String,org.apache.kafka.common.metrics.Metrics)>"
App info {} for {} unregistered,info,"<org.apache.kafka.common.utils.AppInfoParser: void unregisterAppInfo(java.lang.String,java.lang.String,org.apache.kafka.common.metrics.Metrics)>"
App info {} for {} unregistered,info,"<org.apache.kafka.common.utils.AppInfoParser: void unregisterAppInfo(java.lang.String,java.lang.String,org.apache.kafka.common.metrics.Metrics)>"
refused to allocate buffer of size {},trace,<org.apache.kafka.common.memory.SimpleMemoryPool: java.nio.ByteBuffer tryAllocate(int)>
Skipping assignment for topic {} since no metadata is available,debug,"<org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor: org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$GroupAssignment assign(org.apache.kafka.common.Cluster,org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$GroupSubscription)>"
Constructed overflow message batch for partition {} with length={},debug,<org.apache.kafka.common.record.LazyDownConversionRecordsSend: org.apache.kafka.common.record.MemoryRecords buildOverflowBatch(int)>
Sending {} request with header {} and timeout {} to node {}: {},debug,"<org.apache.kafka.clients.NetworkClient: void doSend(org.apache.kafka.clients.ClientRequest,boolean,long,org.apache.kafka.common.requests.AbstractRequest)>"
Failed to create channel due to ,info,"<org.apache.kafka.common.network.SaslChannelBuilder: org.apache.kafka.common.network.KafkaChannel buildChannel(java.lang.String,java.nio.channels.SelectionKey,int,org.apache.kafka.common.memory.MemoryPool,org.apache.kafka.common.network.ChannelMetadataRegistry)>"
SSLHandshake handshakeUnwrap {},trace,"<org.apache.kafka.common.network.SslTransportLayer: javax.net.ssl.SSLEngineResult handshakeUnwrap(boolean,boolean)>"
SSLHandshake handshakeUnwrap: handshakeStatus {} status {},trace,"<org.apache.kafka.common.network.SslTransportLayer: javax.net.ssl.SSLEngineResult handshakeUnwrap(boolean,boolean)>"
DYNAMIC_BROKER_LOGGER_CONFIG,<init>,<org.apache.kafka.clients.admin.ConfigEntry$ConfigSource: void <clinit>()>
msg,error,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void error(java.lang.String,java.lang.Throwable)>"
"Incremented producer epoch, current producer ID and epoch are now {}",debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void bumpIdempotentProducerEpoch()>
allocated buffer of size {} and identity {},trace,<org.apache.kafka.common.memory.GarbageCollectedMemoryPool: void bufferToBeReturned(java.nio.ByteBuffer)>
Sending synchronous auto-commit of offsets {},debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeAutoCommitOffsetsSync(org.apache.kafka.common.utils.Timer)>
Auto-commit of offsets {} timed out before completion,debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeAutoCommitOffsetsSync(org.apache.kafka.common.utils.Timer)>
Auto-commit of offsets {} was interrupted before completion,debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeAutoCommitOffsetsSync(org.apache.kafka.common.utils.Timer)>
Synchronous auto-commit of offsets {} failed: {},warn,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeAutoCommitOffsetsSync(org.apache.kafka.common.utils.Timer)>
Completed connection to node {}. Fetching API versions.,debug,<org.apache.kafka.clients.NetworkClient: void handleConnections()>
Completed connection to node {}. Ready.,debug,<org.apache.kafka.clients.NetworkClient: void handleConnections()>
ALTER_REPLICA_LOG_DIRS,<init>,<org.apache.kafka.common.message.ApiMessageType: void <clinit>()>
DESCRIBE_LOG_DIRS,<init>,<org.apache.kafka.common.message.ApiMessageType: void <clinit>()>
getClaim - {}: {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.LoginAccessTokenValidator: java.lang.Object getClaim(java.util.Map,java.lang.String)>"
`DescribeGroups` request for group id {} failed due to error {},debug,"<org.apache.kafka.clients.admin.internals.DescribeConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DescribeGroups` request for group id {} failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.DescribeConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DescribeGroups` request for group id {} returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.DescribeConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`DescribeGroups` request for group id {} failed due to unexpected error {},error,"<org.apache.kafka.clients.admin.internals.DescribeConsumerGroupsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
Aborting producer batches due to fatal error,error,<org.apache.kafka.clients.producer.internals.Sender: void maybeAbortBatches(java.lang.RuntimeException)>
Skipping aborted record batch from partition {} with producerId {} and offsets {} to {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher$CompletedFetch: org.apache.kafka.common.record.Record nextFetchedRecord()>
Error during retry attempt {},warn,<org.apache.kafka.common.security.oauthbearer.secured.Retry: java.lang.Object execute(org.apache.kafka.common.security.oauthbearer.secured.Retryable)>
Discarding error in OffsetsForLeaderEpoch because another error is pending,error,<org.apache.kafka.clients.consumer.internals.Fetcher: void maybeSetOffsetForLeaderException(java.lang.RuntimeException)>
Cannot call logout() immediately after login(); need to first invoke commit() or abort(),<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Nothing here to log out,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Logging out my token; current committed token count = {},trace,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Done logging out my token; committed token count is now {},debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
No tokens to logout for this login,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Logging out my extensions,trace,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Done logging out my extensions,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
No extensions to logout for this login,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean logout()>
Still waiting for other calls to finish on node {}.,trace,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: long sendEligibleCalls(long)>
Disconnecting from {} and revoking {} node assignment(s) because the node is taking too long to become ready.,info,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: long sendEligibleCalls(long)>
Client is not ready to send to {}. Must delay {} ms,trace,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: long sendEligibleCalls(long)>
"Sending {} to {}. correlationId={}, timeoutMs={}",debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: long sendEligibleCalls(long)>
Cancelled request with header {} due to node {} being disconnected,debug,<org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler: void fireCompletion()>
"Removing unused topic {} from the metadata list, expiryMs {} now {}",debug,"<org.apache.kafka.clients.producer.internals.ProducerMetadata: boolean retainTopic(java.lang.String,boolean,long)>"
Sending asynchronous auto-commit of offsets {},debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture autoCommitOffsetsAsync()>
Failed to serialize list due to,error,"<org.apache.kafka.common.serialization.ListSerializer: byte[] serialize(java.lang.String,java.util.List)>"
List that could not be serialized: {},trace,"<org.apache.kafka.common.serialization.ListSerializer: byte[] serialize(java.lang.String,java.util.List)>"
Executing onJoinPrepare with generation {} and memberId {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean onJoinPrepare(org.apache.kafka.common.utils.Timer,int,java.lang.String)>"
Asynchronous auto-commit of offsets failed: joinPrepare timeout. Will continue to join group,error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean onJoinPrepare(org.apache.kafka.common.utils.Timer,int,java.lang.String)>"
Asynchronous auto-commit of offsets failed with retryable error: {}. Will retry it.,debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean onJoinPrepare(org.apache.kafka.common.utils.Timer,int,java.lang.String)>"
Asynchronous auto-commit of offsets failed: {}. Will continue to join group.,error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean onJoinPrepare(org.apache.kafka.common.utils.Timer,int,java.lang.String)>"
"Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group",info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: boolean onJoinPrepare(org.apache.kafka.common.utils.Timer,int,java.lang.String)>"
JWKS validation key calling refresh of {} starting,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: java.util.List lambda$refresh$0()>
JWKS validation key refresh of {} complete,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: java.util.List lambda$refresh$0()>
"SSLException while unwrapping data after IOException, original IOException will be propagated",debug,"<org.apache.kafka.common.network.SslTransportLayer: void maybeProcessHandshakeFailure(javax.net.ssl.SSLException,boolean,java.io.IOException)>"
Validation of dynamic config update of SSLFactory failed.,debug,<org.apache.kafka.common.security.ssl.SslFactory: org.apache.kafka.common.security.auth.SslEngineFactory createNewSslEngineFactory(java.util.Map)>
"Authentication complete; session max lifetime from broker config={} ms, credential expiration={} ({} ms); session expiration = {} ({} ms), sending {} ms to client",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator$ReauthInfo: long calcCompletionTimesAndReturnSessionLifetimeMs()>
"Authentication complete; session max lifetime from broker config={} ms, credential expiration={} ({} ms); no session expiration, sending  ms to client",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator$ReauthInfo: long calcCompletionTimesAndReturnSessionLifetimeMs()>
"Authentication complete; session max lifetime from broker config={} ms, no credential expiration; session expiration = {} ({} ms), sending {} ms to client",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator$ReauthInfo: long calcCompletionTimesAndReturnSessionLifetimeMs()>
"Authentication complete; session max lifetime from broker config={} ms, no credential expiration; no session expiration, sending  ms to client",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator$ReauthInfo: long calcCompletionTimesAndReturnSessionLifetimeMs()>
Did not attempt to add partition {} to transaction because other partitions in the batch had errors.,debug,<org.apache.kafka.clients.producer.internals.TransactionManager$AddPartitionsToTxnHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Could not add partition {} due to unexpected error {},error,<org.apache.kafka.clients.producer.internals.TransactionManager$AddPartitionsToTxnHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Successfully added partitions {} to transaction,debug,<org.apache.kafka.clients.producer.internals.TransactionManager$AddPartitionsToTxnHandler: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Received user wakeup,debug,<org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient: void wakeup()>
Skipped assignment for returning static leader at generation {}. The static leader will continue with its existing assignment.,info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.util.Map onLeaderElected(java.lang.String,java.lang.String,java.util.List,boolean)>"
Performing assignment using strategy {} with subscriptions {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.util.Map onLeaderElected(java.lang.String,java.lang.String,java.util.List,boolean)>"
Finished assignment for group at generation {}: {},info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.util.Map onLeaderElected(java.lang.String,java.lang.String,java.util.List,boolean)>"
Configured native GSSAPI private credentials for {}@{},info,<org.apache.kafka.common.network.SaslChannelBuilder: void maybeAddNativeGssapiCredentials(javax.security.auth.Subject)>
Cannot add private credential to subject; clients authentication may fail,warn,<org.apache.kafka.common.network.SaslChannelBuilder: void maybeAddNativeGssapiCredentials(javax.security.auth.Subject)>
last_offset_epoch,<init>,<org.apache.kafka.common.message.VoteRequestData$PartitionData: void <clinit>()>
last_offset,<init>,<org.apache.kafka.common.message.VoteRequestData$PartitionData: void <clinit>()>
"SSL peer is not authenticated, returning ANONYMOUS instead",debug,<org.apache.kafka.common.network.SslTransportLayer: java.security.Principal peerPrincipal()>
Invoking InitProducerId for the first time in order to acquire a producer ID,info,"<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionalRequestResult lambda$initializeTransactions$1(boolean,org.apache.kafka.common.utils.ProducerIdAndEpoch)>"
Invoking InitProducerId with current producer ID and epoch {} in order to bump the epoch,info,"<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionalRequestResult lambda$initializeTransactions$1(boolean,org.apache.kafka.common.utils.ProducerIdAndEpoch)>"
Enqueuing transactional request {},debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void enqueueRequest(org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler)>
"Received unrequested topic or partition {} from response, ignoring.",warn,"<org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient$OffsetForEpochResult handleResponse(org.apache.kafka.common.Node,java.util.Map,org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse)>"
Handling OffsetsForLeaderEpoch response for {}. Got offset {} for epoch {}.,debug,"<org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient$OffsetForEpochResult handleResponse(org.apache.kafka.common.Node,java.util.Map,org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse)>"
"Attempt to fetch offsets for partition {} failed due to {}, retrying.",debug,"<org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient$OffsetForEpochResult handleResponse(org.apache.kafka.common.Node,java.util.Map,org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse)>"
Received unknown topic or partition error in OffsetsForLeaderEpoch request for partition {}.,warn,"<org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient$OffsetForEpochResult handleResponse(org.apache.kafka.common.Node,java.util.Map,org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse)>"
"Attempt to fetch offsets for partition {} failed due to: {}, retrying.",warn,"<org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient: org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient$OffsetForEpochResult handleResponse(org.apache.kafka.common.Node,java.util.Map,org.apache.kafka.common.requests.OffsetsForLeaderEpochResponse)>"
"Extensions callback is not supported by client callback handler {}, no extensions will be added",debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClient: org.apache.kafka.common.security.auth.SaslExtensions retrieveCustomExtensions()>
Transiting to fatal error state due to {},info,<org.apache.kafka.clients.producer.internals.TransactionManager: void transitionToFatalError(java.lang.RuntimeException)>
No version information found when sending {} with correlation id {} to node {}. Assuming version {}.,trace,"<org.apache.kafka.clients.NetworkClient: void doSend(org.apache.kafka.clients.ClientRequest,boolean,long)>"
Version mismatch when attempting to send {} with correlation id {} to {},debug,"<org.apache.kafka.clients.NetworkClient: void doSend(org.apache.kafka.clients.ClientRequest,boolean,long)>"
Assigned {} to node {},trace,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean maybeDrainPendingCall(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
Unable to assign {} to a node.,trace,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean maybeDrainPendingCall(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
Unable to choose node for {},debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean maybeDrainPendingCall(org.apache.kafka.clients.admin.KafkaAdminClient$Call,long)>"
Skipping validation of fetch offsets for partitions {} since the broker does not support the required protocol version (introduced in Kafka .),debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: void lambda$validateOffsetsAsync$5(long,org.apache.kafka.common.Node,java.util.Map)>"
NOT_ENOUGH_REPLICAS_AFTER_APPEND,<init>,<org.apache.kafka.common.protocol.Errors: void <clinit>()>
INVALID_REQUEST,<init>,<org.apache.kafka.common.protocol.Errors: void <clinit>()>
KAFKA_STORAGE_ERROR,<init>,<org.apache.kafka.common.protocol.Errors: void <clinit>()>
LOG_DIR_NOT_FOUND,<init>,<org.apache.kafka.common.protocol.Errors: void <clinit>()>
INCONSISTENT_TOPIC_ID,<init>,<org.apache.kafka.common.protocol.Errors: void <clinit>()>
{} should be equal to or larger than {} + {}. Setting it to {}.,warn,"<org.apache.kafka.clients.producer.KafkaProducer: int configureDeliveryTimeout(org.apache.kafka.clients.producer.ProducerConfig,org.slf4j.Logger)>"
<*>,add,"<org.apache.kafka.clients.admin.KafkaAdminClient: org.apache.kafka.clients.admin.AlterReplicaLogDirsResult alterReplicaLogDirs(java.util.Map,org.apache.kafka.clients.admin.AlterReplicaLogDirsOptions)>"
Non-atomic move of {} to {} succeeded after atomic move failed due to {},debug,"<org.apache.kafka.common.utils.Utils: void atomicMoveWithFallback(java.nio.file.Path,java.nio.file.Path,boolean)>"
Enabling heartbeat thread,debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread: void enable()>
Assigned partition {} for non-subscribed topic regex pattern; subscription pattern is {},info,<org.apache.kafka.clients.consumer.internals.SubscriptionState: boolean checkAssignmentMatchedSubscription(java.util.Collection)>
Assigned partition {} for non-subscribed topic; subscription is {},info,<org.apache.kafka.clients.consumer.internals.SubscriptionState: boolean checkAssignmentMatchedSubscription(java.util.Collection)>
Skipping transition to abortable error state since the transaction is already being aborted. Underlying exception: ,debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void transitionToAbortableError(java.lang.RuntimeException)>
Transiting to abortable error state due to {},info,<org.apache.kafka.clients.producer.internals.TransactionManager: void transitionToAbortableError(java.lang.RuntimeException)>
Fallback the requireStable flag to false as broker only supports OffsetFetchRequest version {}. Need v or newer to enable this feature,trace,<org.apache.kafka.common.requests.OffsetFetchRequest$Builder: org.apache.kafka.common.requests.OffsetFetchRequest build(short)>
Metadata response reported invalid topics {},error,<org.apache.kafka.clients.Metadata: void checkInvalidTopics(org.apache.kafka.common.Cluster)>
"Could not configure ListDeserializer as some parameters were already set -- listClass: {}, inner: {}",error,"<org.apache.kafka.common.serialization.ListDeserializer: void configure(java.util.Map,boolean)>"
,<init>,"<org.apache.kafka.common.record.FileRecords: org.apache.kafka.common.record.FileRecords$LogOffsetPosition searchForOffsetWithSize(long,int)>"
Unable to find FetchSessionHandler for node {}. Ignoring fetch response.,error,<org.apache.kafka.clients.consumer.internals.Fetcher$1: void onSuccess(org.apache.kafka.clients.ClientResponse)>
Fetch {} at offset {} for partition {} returned fetch data {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher$1: void onSuccess(org.apache.kafka.clients.ClientResponse)>
Offset commit with offsets {} failed,error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$DefaultOffsetCommitCallback: void onComplete(java.util.Map,java.lang.Exception)>"
Disconnecting from node {} due to request timeout.,info,"<org.apache.kafka.clients.NetworkClient: void handleTimedOutRequests(java.util.List,long)>"
Trying to choose nodes for {} at {},trace,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: long maybeDrainPendingCalls(long)>
,<init>,<org.apache.kafka.common.message.AlterReplicaLogDirsRequestData: org.apache.kafka.common.message.AlterReplicaLogDirsRequestData duplicate()>
,add,<org.apache.kafka.common.message.AlterReplicaLogDirsRequestData: org.apache.kafka.common.message.AlterReplicaLogDirsRequestData duplicate()>
Failed to close producer interceptor ,error,<org.apache.kafka.clients.producer.internals.ProducerInterceptors: void close()>
Metadata is not usable: failed to get metadata.,debug,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: boolean isReady()>
Metadata is not ready: bootstrap nodes have not been initialized yet.,trace,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: boolean isReady()>
Metadata is not ready: we have not fetched metadata from the bootstrap nodes yet.,trace,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: boolean isReady()>
Metadata is ready to use.,trace,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: boolean isReady()>
message,info,<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void info(java.lang.String)>
"Created SSL context with keystore {}, truststore {}, provider {}.",debug,"<org.apache.kafka.common.security.ssl.DefaultSslEngineFactory: javax.net.ssl.SSLContext createSSLContext(org.apache.kafka.common.security.ssl.DefaultSslEngineFactory$SecurityStore,org.apache.kafka.common.security.ssl.DefaultSslEngineFactory$SecurityStore)>"
"Got error produce response in correlation id {} on topic-partition {}, splitting and retrying ({} attempts left). Error: {}",warn,"<org.apache.kafka.clients.producer.internals.Sender: void completeBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.common.requests.ProduceResponse$PartitionResponse,long,long)>"
"Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}",warn,"<org.apache.kafka.clients.producer.internals.Sender: void completeBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.common.requests.ProduceResponse$PartitionResponse,long,long)>"
Received unknown topic or partition error in produce request on partition {}. The topic-partition may not exist or the user may not have Describe access to it,warn,"<org.apache.kafka.clients.producer.internals.Sender: void completeBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.common.requests.ProduceResponse$PartitionResponse,long,long)>"
Received invalid metadata error in produce request on partition {} due to {}. Going to request metadata update now,warn,"<org.apache.kafka.clients.producer.internals.Sender: void completeBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.common.requests.ProduceResponse$PartitionResponse,long,long)>"
Node {} disconnected.,info,"<org.apache.kafka.clients.NetworkClient: void handleDisconnections(java.util.List,long)>"
log_dir,<init>,<org.apache.kafka.common.message.DescribeLogDirsResponseData$DescribeLogDirsResult: void <clinit>()>
log_dir,<init>,<org.apache.kafka.common.message.DescribeLogDirsResponseData$DescribeLogDirsResult: void <clinit>()>
Unregister broker request for broker ID {} failed: {},error,<org.apache.kafka.clients.admin.KafkaAdminClient$35: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Stickiness is violated for topic {}\nPartition movements for this topic occurred among the following consumer pairs:\n{},error,<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor$PartitionMovements: boolean isSticky()>
Received ListOffsetResponse {} from broker {},trace,"<org.apache.kafka.clients.consumer.internals.Fetcher$6: void onSuccess(org.apache.kafka.clients.ClientResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Disconnecting from node {} due to socket connection setup timeout. The timeout value is {} ms.,info,"<org.apache.kafka.clients.NetworkClient: void handleTimedOutConnections(java.util.List,long)>"
,warn,<org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler: void handleTokenCallback(org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback)>
Discarding write buffer {} since peer has disconnected,debug,<org.apache.kafka.common.network.SslTransportLayer: void clearWriteBuffer()>
Removing expired sensor {},debug,<org.apache.kafka.common.metrics.Metrics$ExpireSensorTask: void run()>
"Ignoring batch {} with producer id {}, epoch {}, and sequence number {} since the producer is already in fatal error state",debug,"<org.apache.kafka.clients.producer.internals.TransactionManager: void handleFailedBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,java.lang.RuntimeException,boolean)>"
"The broker returned {} for topic-partition {} with producerId {}, epoch {}, and sequence number {}",error,"<org.apache.kafka.clients.producer.internals.TransactionManager: void handleFailedBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,java.lang.RuntimeException,boolean)>"
WriteTxnMarkers request for abort spec {} failed cluster authorization,error,<org.apache.kafka.clients.admin.internals.AbortTransactionHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.common.protocol.Errors)>
WriteTxnMarkers request for abort spec {} failed due to an invalid producer epoch,error,<org.apache.kafka.clients.admin.internals.AbortTransactionHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.common.protocol.Errors)>
WriteTxnMarkers request for abort spec {} failed because the coordinator epoch is fenced,error,<org.apache.kafka.clients.admin.internals.AbortTransactionHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.common.protocol.Errors)>
WriteTxnMarkers request for abort spec {} failed due to {}. Will retry after attempting to find the leader again,debug,<org.apache.kafka.clients.admin.internals.AbortTransactionHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.common.protocol.Errors)>
WriteTxnMarkers request for abort spec {} failed due to an unexpected error {},error,<org.apache.kafka.clients.admin.internals.AbortTransactionHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleError(org.apache.kafka.common.protocol.Errors)>
"SSLHandshake NEED_TASK channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void doHandshake()>
"SSLHandshake NEED_WRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void doHandshake()>
"SSLHandshake NEED_WRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void doHandshake()>
"SSLHandshake NEED_UNWRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void doHandshake()>
"SSLHandshake NEED_UNWRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",trace,<org.apache.kafka.common.network.SslTransportLayer: void doHandshake()>
close started,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
JWKS validation key refresh thread shutting down,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
JWKS validation key refresh thread termination did not end after {} {},warn,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
JWKS validation key refresh thread error during close,warn,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
close completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
close completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: void close()>
"Error when sending message to topic {} with key: {}, value: {} with error:",error,"<org.apache.kafka.clients.producer.internals.ErrorLoggingCallback: void onCompletion(org.apache.kafka.clients.producer.RecordMetadata,java.lang.Exception)>"
Nothing here to commit,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean commit()>
Committing my token; current committed token count = {},trace,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean commit()>
Done committing my token; committed token count is now {},debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean commit()>
"No tokens to commit, this login cannot be used to establish client connections",debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean commit()>
Received unknown control record key version {}. Parsing as version {},debug,<org.apache.kafka.common.record.ControlRecordType: short parseTypeId(java.nio.ByteBuffer)>
Closing the Kafka consumer,trace,"<org.apache.kafka.clients.consumer.KafkaConsumer: void close(long,boolean)>"
Failed to close coordinator,error,"<org.apache.kafka.clients.consumer.KafkaConsumer: void close(long,boolean)>"
Kafka consumer has been closed,debug,"<org.apache.kafka.clients.consumer.KafkaConsumer: void close(long,boolean)>"
{} for mechanism={}: {},trace,"<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: void <init>(java.util.Map,java.util.Map,java.lang.String,java.util.Map,org.apache.kafka.common.security.kerberos.KerberosShortNamer,org.apache.kafka.common.network.ListenerName,org.apache.kafka.common.security.auth.SecurityProtocol,org.apache.kafka.common.network.TransportLayer,java.util.Map,org.apache.kafka.common.network.ChannelMetadataRegistry,org.apache.kafka.common.utils.Time,java.util.function.Supplier)>"
Setting SASL/{} server state to {},debug,<org.apache.kafka.common.security.scram.internals.ScramSaslServer: void setState(org.apache.kafka.common.security.scram.internals.ScramSaslServer$State)>
SSL Handshake failed,debug,"<org.apache.kafka.common.network.SslTransportLayer: void handshakeFailure(javax.net.ssl.SSLException,boolean)>"
SSLEngine.closeInBound() raised an exception.,debug,"<org.apache.kafka.common.network.SslTransportLayer: void handshakeFailure(javax.net.ssl.SSLException,boolean)>"
Delay propagation of handshake exception till {} bytes remaining are flushed,debug,"<org.apache.kafka.common.network.SslTransportLayer: void handshakeFailure(javax.net.ssl.SSLException,boolean)>"
init started,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void init()>
init completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void init()>
init completed,debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwksVerificationKeyResolver: void init()>
"Target node {} not ready within request timeout, will retry when node is ready.",trace,<org.apache.kafka.clients.producer.internals.Sender: boolean maybeSendAndPollTransactionalRequest()>
"Coordinator not known for {}, will retry {} after finding coordinator.",trace,<org.apache.kafka.clients.producer.internals.Sender: boolean maybeSendAndPollTransactionalRequest()>
"No nodes available to send requests, will poll and retry when until a node is ready.",trace,<org.apache.kafka.clients.producer.internals.Sender: boolean maybeSendAndPollTransactionalRequest()>
Sending transactional request {} to node {} with correlation ID {},debug,<org.apache.kafka.clients.producer.internals.Sender: boolean maybeSendAndPollTransactionalRequest()>
Disconnect from {} while trying to send request {}. Going to back off and retry.,debug,<org.apache.kafka.clients.producer.internals.Sender: boolean maybeSendAndPollTransactionalRequest()>
Built full fetch {} for node {} with {}.,debug,<org.apache.kafka.clients.FetchSessionHandler$Builder: org.apache.kafka.clients.FetchSessionHandler$FetchRequestData build()>
"Built incremental fetch {} for node {}. Added {}, altered {}, removed {}, replaced {} out of {}",debug,<org.apache.kafka.clients.FetchSessionHandler$Builder: org.apache.kafka.clients.FetchSessionHandler$FetchRequestData build()>
message,warn,<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void warn(java.lang.String)>
Error executing user-provided callback on message for topic-partition \'{}\',error,"<org.apache.kafka.clients.producer.internals.ProducerBatch: void completeFutureAndFireCallbacks(long,long,java.util.function.Function)>"
Sending leader SyncGroup to coordinator {}: {},debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture onLeaderElected(org.apache.kafka.common.requests.JoinGroupResponse)>
Ran out of bytes in serialized list,error,"<org.apache.kafka.common.serialization.ListDeserializer: java.util.List deserialize(java.lang.String,byte[])>"
Deserialized list so far: {},trace,"<org.apache.kafka.common.serialization.ListDeserializer: java.util.List deserialize(java.lang.String,byte[])>"
Login aborted,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean abort()>
Nothing here to abort,debug,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: boolean abort()>
message,error,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void error(java.lang.String,java.lang.Object)>"
Received error {} from node {} when making an ApiVersionsRequest with correlation id {}. Disconnecting.,warn,"<org.apache.kafka.clients.NetworkClient: void handleApiVersionsResponse(java.util.List,org.apache.kafka.clients.NetworkClient$InFlightRequest,long,org.apache.kafka.common.requests.ApiVersionsResponse)>"
"Node {} has finalized features epoch: {}, finalized features: {}, supported features: {}, API versions: {}.",debug,"<org.apache.kafka.clients.NetworkClient: void handleApiVersionsResponse(java.util.List,org.apache.kafka.clients.NetworkClient$InFlightRequest,long,org.apache.kafka.common.requests.ApiVersionsResponse)>"
Not sending transactional request {} because we are in an error state,trace,<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler nextRequest(boolean)>
Not sending EndTxn for completed transaction since no partitions or offsets were successfully added,debug,<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler nextRequest(boolean)>
Request {} dequeued for sending,trace,<org.apache.kafka.clients.producer.internals.TransactionManager: org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler nextRequest(boolean)>
Adding newly assigned partitions: {},info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsAssigned(java.util.SortedSet)>
User provided listener {} failed on invocation of onPartitionsAssigned for partitions {},error,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsAssigned(java.util.SortedSet)>
Sending follower SyncGroup to coordinator {}: {},debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture onJoinFollower()>
The OAuth login configuration encountered an error when initializing the AccessTokenRetriever,<init>,"<org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler: void init(org.apache.kafka.common.security.oauthbearer.secured.AccessTokenRetriever,org.apache.kafka.common.security.oauthbearer.secured.AccessTokenValidator)>"
Handling Kafka request {} during {},debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: boolean handleKafkaRequest(byte[])>
"Received client packet of length {} starting with bytes x{}, process as GSSAPI packet",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: boolean handleKafkaRequest(byte[])>
"First client packet is not a SASL mechanism request, using default mechanism GSSAPI",debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: boolean handleKafkaRequest(byte[])>
Setting bootstrap cluster metadata {}.,debug,"<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void update(org.apache.kafka.common.Cluster,long)>"
Updating cluster metadata to {},debug,"<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void update(org.apache.kafka.common.Cluster,long)>"
"XCertificate: Alg:{}, Serial:{}, Subject:{}, Issuer:{}, Key type:{}, Length:{}, Cert Id:{}, Valid from:{}, Valid until:{}",fine,"<jdk.internal.event.EventHelper: void logX509CertificateEvent(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,long,long,long)>"
Sending {} {} to broker {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: int sendFetches()>
"Heartbeat groupID=, <*>,  , ",<init>,"<org.apache.kafka.clients.consumer.internals.Heartbeat: void <init>(org.apache.kafka.clients.GroupRebalanceConfig,org.apache.kafka.common.utils.Time)>"
"Not fetching from {} for partition {} since it is marked offline or is missing from our metadata, using the leader instead.",trace,"<org.apache.kafka.clients.consumer.internals.Fetcher: org.apache.kafka.common.Node selectReadReplica(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.Node,long)>"
`OffsetFetch` request for group id {} failed due to error {},debug,"<org.apache.kafka.clients.admin.internals.ListConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetFetch` request for group id {} failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.ListConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetFetch` request for group id {} returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.ListConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
`OffsetFetch` request for group id {} failed due to unexpected error {},error,"<org.apache.kafka.clients.admin.internals.ListConsumerGroupOffsetsHandler: void handleGroupError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set)>"
Failed during {}: {},debug,<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: void authenticate()>
"Principal could not be determined from Subject, this may be a transient failure due to Kerberos re-login",<init>,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator: java.lang.String firstPrincipal(javax.security.auth.Subject)>
Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms,trace,"<org.apache.kafka.clients.producer.internals.RecordAccumulator: org.apache.kafka.clients.producer.internals.RecordAccumulator$RecordAppendResult append(org.apache.kafka.common.TopicPartition,long,byte[],byte[],org.apache.kafka.common.header.Header[],org.apache.kafka.clients.producer.Callback,long,boolean,long)>"
"Could not construct ListDeserializer as not all required parameters were present -- listClass: {}, inner: {}",error,"<org.apache.kafka.common.serialization.ListDeserializer: void <init>(java.lang.Class,org.apache.kafka.common.serialization.Deserializer)>"
Sent produce request to {}: {},trace,"<org.apache.kafka.clients.producer.internals.Sender: void sendProduceRequest(long,int,short,int,java.util.List)>"
", , <*> <*> bufferSupplier",<init>,"<org.apache.kafka.common.record.AbstractLegacyRecordBatch$DeepRecordsIterator: void <init>(org.apache.kafka.common.record.AbstractLegacyRecordBatch,boolean,int,org.apache.kafka.common.utils.BufferSupplier)>"
"All work has been completed, and the I/O thread is now exiting.",trace,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean threadShouldExit(long,long)>"
Forcing a hard I/O thread shutdown. Requests in progress will be aborted.,info,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean threadShouldExit(long,long)>"
Hard shutdown in {} ms.,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: boolean threadShouldExit(long,long)>"
Principal={}: TGT valid starting at: {},info,<org.apache.kafka.common.security.kerberos.KerberosLogin: long getRefreshTime(javax.security.auth.kerberos.KerberosTicket)>
Principal={}: TGT expires: {},info,<org.apache.kafka.common.security.kerberos.KerberosLogin: long getRefreshTime(javax.security.auth.kerberos.KerberosTicket)>
Created SSLSocketFactory: {},debug,<org.apache.kafka.common.security.oauthbearer.secured.JaasOptionsUtils: javax.net.ssl.SSLSocketFactory createSSLSocketFactory()>
{} timed out at {} after {} attempt(s),debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$Call: void handleTimeoutFailure(long,java.lang.Throwable)>"
Uncaught error in request completion:,error,<org.apache.kafka.clients.NetworkClient: void completeResponses(java.util.List)>
Created new {} SSL engine builder with keystore {} truststore {},info,<org.apache.kafka.common.security.ssl.SslFactory: void reconfigure(java.util.Map)>
"Disabling exponential reconnect backoff because {} is set, but {} is not.",debug,"<org.apache.kafka.clients.CommonClientConfigs: java.util.Map postProcessReconnectBackoffConfigs(org.apache.kafka.common.config.AbstractConfig,java.util.Map)>"
Begin adding new partition {} to transaction,debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void maybeAddPartition(org.apache.kafka.common.TopicPartition)>
Thread starting,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void run()>
Timed out {} remaining operation(s) during close.,info,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void run()>
Exiting AdminClientRunnable thread.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void run()>
Timed out {} remaining operation(s) during close.,info,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void run()>
Exiting AdminClientRunnable thread.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void run()>
{} is assigned to more than one consumer.,error,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean isBalanced(java.util.Map,java.util.TreeSet,java.util.Map,java.util.Map,int)>"
{} can be moved from consumer {} to consumer {} for a more balanced assignment.,debug,"<org.apache.kafka.clients.consumer.internals.AbstractStickyAssignor: boolean isBalanced(java.util.Map,java.util.TreeSet,java.util.Map,java.util.Map,int)>"
"Broker configuration \'{}\' is applied only to SSL listeners. Listener-prefixed configuration can be used to enable SSL client authentication for SASL_SSL listeners. In future releases, broker-wide option without listener prefix may be applied to SASL_SSL listeners as well. All configuration options intended for specific listeners should be listener-prefixed.",warn,"<org.apache.kafka.common.network.ChannelBuilders: org.apache.kafka.common.network.ChannelBuilder create(org.apache.kafka.common.security.auth.SecurityProtocol,org.apache.kafka.common.network.Mode,org.apache.kafka.common.security.JaasContext$Type,org.apache.kafka.common.config.AbstractConfig,org.apache.kafka.common.network.ListenerName,boolean,java.lang.String,boolean,org.apache.kafka.common.security.authenticator.CredentialCache,org.apache.kafka.common.security.token.delegation.internals.DelegationTokenCache,org.apache.kafka.common.utils.Time,org.apache.kafka.common.utils.LogContext,java.util.function.Supplier)>"
Creating SaslClient: client={};service={};serviceHostname={};mechs={},debug,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator: javax.security.sasl.SaslClient lambda$createSaslClient$0()>
Resetting idempotent producer ID. ID and epoch before reset are {},debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void resetIdempotentProducerId()>
"producerId: {}, send to partition {} failed fatally. Reducing future sequence numbers by {}",debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void adjustSequencesDueToFailedBatch(org.apache.kafka.clients.producer.internals.ProducerBatch)>
Login module control flag not specified in JAAS config,<init>,<org.apache.kafka.common.security.JaasConfig: javax.security.auth.login.AppConfigurationEntry parseAppConfigurationEntry(java.io.StreamTokenizer)>
Resetting the last seen epoch of partition {} to {} since the associated topicId changed from {} to {},info,"<org.apache.kafka.clients.Metadata: java.util.Optional updateLatestMetadata(org.apache.kafka.common.requests.MetadataResponse$PartitionMetadata,boolean,org.apache.kafka.common.Uuid,org.apache.kafka.common.Uuid)>"
Updating last seen epoch for partition {} from {} to epoch {} from new metadata,debug,"<org.apache.kafka.clients.Metadata: java.util.Optional updateLatestMetadata(org.apache.kafka.common.requests.MetadataResponse$PartitionMetadata,boolean,org.apache.kafka.common.Uuid,org.apache.kafka.common.Uuid)>"
"Got metadata for an older epoch {} (current is {}) for partition {}, not updating",debug,"<org.apache.kafka.clients.Metadata: java.util.Optional updateLatestMetadata(org.apache.kafka.common.requests.MetadataResponse$PartitionMetadata,boolean,org.apache.kafka.common.Uuid,org.apache.kafka.common.Uuid)>"
Could not read file {} for property {},error,<org.apache.kafka.common.config.provider.DirectoryConfigProvider: java.lang.String read(java.nio.file.Path)>
msg,warn,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void warn(java.lang.String,java.lang.Throwable)>"
OffsetCommit request for group id {} returned error {}. Will retry.,debug,"<org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set,java.util.Set)>"
OffsetCommit request for group id {} returned error {}. Will rediscover the coordinator and retry.,debug,"<org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set,java.util.Set)>"
OffsetCommit request for group id {} failed due to error {}.,debug,"<org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set,java.util.Set)>"
OffsetCommit request for group id {} and partition {} failed due to error {}.,debug,"<org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set,java.util.Set)>"
OffsetCommit request for group id {} and partition {} failed due to unexpected error {}.,error,"<org.apache.kafka.clients.admin.internals.AlterConsumerGroupOffsetsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.Set,java.util.Set)>"
"Updating preferred read replica for partition {} to {}, set to expire at {}",debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: long lambda$initializeCompletedFetch$13(org.apache.kafka.common.TopicPartition,org.apache.kafka.common.message.FetchResponseData$PartitionData)>"
Could not configure ListSerializer as the parameter has already been set -- inner: {},error,"<org.apache.kafka.common.serialization.ListSerializer: void configure(java.util.Map,boolean)>"
Give up sending metadata request since no node is available,debug,<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: long maybeUpdate(long)>
JAAS config property does not contain any login modules,<init>,"<org.apache.kafka.common.security.JaasContext: org.apache.kafka.common.security.JaasContext load(org.apache.kafka.common.security.JaasContext$Type,java.lang.String,java.lang.String,org.apache.kafka.common.config.types.Password)>"
Received %x response from client after it received our error,debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslServer: byte[] evaluateResponse(byte[])>
Kafka version: {},info,<org.apache.kafka.common.utils.AppInfoParser$AppInfo: void <init>(long)>
Kafka commitId: {},info,<org.apache.kafka.common.utils.AppInfoParser$AppInfo: void <init>(long)>
Kafka startTimeMs: {},info,<org.apache.kafka.common.utils.AppInfoParser$AppInfo: void <init>(long)>
"JoinGroup failed: Inconsistent Protocol Type, received {} but expected {}",error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Received successful JoinGroup response: {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Successfully joined group with generation {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed: Coordinator {} is loading the group.,info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed: {} Need to re-join the group. Sent generation was {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed: {} Marking coordinator unknown. Sent generation was {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed: The group instance id {} has been fenced by another instance. Sent generation was {},error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed due to fatal error: {},error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed due to unsupported version error. Please unset field group.instance.id and retry to see if the problem resolves,error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed due to non-fatal error: {}. Will set the member id as {} and then rejoin. Sent generation was {},debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"JoinGroup failed due to non-fatal error: REBALANCE_IN_PROGRESS, which could indicate a replication timeout on the broker. Will retry.",info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
JoinGroup failed due to unexpected error: {},error,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler: void handle(org.apache.kafka.common.requests.JoinGroupResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Successfully authenticated client: authenticationID={}; authorizationID={}.,info,<org.apache.kafka.common.security.authenticator.SaslServerCallbackHandler: void handleAuthorizeCallback(javax.security.sasl.AuthorizeCallback)>
Initiating close operation.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
Moving hard shutdown time forward.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
Hard shutdown time is already earlier than requested.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
Waiting for the I/O thread to exit. Hard shutdown in {} ms.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
Kafka admin client closed.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
Interrupted while joining I/O thread,debug,<org.apache.kafka.clients.admin.KafkaAdminClient: void close(java.time.Duration)>
mismatch in sending bytes over socket; expected: {} actual: {},error,<org.apache.kafka.common.record.MultiRecordsSend: long writeTo(org.apache.kafka.common.network.TransferableChannel)>
"Bytes written as part of multi-send call: {}, total bytes written so far: {}, expected bytes to write: {}",trace,<org.apache.kafka.common.record.MultiRecordsSend: long writeTo(org.apache.kafka.common.network.TransferableChannel)>
Unrecognized client authentication configuration {}.  Falling back to NONE.  Recognized client authentication configurations are {}.,warn,<org.apache.kafka.common.security.ssl.DefaultSslEngineFactory: org.apache.kafka.common.config.SslClientAuth createSslClientAuth(java.lang.String)>
Reordered incoming batch with sequence {} for partition {}. It was placed in the queue at position {},debug,"<org.apache.kafka.clients.producer.internals.RecordAccumulator: void insertInSequenceOrder(java.util.Deque,org.apache.kafka.clients.producer.internals.ProducerBatch)>"
{}: gauge suite is already closed.,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void close()>
{}: closed {} metric(s).,trace,<org.apache.kafka.common.metrics.internals.IntGaugeSuite: void close()>
Skipping completed validation for partition {} which is not currently assigned.,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
Skipping completed validation for partition {} which is no longer expecting validation.,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
Skipping completed validation for partition {} since the current position {} no longer matches the position {} when the request was sent,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
"Truncation detected for partition {} at offset {}, resetting offset",info,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
"Truncation detected for partition {} at offset {}, but no reset policy is set",warn,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
"Truncation detected for partition {} at offset {}, resetting offset to the first offset known to diverge {}",info,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
"Truncation detected for partition {} at offset {} (the end offset from the broker is {}), but no reset policy is set",warn,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: java.util.Optional maybeCompleteValidation(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.message.OffsetForLeaderEpochResponseData$EpochEndOffset)>"
Server response mentioned unknown topic {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$2: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Initiating connection to node {} using address {},debug,"<org.apache.kafka.clients.NetworkClient: void initiateConnect(org.apache.kafka.common.Node,long)>"
Error connecting to node {},warn,"<org.apache.kafka.clients.NetworkClient: void initiateConnect(org.apache.kafka.common.Node,long)>"
Successfully logged in.,info,<org.apache.kafka.common.security.authenticator.AbstractLogin: javax.security.auth.login.LoginContext login()>
Added sensor with name {},trace,"<org.apache.kafka.common.metrics.Metrics: org.apache.kafka.common.metrics.Sensor sensor(java.lang.String,org.apache.kafka.common.metrics.MetricConfig,long,org.apache.kafka.common.metrics.Sensor$RecordingLevel,org.apache.kafka.common.metrics.Sensor[])>"
Lost previously assigned partitions {},info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsLost(java.util.SortedSet)>
The pause flag in partitions {} will be removed due to partition lost.,info,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsLost(java.util.SortedSet)>
User provided listener {} failed on invocation of onPartitionsLost for partitions {},error,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: java.lang.Exception invokePartitionsLost(java.util.SortedSet)>
Requesting metadata update for partition {} since the position {} is missing the current leader node,debug,<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map prepareFetchRequests()>
Skipping fetch for partition {} because node {} is awaiting reconnect backoff,trace,<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map prepareFetchRequests()>
Skipping fetch for partition {} because previous request to {} has not been processed,trace,<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map prepareFetchRequests()>
Added {} fetch request for partition {} at position {} to node {},debug,<org.apache.kafka.clients.consumer.internals.Fetcher: java.util.Map prepareFetchRequests()>
DescribeTransactions request for transactionalId `{}` failed because the coordinator is still in the process of loading state. Will retry,debug,"<org.apache.kafka.clients.admin.internals.DescribeTransactionsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.List)>"
DescribeTransactions request for transactionalId `{}` returned error {}. Will attempt to find the coordinator again and retry,debug,"<org.apache.kafka.clients.admin.internals.DescribeTransactionsHandler: void handleError(org.apache.kafka.clients.admin.internals.CoordinatorKey,org.apache.kafka.common.protocol.Errors,java.util.Map,java.util.List)>"
msg,info,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void info(java.lang.String,java.lang.Throwable)>"
"Skipping next batch expiry time update due to addition overflow: batch.createMs={}, deliveryTimeoutMs={}",warn,<org.apache.kafka.clients.producer.internals.RecordAccumulator: void maybeUpdateNextBatchExpiryTime(org.apache.kafka.clients.producer.internals.ProducerBatch)>
{} acquired,trace,<org.apache.kafka.common.security.authenticator.LoginManager: org.apache.kafka.common.security.authenticator.LoginManager acquire()>
"Internal server error on {}: server returned information about unknown correlation ID {}, requestHeader = {}",error,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void handleResponses(long,java.util.List)>"
Internal server error on {}: ignoring call {} in correlationIdToCall that did not exist in callsInFlight,error,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void handleResponses(long,java.util.List)>"
{} got response {},trace,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void handleResponses(long,java.util.List)>"
{} handleResponse failed with {},trace,"<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: void handleResponses(long,java.util.List)>"
Failed to close consumer interceptor ,error,<org.apache.kafka.clients.consumer.internals.ConsumerInterceptors: void close()>
Determining if we should replace existing epoch {} with new epoch {} for partition {},trace,"<org.apache.kafka.clients.Metadata: boolean updateLastSeenEpochIfNewer(org.apache.kafka.common.TopicPartition,int)>"
Not replacing null epoch with new epoch {} for partition {},debug,"<org.apache.kafka.clients.Metadata: boolean updateLastSeenEpochIfNewer(org.apache.kafka.common.TopicPartition,int)>"
Updating last seen epoch from {} to {} for partition {},debug,"<org.apache.kafka.clients.Metadata: boolean updateLastSeenEpochIfNewer(org.apache.kafka.common.TopicPartition,int)>"
Not replacing existing epoch {} with new epoch {} for partition {},debug,"<org.apache.kafka.clients.Metadata: boolean updateLastSeenEpochIfNewer(org.apache.kafka.common.TopicPartition,int)>"
Committed offset {} for partition {},debug,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Offset commit failed on partition {} at offset {}: {},warn,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Offset commit failed on partition {} at offset {}: {},error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
OffsetCommit failed with {} due to group instance id {} fenced,info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
OffsetCommit failed with {}: {},info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Not authorized to commit to topics {},error,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler: void handle(org.apache.kafka.common.requests.OffsetCommitResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Modification time of key store could not be obtained: , path, ",error,<org.apache.kafka.common.security.ssl.DefaultSslEngineFactory$FileBasedStore: java.lang.Long lastModifiedMs(java.lang.String)>
Kerberos return code could not be determined from {} due to {},trace,<org.apache.kafka.common.security.kerberos.KerberosError: org.apache.kafka.common.security.kerberos.KerberosError fromException(java.lang.Exception)>
"{}, resetting offset",info,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleOffsetOutOfRange(org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.TopicPartition)>"
"{}, raising error to the application since no reset policy is configured",info,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleOffsetOutOfRange(org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.common.TopicPartition)>"
Error executing interceptor onConsume callback,warn,<org.apache.kafka.clients.consumer.internals.ConsumerInterceptors: org.apache.kafka.clients.consumer.ConsumerRecords onConsume(org.apache.kafka.clients.consumer.ConsumerRecords)>
"Client requested disconnect from node {}, which is already disconnected",debug,<org.apache.kafka.clients.NetworkClient: void disconnect(java.lang.String)>
Client requested disconnect from node {},info,<org.apache.kafka.clients.NetworkClient: void disconnect(java.lang.String)>
Finished {} with session expiration in {} ms and session re-authentication on or after {} ms,debug,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator$ReauthInfo: void setAuthenticationEndAndSessionReauthenticationTimes(long)>
Finished {} with no session expiration and no session re-authentication,debug,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator$ReauthInfo: void setAuthenticationEndAndSessionReauthenticationTimes(long)>
"System property \'java.security.auth.login.config\' and Kafka SASL property \'sasl.jaas.config\' are not set, using default JAAS configuration.",debug,"<org.apache.kafka.common.security.JaasContext: org.apache.kafka.common.security.JaasContext defaultContext(org.apache.kafka.common.security.JaasContext$Type,java.lang.String,java.lang.String)>"
"System property \'java.security.auth.login.config\' is not set, using default JAAS configuration.",debug,"<org.apache.kafka.common.security.JaasContext: org.apache.kafka.common.security.JaasContext defaultContext(org.apache.kafka.common.security.JaasContext$Type,java.lang.String,java.lang.String)>"
Sending %%x response to server after receiving an error: {},debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClient: byte[] evaluateChallenge(byte[])>
Successfully authenticated as {},debug,<org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerSaslClient: byte[] evaluateChallenge(byte[])>
Creators provided through security.providers are expected to be sub-classes of SecurityProviderCreator,error,<org.apache.kafka.common.utils.SecurityUtils: void addConfiguredSecurityProviders(java.util.Map)>
Unrecognized security provider creator class,error,<org.apache.kafka.common.utils.SecurityUtils: void addConfiguredSecurityProviders(java.util.Map)>
Unexpected implementation of security provider creator class,error,<org.apache.kafka.common.utils.SecurityUtils: void addConfiguredSecurityProviders(java.util.Map)>
Clearing cached controller node {}.,trace,<org.apache.kafka.clients.admin.internals.AdminMetadataManager: void clearController()>
Unsubscribed all topics or patterns and assigned partitions,info,<org.apache.kafka.clients.consumer.KafkaConsumer: void unsubscribe()>
handleInput - starting post for {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: void handleInput(java.net.HttpURLConnection,java.util.Map,java.lang.String,java.lang.Integer,java.lang.Integer)>"
handleInput - preparing to connect to {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: void handleInput(java.net.HttpURLConnection,java.util.Map,java.lang.String,java.lang.Integer,java.lang.Integer)>"
handleInput - preparing to write request body to {},debug,"<org.apache.kafka.common.security.oauthbearer.secured.HttpAccessTokenRetriever: void handleInput(java.net.HttpURLConnection,java.util.Map,java.lang.String,java.lang.Integer,java.lang.Integer)>"
Skipping return offset for {} due to error {}.,warn,"<org.apache.kafka.clients.admin.internals.ListConsumerGroupOffsetsHandler: void lambda$handleResponse$0(java.util.Map,org.apache.kafka.common.TopicPartition,org.apache.kafka.common.requests.OffsetFetchResponse$PartitionData)>"
The config {} in the response from broker {} is not in the request,warn,<org.apache.kafka.clients.admin.KafkaAdminClient$11: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
The config {} in the response from the least loaded broker is not in the request,warn,<org.apache.kafka.clients.admin.KafkaAdminClient$11: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
allocated buffer of size {} ,trace,<org.apache.kafka.common.memory.SimpleMemoryPool: void bufferToBeReturned(java.nio.ByteBuffer)>
Handling v ListOffsetResponse response for {}. Fetched offset {},debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}",debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Cannot search by timestamp for partition {} because the message format version is before ..,debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Attempt to fetch offsets for partition {} failed due to {}, retrying.",debug,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Received unknown topic or partition error in ListOffset request for partition {},warn,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Attempt to fetch offsets for partition {} failed due to unexpected exception: {}, retrying.",warn,"<org.apache.kafka.clients.consumer.internals.Fetcher: void handleListOffsetResponse(org.apache.kafka.common.requests.ListOffsetsResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
message,trace,<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void trace(java.lang.String)>
"ValidationChain: {}, {}",fine,"<jdk.internal.event.EventHelper: void logX509ValidationEvent(int,int[])>"
message,trace,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void trace(java.lang.String,java.lang.Object)>"
message,info,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void info(java.lang.String,java.lang.Object,java.lang.Object)>"
Broker low on memory - could not allocate buffer of size {} for source {},trace,<org.apache.kafka.common.network.NetworkReceive: long readFrom(java.nio.channels.ScatteringByteChannel)>
"Response included transactionalId `{}`, which was not requested",warn,"<org.apache.kafka.clients.admin.internals.DescribeTransactionsHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleResponse(org.apache.kafka.common.Node,java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
Metadata request for topic {} returned topic-level error {}. Will retry,debug,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handleTopicError(java.lang.String,org.apache.kafka.common.protocol.Errors,java.util.Set,java.util.Map)>"
Received authorization failure for topic {} in `Metadata` response,error,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handleTopicError(java.lang.String,org.apache.kafka.common.protocol.Errors,java.util.Set,java.util.Map)>"
Received invalid topic error for topic {} in `Metadata` response,error,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handleTopicError(java.lang.String,org.apache.kafka.common.protocol.Errors,java.util.Set,java.util.Map)>"
Received unexpected error for topic {} in `Metadata` response,error,"<org.apache.kafka.clients.admin.internals.PartitionLeaderStrategy: void handleTopicError(java.lang.String,org.apache.kafka.common.protocol.Errors,java.util.Set,java.util.Map)>"
The following subscribed topics are not assigned to any members: {} ,warn,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeUpdateGroupSubscription(java.lang.String,java.util.Map,java.util.Set)>"
"The following not-subscribed topics are assigned, and their metadata will be fetched from the brokers: {}",info,"<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void maybeUpdateGroupSubscription(java.lang.String,java.util.Map,java.util.Set)>"
Closing the Kafka producer with timeoutMillis = {} ms.,info,"<org.apache.kafka.clients.producer.KafkaProducer: void close(java.time.Duration,boolean)>"
Overriding close timeout {} ms to  ms in order to prevent useless blocking due to self-join. This means you have incorrectly invoked close with a non-zero timeout from the producer call-back.,warn,"<org.apache.kafka.clients.producer.KafkaProducer: void close(java.time.Duration,boolean)>"
Interrupted while joining ioThread,error,"<org.apache.kafka.clients.producer.KafkaProducer: void close(java.time.Duration,boolean)>"
Proceeding to force close the producer since pending requests could not be completed within timeout {} ms.,info,"<org.apache.kafka.clients.producer.KafkaProducer: void close(java.time.Duration,boolean)>"
Kafka producer has been closed,debug,"<org.apache.kafka.clients.producer.KafkaProducer: void close(java.time.Duration,boolean)>"
Checking login config for Zookeeper JAAS context {},debug,<org.apache.kafka.common.security.JaasUtils: boolean isZkSaslEnabled()>
"JAAS configuration is present, but system property zookeeper.sasl.client is set to false, which disables SASL in the ZooKeeper client",error,<org.apache.kafka.common.security.JaasUtils: boolean isZkSaslEnabled()>
Idempotence will be disabled because {} is set to .,info,<org.apache.kafka.clients.producer.ProducerConfig: void postProcessAndValidateIdempotenceConfigs(java.util.Map)>
"Idempotence will be disabled because {} is set to {}, not set to \'all\'.",info,<org.apache.kafka.clients.producer.ProducerConfig: void postProcessAndValidateIdempotenceConfigs(java.util.Map)>
"Idempotence will be disabled because {} is set to {}, which is greater than . Please note that in v.. and onward, this will become an error.",warn,<org.apache.kafka.clients.producer.ProducerConfig: void postProcessAndValidateIdempotenceConfigs(java.util.Map)>
The partition {} in the response from broker {} is not in the request,warn,<org.apache.kafka.clients.admin.KafkaAdminClient$14: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Timed out {} call(s) with assigned nodes.,debug,<org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable: int timeoutCallsToSend(org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor)>
Terminating process due to signal {},info,"<org.apache.kafka.common.utils.LoggingSignalHandler$1: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
About to close the idle connection from {} due to being idle for {} millis,trace,<org.apache.kafka.common.network.Selector: void maybeCloseOldestConnection(long)>
The path {} is not a directory,warn,"<org.apache.kafka.common.config.provider.DirectoryConfigProvider: org.apache.kafka.common.config.ConfigData get(java.lang.String,java.util.function.Predicate)>"
Could not list directory {},error,"<org.apache.kafka.common.config.provider.DirectoryConfigProvider: org.apache.kafka.common.config.ConfigData get(java.lang.String,java.util.function.Predicate)>"
message,debug,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void debug(java.lang.String,java.lang.Object)>"
Sending metadata request {} to node {},debug,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: long maybeUpdate(long,org.apache.kafka.common.Node)>"
Initialize connection to node {} for sending metadata request,debug,"<org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater: long maybeUpdate(long,org.apache.kafka.common.Node)>"
Successfully logged in.,info,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: javax.security.auth.login.LoginContext login()>
No Expiring Credential,debug,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: javax.security.auth.login.LoginContext login()>
"Principal={}: Current clock: {} is later than expiry {}. This may indicate a clock skew problem. Check that this host\'s and remote host\'s clocks are in sync. Not starting refresh thread. This process is likely unable to authenticate SASL connections (for example, it is unlikely to be able to authenticate a connection with a Kafka Broker).",error,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: javax.security.auth.login.LoginContext login()>
Principal={}: It is an expiring credential,debug,<org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin: javax.security.auth.login.LoginContext login()>
Subscribed to partition(s): {},info,<org.apache.kafka.clients.consumer.KafkaConsumer: void assign(java.util.Collection)>
Topic authorization failed for topics {},error,<org.apache.kafka.clients.Metadata: void checkUnauthorizedTopics(org.apache.kafka.common.Cluster)>
message,debug,<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void debug(java.lang.String)>
Executing onLeavePrepare with generation {},debug,<org.apache.kafka.clients.consumer.internals.ConsumerCoordinator: void onLeavePrepare()>
"Invalid SASL mechanism response, server may be expecting only GSSAPI tokens",debug,<org.apache.kafka.common.security.authenticator.SaslClientAuthenticator: org.apache.kafka.common.requests.AbstractResponse receiveKafkaResponse()>
Using SASL mechanism \'{}\' provided by client,debug,"<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: java.lang.String handleHandshakeRequest(org.apache.kafka.common.requests.RequestContext,org.apache.kafka.common.requests.SaslHandshakeRequest)>"
SASL mechanism \'{}\' requested by client is not supported,debug,"<org.apache.kafka.common.security.authenticator.SaslServerAuthenticator: java.lang.String handleHandshakeRequest(org.apache.kafka.common.requests.RequestContext,org.apache.kafka.common.requests.SaslHandshakeRequest)>"
throttleTimeMs,<init>,"<org.apache.kafka.common.requests.AlterReplicaLogDirsRequest: org.apache.kafka.common.requests.AlterReplicaLogDirsResponse getErrorResponse(int,java.lang.Throwable)>"
Resetting generation {}due to: {},info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void resetStateAndGeneration(java.lang.String,boolean)>"
Seeking to {} offset of partition {},info,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: void lambda$requestOffsetReset$3(org.apache.kafka.clients.consumer.OffsetResetStrategy,org.apache.kafka.common.TopicPartition)>"
snapshot_id,<init>,<org.apache.kafka.common.message.FetchResponseData$PartitionData: void <clinit>()>
An internal error occurred while retrieving token from callback handler,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyToken()>
Login failed: {} : {} (URI={}),info,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule: void identifyToken()>
Skipping reset of partition {} since it is no longer assigned,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: void maybeSeekUnvalidated(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.clients.consumer.OffsetResetStrategy)>"
Skipping reset of partition {} since reset is no longer needed,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: void maybeSeekUnvalidated(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.clients.consumer.OffsetResetStrategy)>"
Skipping reset of partition {} since an alternative reset has been requested,debug,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: void maybeSeekUnvalidated(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.clients.consumer.OffsetResetStrategy)>"
Resetting offset for partition {} to position {}.,info,"<org.apache.kafka.clients.consumer.internals.SubscriptionState: void maybeSeekUnvalidated(org.apache.kafka.common.TopicPartition,org.apache.kafka.clients.consumer.internals.SubscriptionState$FetchPosition,org.apache.kafka.clients.consumer.OffsetResetStrategy)>"
Requesting metadata update for partition {} of topic {}.,trace,"<org.apache.kafka.clients.producer.KafkaProducer: org.apache.kafka.clients.producer.KafkaProducer$ClusterAndWaitTime waitOnMetadata(java.lang.String,java.lang.Integer,long,long)>"
Requesting metadata update for topic {}.,trace,"<org.apache.kafka.clients.producer.KafkaProducer: org.apache.kafka.clients.producer.KafkaProducer$ClusterAndWaitTime waitOnMetadata(java.lang.String,java.lang.Integer,long,long)>"
Cluster ID: {},info,"<org.apache.kafka.clients.Metadata: void update(int,org.apache.kafka.common.requests.MetadataResponse,boolean,long)>"
Updated cluster metadata updateVersion {} to {},debug,"<org.apache.kafka.clients.Metadata: void update(int,org.apache.kafka.common.requests.MetadataResponse,boolean,long)>"
The `ListTransactions` request sent to broker {} failed because the coordinator is still loading state. Will try again after backing off,debug,"<org.apache.kafka.clients.admin.internals.ListTransactionsHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleResponse(org.apache.kafka.common.Node,java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
The `ListTransactions` request sent to broker {} failed because the coordinator is shutting down,debug,"<org.apache.kafka.clients.admin.internals.ListTransactionsHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleResponse(org.apache.kafka.common.Node,java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
The `ListTransactions` request sent to broker {} failed because of an unexpected error {},error,"<org.apache.kafka.clients.admin.internals.ListTransactionsHandler: org.apache.kafka.clients.admin.internals.AdminApiHandler$ApiResult handleResponse(org.apache.kafka.common.Node,java.util.Set,org.apache.kafka.common.requests.AbstractResponse)>"
Attempting to append record {} with callback {} to topic {} partition {},trace,"<org.apache.kafka.clients.producer.KafkaProducer: java.util.concurrent.Future doSend(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.clients.producer.Callback)>"
Retrying append due to new batch creation for topic {} partition {}. The old partition was {},trace,"<org.apache.kafka.clients.producer.KafkaProducer: java.util.concurrent.Future doSend(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.clients.producer.Callback)>"
Waking up the sender since topic {} partition {} is either full or getting a new batch,trace,"<org.apache.kafka.clients.producer.KafkaProducer: java.util.concurrent.Future doSend(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.clients.producer.Callback)>"
Exception occurred during message send:,debug,"<org.apache.kafka.clients.producer.KafkaProducer: java.util.concurrent.Future doSend(org.apache.kafka.clients.producer.ProducerRecord,org.apache.kafka.clients.producer.Callback)>"
Error getting JMX attribute \'{}\',warn,<org.apache.kafka.common.metrics.JmxReporter$KafkaMbean: javax.management.AttributeList getAttributes(java.lang.String[])>
Login module not specified in JAAS config,<init>,"<org.apache.kafka.common.security.JaasConfig: void <init>(java.lang.String,java.lang.String)>"
Exception handling close on authentication failure node {},error,<org.apache.kafka.common.network.Selector: void handleCloseOnAuthenticationFailure(org.apache.kafka.common.network.KafkaChannel)>
message,trace,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void trace(java.lang.String,java.lang.Object[])>"
Received successful Heartbeat response,debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler: void handle(org.apache.kafka.common.requests.HeartbeatResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Attempt to heartbeat failed since coordinator {} is either not started or not valid,info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler: void handle(org.apache.kafka.common.requests.HeartbeatResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
Ignoring heartbeat response with error {} during {} state,debug,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler: void handle(org.apache.kafka.common.requests.HeartbeatResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Attempt to heartbeat with {} and group instance id {} failed due to {}, resetting generation",info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler: void handle(org.apache.kafka.common.requests.HeartbeatResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
"Attempt to heartbeat with stale {} and group instance id {} failed due to {}, ignoring the error",info,"<org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatResponseHandler: void handle(org.apache.kafka.common.requests.HeartbeatResponse,org.apache.kafka.clients.consumer.internals.RequestFuture)>"
No broker available to send FindCoordinator request,debug,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: org.apache.kafka.clients.consumer.internals.RequestFuture lookupCoordinator()>
"Error when registering metric on , <*>, ",error,<org.apache.kafka.common.metrics.Metrics: void registerMetric(org.apache.kafka.common.metrics.KafkaMetric)>
Registered metric named {},trace,<org.apache.kafka.common.metrics.Metrics: void registerMetric(org.apache.kafka.common.metrics.KafkaMetric)>
Server response mentioned unknown topic {},warn,<org.apache.kafka.clients.admin.KafkaAdminClient$17: void handleResponse(org.apache.kafka.common.requests.AbstractResponse)>
Server response mentioned unknown user {},warn,"<org.apache.kafka.clients.admin.KafkaAdminClient$32: void lambda$handleResponse$7(java.util.Map,org.apache.kafka.common.message.AlterUserScramCredentialsResponseData$AlterUserScramCredentialsResult)>"
NOT_LOGGED_IN,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule$LoginState: void <clinit>()>
LOGGED_IN_NOT_COMMITTED,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule$LoginState: void <clinit>()>
COMMITTED,<init>,<org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule$LoginState: void <clinit>()>
"dirs, ",<init>,"<org.apache.kafka.common.message.AlterReplicaLogDirsRequestDataJsonConverter: org.apache.kafka.common.message.AlterReplicaLogDirsRequestData read(com.fasterxml.jackson.databind.JsonNode,short)>"
Principal={}: Not attempting to re-login since the last re-login was attempted less than {} seconds before.,warn,<org.apache.kafka.common.security.kerberos.KerberosLogin: boolean hasSufficientTimeElapsed()>
Interrupted while waiting for consumer heartbeat thread to close,warn,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void closeHeartbeatThread()>
ProducerId of partition {} set to {} with epoch {}. Reinitialize sequence at beginning.,debug,<org.apache.kafka.clients.producer.internals.TransactionManager: void maybeUpdateProducerIdAndEpoch(org.apache.kafka.common.TopicPartition)>
Key ID {} was too long to cache,warn,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: boolean maybeExpediteRefresh(java.lang.String)>
"For key ID {}, nextCheckTime: {}, currTime: {}",debug,<org.apache.kafka.common.security.oauthbearer.secured.RefreshingHttpsJwks: boolean maybeExpediteRefresh(java.lang.String)>
Removed sensor with name {},trace,<org.apache.kafka.common.metrics.Metrics: void removeSensor(java.lang.String)>
Seeking to offset {} for partition {},info,"<org.apache.kafka.clients.consumer.KafkaConsumer: void seek(org.apache.kafka.common.TopicPartition,long)>"
Retrying to fetch metadata.,info,"<org.apache.kafka.clients.admin.KafkaAdminClient: void rescheduleMetadataTask(org.apache.kafka.clients.admin.internals.MetadataOperationContext,java.util.function.Supplier)>"
ProducerId: {}; Set last ack\'d sequence number for topic-partition {} to {},debug,"<org.apache.kafka.clients.producer.internals.TransactionManager: void handleCompletedBatch(org.apache.kafka.clients.producer.internals.ProducerBatch,org.apache.kafka.common.requests.ProduceResponse$PartitionResponse)>"
message,error,<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void error(java.lang.String)>
"Close timed out with {} pending requests to coordinator, terminating client connections",warn,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void close(org.apache.kafka.common.utils.Timer)>
"Close timed out with {} pending requests to coordinator, terminating client connections",warn,<org.apache.kafka.clients.consumer.internals.AbstractCoordinator: void close(org.apache.kafka.common.utils.Timer)>
message,error,"<org.apache.kafka.common.utils.LogContext$LocationIgnorantKafkaLogger: void error(java.lang.String,java.lang.Object,java.lang.Object)>"
{} attempting protocol downgrade and then retry.,debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$Call: void fail(long,java.lang.Throwable)>"
{} failed with non-retriable exception after {} attempt(s),debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$Call: void fail(long,java.lang.Throwable)>"
{} failed: {}. Beginning retry #{},debug,"<org.apache.kafka.clients.admin.KafkaAdminClient$Call: void fail(long,java.lang.Throwable)>"
"principal = {} is a super user, allowing operation without checking acls.",debug,<kafka.security.authorizer.AclAuthorizer: boolean isSuperUser(org.apache.kafka.common.security.auth.KafkaPrincipal)>
Unable to update IO metrics,warn,<kafka.metrics.LinuxIoMetricsCollector: boolean updateValues(long)>
Producer state must be empty during log initialization,<init>,<kafka.log.LogLoader: kafka.log.LoadedLogOffsets load()>
max-dirty-percent,newGauge,"<kafka.log.LogCleanerManager: void <init>(scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel)>"
time-since-last-run-ms,newGauge,"<kafka.log.LogCleanerManager: void <init>(scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel)>"
"ReplicaManager broker=, <*>,  , ",logIdent_$eq,"<kafka.server.ReplicaManager: void <init>(kafka.server.KafkaConfig,org.apache.kafka.common.metrics.Metrics,org.apache.kafka.common.utils.Time,kafka.utils.Scheduler,kafka.log.LogManager,kafka.server.QuotaFactory$QuotaManagers,kafka.server.MetadataCache,kafka.server.LogDirFailureChannel,kafka.server.AlterIsrManager,kafka.server.BrokerTopicStats,java.util.concurrent.atomic.AtomicBoolean,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option)>"
kafka-delete-logs,schedule,<kafka.log.LogManager: void deleteLogs()>
kafka-delete-logs,schedule,<kafka.log.LogManager: void deleteLogs()>
"ProducerStateManager partition=, topicPartition,  , ",logIdent_$eq,"<kafka.log.ProducerStateManager: void <init>(org.apache.kafka.common.TopicPartition,java.io.File,int,int,org.apache.kafka.common.utils.Time)>"
"ControllerEventThread controllerId=, <*>,  , ",logIdent_$eq,"<kafka.controller.ControllerEventManager$ControllerEventThread: void <init>(kafka.controller.ControllerEventManager,java.lang.String)>"
"BrokerMetadataListener id=, brokerId,  , ",<init>,"<kafka.server.metadata.BrokerMetadataListener: void <init>(int,org.apache.kafka.common.utils.Time,scala.Option,long,scala.Option)>"
start end,addAbortedTransactions,"<kafka.log.Cleaner: void buildOffsetMap(kafka.log.UnifiedLog,long,long,kafka.log.OffsetMap,kafka.log.CleanerStats)>"
", <*>",options_$eq,<kafka.tools.DumpLogSegments$DumpLogSegmentsOptions: void <init>(java.lang.String[])>
/proc,<init>,<kafka.server.KafkaBroker: void $init$(kafka.server.KafkaBroker)>
"GroupMetadataManager brokerId=, brokerId,  , ",logIdent_$eq,"<kafka.coordinator.group.GroupMetadataManager: void <init>(int,kafka.api.ApiVersion,kafka.coordinator.group.OffsetConfig,kafka.server.ReplicaManager,org.apache.kafka.common.utils.Time,org.apache.kafka.common.metrics.Metrics)>"
"The controller returned fewer results than we , <*>, ",error,"<kafka.server.ConfigAdminManager$: void $anonfun$reassembleIncrementalResponse$2(java.util.IdentityHashMap,scala.collection.immutable.Map,org.apache.kafka.common.message.IncrementalAlterConfigsResponseData,org.apache.kafka.common.message.IncrementalAlterConfigsRequestData$AlterConfigsResource)>"
max-buffer-utilization-percent,newGauge,"<kafka.log.LogCleaner: void <init>(kafka.log.CleanerConfig,scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel,org.apache.kafka.common.utils.Time)>"
cleaner-recopy-percent,newGauge,"<kafka.log.LogCleaner: void <init>(kafka.log.CleanerConfig,scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel,org.apache.kafka.common.utils.Time)>"
max-clean-time-secs,newGauge,"<kafka.log.LogCleaner: void <init>(kafka.log.CleanerConfig,scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel,org.apache.kafka.common.utils.Time)>"
max-compaction-delay-secs,newGauge,"<kafka.log.LogCleaner: void <init>(kafka.log.CleanerConfig,scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel,org.apache.kafka.common.utils.Time)>"
DeadThreadCount,newGauge,"<kafka.log.LogCleaner: void <init>(kafka.log.CleanerConfig,scala.collection.Seq,kafka.utils.Pool,kafka.server.LogDirFailureChannel,org.apache.kafka.common.utils.Time)>"
"Failed to register Mbean , name, ",error,"<kafka.utils.CoreUtils$: boolean registerMBean(java.lang.Object,java.lang.String)>"
kafka-log-retention,schedule,"<kafka.log.LogManager: void startupWithConfigOverrides(kafka.log.LogConfig,scala.collection.Map)>"
kafka-log-flusher,schedule,"<kafka.log.LogManager: void startupWithConfigOverrides(kafka.log.LogConfig,scala.collection.Map)>"
kafka-log-start-offset-checkpoint,schedule,"<kafka.log.LogManager: void startupWithConfigOverrides(kafka.log.LogConfig,scala.collection.Map)>"
kafka-delete-logs,schedule,"<kafka.log.LogManager: void startupWithConfigOverrides(kafka.log.LogConfig,scala.collection.Map)>"
Log cleaner threads should be at least ,<init>,<kafka.log.LogCleaner: void validateReconfiguration(kafka.server.KafkaConfig)>
"<*>, requestThrottleMs",<init>,"<kafka.server.KafkaApis: org.apache.kafka.common.requests.AlterReplicaLogDirsResponse $anonfun$handleAlterReplicaLogDirsRequest$1(scala.collection.Map,int)>"
"Snapshot contained an unknown file version , <*>, ",<init>,<kafka.log.ProducerStateManager$: scala.collection.Iterable readSnapshot(java.io.File)>
"Snapshot is corrupt (CRC is no longer valid). , <*>, ",<init>,<kafka.log.ProducerStateManager$: scala.collection.Iterable readSnapshot(java.io.File)>
"Snapshot failed schema validation: , <*>, ",<init>,<kafka.log.ProducerStateManager$: scala.collection.Iterable readSnapshot(java.io.File)>
<*> <*> <*> <*>  <*> <*> ,add,"<kafka.log.LocalLog: kafka.log.LogSegment $anonfun$roll$2(kafka.log.LocalLog,scala.Option)>"
"Failed config command with args \', <*>, \', ",debug,<kafka.admin.ConfigCommand$: void main(java.lang.String[])>
"Error while executing config command with args \', <*>, \', ",debug,<kafka.admin.ConfigCommand$: void main(java.lang.String[])>
OfflineLogDirectoryCount,removeMetric,<kafka.log.LogManager: void shutdown()>
"BrokerMetadataSnapshotter id=, brokerId,  , ",<init>,"<kafka.server.metadata.BrokerMetadataSnapshotter: void <init>(int,org.apache.kafka.common.utils.Time,scala.Option,kafka.server.metadata.SnapshotWriterBuilder)>"
operation = {} on resource = {} from host = {} is {} based on acl = {},debug,"<kafka.security.authorizer.AclAuthorizer: boolean $anonfun$matchingAclExists$2(kafka.security.authorizer.AclAuthorizer,org.apache.kafka.common.acl.AclOperation,org.apache.kafka.common.resource.ResourcePattern,java.lang.String,org.apache.kafka.common.acl.AclPermissionType,kafka.security.authorizer.AclEntry)>"
Starting consumer...,info,<kafka.tools.ConsumerPerformance$: void main(java.lang.String[])>
"Starting to publish metadata events at offset , <*>, ., ",info,<kafka.server.metadata.BrokerMetadataListener$StartPublishingEvent: void run()>
/log_dir_event_notification,<init>,<kafka.zk.KafkaZkClient: void deleteLogDirEventNotifications(int)>
"Log dir , logDir,  is not found in the config., ",<init>,<kafka.log.LogManager: boolean isLogDirOnline(java.lang.String)>
"Couldn\'t query reassignments through the AdminClient API: , <*>, ",debug,<kafka.admin.TopicCommand$TopicService: scala.collection.Map listAllReassignments(java.util.Set)>
"PartitionStateMachine controllerId=, <*>,  , ",logIdent_$eq,"<kafka.controller.ZkPartitionStateMachine: void <init>(kafka.server.KafkaConfig,kafka.controller.StateChangeLogger,kafka.controller.ControllerContext,kafka.zk.KafkaZkClient,kafka.controller.ControllerBrokerRequestBatch)>"
"ReplicaAlterLogDirsManager on broker , <*>, ",<init>,"<kafka.server.ReplicaAlterLogDirsManager: void <init>(kafka.server.KafkaConfig,kafka.server.ReplicaManager,kafka.server.ReplicationQuotaManager,kafka.server.BrokerTopicStats)>"