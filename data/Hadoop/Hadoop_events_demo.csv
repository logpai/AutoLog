Log,LoggingLevel,Method
"pushMetric failed for , <*>",info,<org.apache.hadoop.metrics.util.MetricsTimeVaryingLong: void pushMetric(org.apache.hadoop.metrics.MetricsRecord)>
"SpanReceiver , <*>,  was loaded successfully., ",info,<org.apache.hadoop.tracing.SpanReceiverHost: void loadSpanReceivers(org.apache.hadoop.conf.Configuration)>
Failed to load SpanReceiver,error,<org.apache.hadoop.tracing.SpanReceiverHost: void loadSpanReceivers(org.apache.hadoop.conf.Configuration)>
Config has been overridden during init,debug,<org.apache.hadoop.service.AbstractService: void serviceInit(org.apache.hadoop.conf.Configuration)>
"flushing segment , segments_, ",debug,<org.apache.hadoop.io.SequenceFile$Sorter$SortPass: int run(boolean)>
"enqueue, logicalTime=, logicalTime, ",debug,"<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: boolean putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)>"
"nodePath,  znode already exists !!, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void createPersistentNode(java.lang.String)>
Unable to determine address of the host-falling back to \localhost\ address,warn,<org.apache.hadoop.net.DNS: java.lang.String resolveLocalHostIPAddress()>
Unable to determine local loopback address of \localhost\ -this system\'s network configuration is unsupported,error,<org.apache.hadoop.net.DNS: java.lang.String resolveLocalHostIPAddress()>
"Unable to parse configuration fs.permissions.umask-mode with value , <*>,  as , <*>_,  umask., ",warn,<org.apache.hadoop.fs.permission.FsPermission: org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration)>
"dfs.umask configuration key is deprecated. Convert to fs.permissions.umask-mode, using octal or symbolic umask specifications.",warn,<org.apache.hadoop.fs.permission.FsPermission: org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration)>
Failed to confirm,debug,<org.apache.hadoop.ha.ZKFailoverController: boolean confirmFormat()>
"this, : NotificationHandler: doing a read on , <*>, ",trace,<org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
"this, : NotificationHandler: got EOF on , <*>, ",trace,<org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
"this, : NotificationHandler: read succeeded on , <*>, ",trace,<org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
"this, : NotificationHandler: setting closed to , true for , <*>, ",trace,<org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
"Requested by , <*>,  at , <*>,  to cede active role., ",info,<org.apache.hadoop.ha.ZKFailoverController: void doCedeActive(int)>
Successfully ensured local node is in standby mode,info,<org.apache.hadoop.ha.ZKFailoverController: void doCedeActive(int)>
"Unable to transition local node to standby: , <*>, ",warn,<org.apache.hadoop.ha.ZKFailoverController: void doCedeActive(int)>
Quitting election but indicating that fencing is necessary,warn,<org.apache.hadoop.ha.ZKFailoverController: void doCedeActive(int)>
"Connection: unable to set socket send buffer size to , <*>, ",warn,"<org.apache.hadoop.ipc.Server$Connection: void <init>(org.apache.hadoop.ipc.Server,java.nio.channels.SocketChannel,long)>"
hadoop logout,debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean logout()>
"Local service , <*>,  entered state: , newState, ",info,<org.apache.hadoop.ha.ZKFailoverController: void setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State)>
"Failed readahead on , <*>, ",warn,<org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl: void run()>
"Loaded IP list of size = , <*>,  from file = , fileName, ",debug,<org.apache.hadoop.util.FileBasedIPList: java.lang.String[] readLines(java.lang.String)>
"Missing ip list file : , fileName, ",debug,<org.apache.hadoop.util.FileBasedIPList: java.lang.String[] readLines(java.lang.String)>
"<*>,  is not writable\n, ",warn,<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: void confChanged(org.apache.hadoop.conf.Configuration)>
"Failed to create , <*>, ",warn,<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: void confChanged(org.apache.hadoop.conf.Configuration)>
"Failed to create , <*>, : , <*>, \n, ",warn,<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: void confChanged(org.apache.hadoop.conf.Configuration)>
"MBean , <*>,  already initialized!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void startMBeans()>
Stacktrace: ,debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void startMBeans()>
"MBean for source , <*>,  registered., ",debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void startMBeans()>
 Creating new Groups object,debug,<org.apache.hadoop.security.Groups: org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)>
"Exception while trying to password for alias , alias, : , <*>, ",warn,"<org.apache.hadoop.security.LdapGroupsMapping: java.lang.String getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>"
"Initialized , <*>, ",debug,<org.apache.hadoop.ipc.metrics.RetryCacheMetrics: void <init>(org.apache.hadoop.ipc.RetryCache)>
"Successfully authorized , <*>, ",debug,<org.apache.hadoop.ipc.Server$Connection: void authorizeConnection()>
"Connection from , this,  for protocol , <*>,  is unauthorized for user , <*>, ",info,<org.apache.hadoop.ipc.Server$Connection: void authorizeConnection()>
"<*>,  metrics system timer already started!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void startTimer()>
"Scheduled snapshot period at , <*>,  second(s)., ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void startTimer()>
"unknown metrics type: , <*>, ",error,<org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase: void createMBeanInfo()>
Unable to get operating system page size.  Guessing .,warn,<org.apache.hadoop.io.nativeio.NativeIO: long getOperatingSystemPageSize()>
"name, :an attempt to override final parameter: , attr, ;  Ignoring., ",warn,"<org.apache.hadoop.conf.Configuration: void loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])>"
"Group mapping impl=, <*>, ; cacheTimeout=, <*>, ; warningDeltaMs=, <*>, ",debug,"<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>"
"Exception in closing , c, ",debug,"<org.apache.hadoop.io.IOUtils: void cleanup(org.apache.commons.logging.Log,java.io.Closeable[])>"
clearing userToGroupsMap cache,info,<org.apache.hadoop.security.Groups: void refresh()>
Error refreshing groups cache,warn,<org.apache.hadoop.security.Groups: void refresh()>
"Initialized cache for UID to User mapping with a cache timeout of , <*>,  seconds., ",info,<org.apache.hadoop.io.nativeio.NativeIO: void ensureInitialized()>
"Returned a decompressor: , <*>, ",debug,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnDecompressor(org.apache.hadoop.io.compress.Decompressor)>
Error reading the error stream,warn,<org.apache.hadoop.util.Shell$1: void run()>
"Automatic failover is not enabled for , <*>, .,  Please ensure that automatic failover is enabled in the , configuration before running the ZK failover controller., ",fatal,<org.apache.hadoop.ha.ZKFailoverController: int run(java.lang.String[])>
"<*>, : initing services, size=, <*>, ",debug,<org.apache.hadoop.service.CompositeService: void serviceInit(org.apache.hadoop.conf.Configuration)>
"PrivilegedActionException as:, this,  cause:, <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: java.lang.Object doAs(java.security.PrivilegedExceptionAction)>
failure to login,debug,<org.apache.hadoop.security.UserGroupInformation: void loginUserFromSubject(javax.security.auth.Subject)>
failure to login,<init>,<org.apache.hadoop.security.UserGroupInformation: void loginUserFromSubject(javax.security.auth.Subject)>
"UGI loginUser:, <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: void loginUserFromSubject(javax.security.auth.Subject)>
"Unknown request source: , <*>, ",warn,<org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB: org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto)>
"<*>, : starting, ",debug,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, : , call,  for RpcKind , <*>, ",debug,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, : skipped , call, ",info,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, : , e_, ",info,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, , call , call, ",warn,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, , call , call, ",info,<org.apache.hadoop.ipc.Server$Handler: void run()>
"Large response size , <*>,  for call , <*>, ",warn,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>,  unexpectedly interrupted, ",info,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>,  caught an exception, ",info,<org.apache.hadoop.ipc.Server$Handler: void run()>
"<*>, : exiting, ",debug,<org.apache.hadoop.ipc.Server$Handler: void run()>
running merge pass,debug,<org.apache.hadoop.io.SequenceFile$Sorter: int mergePass(org.apache.hadoop.fs.Path)>
"timeoutMillis, ms timeout elapsed waiting for an attempt , to become active, ",warn,<org.apache.hadoop.ha.ZKFailoverController: org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord waitForActiveAttempt(int)>
"<*>, : readAndProcess caught InterruptedException, ",info,<org.apache.hadoop.ipc.Server$Listener: void doRead(java.nio.channels.SelectionKey)>
"<*>, : readAndProcess from client , <*>,  threw exception , <*>, , ",info,<org.apache.hadoop.ipc.Server$Listener: void doRead(java.nio.channels.SelectionKey)>
"Trying to load Lzo codec class: , <*>_, ",info,<org.apache.hadoop.io.file.tfile.Compression$Algorithm$1: boolean isSupported()>
"<*>, : starting, ",info,<org.apache.hadoop.ipc.Server$Listener: void run()>
Out of Memory in server select,warn,<org.apache.hadoop.ipc.Server$Listener: void run()>
"Stopping , <*>, ",info,<org.apache.hadoop.ipc.Server$Listener: void run()>
"Unable to start failover controller. Unable to connect to ZooKeeper quorum at , <*>, . Please check the , configured value for , ha.zookeeper.quorum,  and ensure that , ZooKeeper is running., ",fatal,<org.apache.hadoop.ha.ZKFailoverController: int doRun(java.lang.String[])>
Unable to start failover controller. Parent znode does not exist.\nRun with -formatZK flag to initialize ZooKeeper.,fatal,<org.apache.hadoop.ha.ZKFailoverController: int doRun(java.lang.String[])>
"Fencing is not configured for , <*>, .\n, You must configure a fencing method before using automatic , failover., ",fatal,<org.apache.hadoop.ha.ZKFailoverController: int doRun(java.lang.String[])>
"Illegal progress value found, progress is Float.NaN. Progress will be changed to ",debug,<org.apache.hadoop.util.Progress: void set(float)>
"Illegal progress value found, progress is Float.NEGATIVE_INFINITY. Progress will be changed to ",debug,<org.apache.hadoop.util.Progress: void set(float)>
"Illegal progress value found, progress is less than . Progress will be changed to ",debug,<org.apache.hadoop.util.Progress: void set(float)>
"Illegal progress value found, progress is larger than . Progress will be changed to ",debug,<org.apache.hadoop.util.Progress: void set(float)>
"Illegal progress value found, progress is Float.POSITIVE_INFINITY. Progress will be changed to ",debug,<org.apache.hadoop.util.Progress: void set(float)>
"setting conf tokensFile: , <*>, ",debug,"<org.apache.hadoop.util.GenericOptionsParser: void processGeneralOptions(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.CommandLine)>"
Aborted,fatal,<org.apache.hadoop.ha.HAAdmin: int runCmd(java.lang.String[])>
Unable to create SSH session,warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"Connecting to , <*>, ..., ",info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"Unable to connect to , <*>,  as user , <*>, ",warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"Connected to , <*>, ",info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
Unable to achieve fencing on remote host,warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"add to , mapName, map:, <*>,  id:, <*>, ",debug,"<org.apache.hadoop.security.ShellBasedIdMapping: void updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>"
"Updated , mapName,  map size: , <*>, ",info,"<org.apache.hadoop.security.ShellBasedIdMapping: void updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>"
Can\'t close BufferedReader of command result,error,"<org.apache.hadoop.security.ShellBasedIdMapping: void updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>"
"Can\'t update , mapName,  map, ",error,"<org.apache.hadoop.security.ShellBasedIdMapping: void updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>"
Can\'t close BufferedReader of command result,error,"<org.apache.hadoop.security.ShellBasedIdMapping: void updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>"
No login principals found!,<init>,"<org.apache.hadoop.security.UserGroupInformation: org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String,java.lang.String)>"
"found more than one principal in the ticket cache file , ticketCache, ",warn,"<org.apache.hadoop.security.UserGroupInformation: org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String,java.lang.String)>"
"Adding a new node: , <*>, ",info,<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void add(org.apache.hadoop.net.Node)>
"NetworkTopology became:\n, <*>, ",debug,<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void add(org.apache.hadoop.net.Node)>
"Received SASL message , <*>, ",debug,"<org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream,java.io.OutputStream)>"
"SASL server GSSAPI callback: setting canonicalized client ID: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
"Regular expression \', <*>, \' for property \', name, \' not valid. Using default, ",warn,"<org.apache.hadoop.conf.Configuration: java.util.regex.Pattern getPattern(java.lang.String,java.util.regex.Pattern)>"
interrupted waiting to send rpc request to server,warn,"<org.apache.hadoop.ipc.Client: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)>"
"Illegal value: there is no element in \, s, \., ",warn,<org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry: org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry parseCommaSeparatedString(java.lang.String)>
"Illegal value: the number of elements in \, s, \ is , <*>,  but an even number of elements is expected., ",warn,<org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry: org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry parseCommaSeparatedString(java.lang.String)>
"Unexpected data length , dataLength, !! from , <*>, ",warn,<org.apache.hadoop.ipc.Server$Connection: void checkDataLength(int)>
"Requested data length , dataLength,  is longer than maximum configured RPC length , <*>, .  RPC came from , <*>, ",warn,<org.apache.hadoop.ipc.Server$Connection: void checkDataLength(int)>
shutdown error: ,error,<org.apache.hadoop.net.unix.DomainSocket: void close()>
"pushMetric failed for , <*>, \n, ",info,<org.apache.hadoop.metrics.util.MetricsIntValue: void pushMetric(org.apache.hadoop.metrics.MetricsRecord)>
"Creating password for identifier: , identifier, , currentKey: , <*>, ",info,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: byte[] createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
Could not store token !!,error,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: byte[] createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
"Cannot initialize JVM Metrics with processName=, processName, , sessionId=, sessionId,  - already initialized, ",info,"<org.apache.hadoop.metrics.jvm.JvmMetrics: org.apache.hadoop.metrics.jvm.JvmMetrics init(java.lang.String,java.lang.String,java.lang.String)>"
"Initializing JVM Metrics with processName=, processName, , sessionId=, sessionId, ",info,"<org.apache.hadoop.metrics.jvm.JvmMetrics: org.apache.hadoop.metrics.jvm.JvmMetrics init(java.lang.String,java.lang.String,java.lang.String)>"
"multipleLinearRandomRetry = , <*>, ",debug,"<org.apache.hadoop.io.retry.RetryUtils: org.apache.hadoop.io.retry.RetryPolicy getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.Class)>"
"Can\'t find user name for uid , uid, . Use default user name , unknown, ",warn,"<org.apache.hadoop.security.ShellBasedIdMapping: java.lang.String getUserName(int,java.lang.String)>"
Received ping message,debug,"<org.apache.hadoop.ipc.Server$Connection: void processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,java.io.DataInputStream)>"
"Cannot find class for token kind , kind, ",warn,<org.apache.hadoop.security.token.Token: java.lang.Class getClassForIdentifier(org.apache.hadoop.io.Text)>
Encountered exception ,warn,<org.apache.hadoop.fs.HarFileSystem$HarMetaData: void parseMetaData()>
"<*>,  metrics system already started!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void start()>
"<*>,  metrics system started, ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void start()>
"wrapping token of length:, len, ",debug,"<org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream: void write(byte[],int,int)>"
"Error invoking method , <*>, ",error,"<org.apache.hadoop.metrics2.lib.MethodMetric$1: void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)>"
"Entering neutral mode for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void enterNeutralMode()>
"Get token info proto:, <*>,  info:, <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.security.token.Token getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
"Removing ZKDTSMDelegationToken_, <*>, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
"Attempted to remove a non-existing znode , <*>, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
does not have a token for renewal,error,<org.apache.hadoop.fs.DelegationTokenRenewer: org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction addRenewAction(org.apache.hadoop.fs.FileSystem)>
====== Beginning Service Fencing Process... ======,info,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"Trying method , i_, /, <*>, : , method, ",info,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"====== Fencing successful by method , method,  ======, ",info,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"Fencing method , method,  misconfigured, ",error,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"Fencing method , method,  failed with an unexpected error., ",error,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"Fencing method , method,  was unsuccessful., ",warn,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
Unable to fence service by any configured method.,error,<org.apache.hadoop.ha.NodeFencer: boolean fence(org.apache.hadoop.ha.HAServiceTarget)>
"Can\'t map user , user, . Use its string hashcode:, <*>, ",info,<org.apache.hadoop.security.ShellBasedIdMapping: int getUidAllowingUnknown(java.lang.String)>
"Loading class: , name, ",debug,"<org.apache.hadoop.util.ApplicationClassLoader: java.lang.Class loadClass(java.lang.String,boolean)>"
"Loaded class: , name,  , ",debug,"<org.apache.hadoop.util.ApplicationClassLoader: java.lang.Class loadClass(java.lang.String,boolean)>"
"Loaded class from parent: , name,  , ",debug,"<org.apache.hadoop.util.ApplicationClassLoader: java.lang.Class loadClass(java.lang.String,boolean)>"
"Error: can\'t add leaf node , <*>,  at depth , newDepth,  to topology:\n, <*>, ",error,<org.apache.hadoop.net.NetworkTopology: void add(org.apache.hadoop.net.Node)>
"Failed to add , <*>, : You cannot have a rack and a non-rack node at the same , level of the network topology., ",<init>,<org.apache.hadoop.net.NetworkTopology: void add(org.apache.hadoop.net.Node)>
"Adding a new node: , <*>, ",info,<org.apache.hadoop.net.NetworkTopology: void add(org.apache.hadoop.net.Node)>
"NetworkTopology became:\n, <*>, ",debug,<org.apache.hadoop.net.NetworkTopology: void add(org.apache.hadoop.net.Node)>
,warn,<org.apache.hadoop.io.compress.lz4.Lz4Compressor: void <clinit>()>
"Cannot load , <*>,  without native hadoop library!, ",error,<org.apache.hadoop.io.compress.lz4.Lz4Compressor: void <clinit>()>
The server is stopped.,warn,<org.apache.hadoop.conf.ReconfigurableBase: void startReconfigurationTask()>
Another reconfiguration task is running.,warn,<org.apache.hadoop.conf.ReconfigurableBase: void startReconfigurationTask()>
"Initialized , <*>, ",debug,"<org.apache.hadoop.ipc.metrics.RpcMetrics: void <init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)>"
"Got brand-new compressor , <*>, , ",info,"<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)>"
Got recycled compressor,debug,"<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)>"
"Entry to state , <*>,  for , <*>, ",info,<org.apache.hadoop.service.LoggingStateChangeListener: void stateChanged(org.apache.hadoop.service.Service)>
"IdentityProvider not specified, defaulting to UserIdentityProvider",info,"<org.apache.hadoop.ipc.DecayRpcScheduler: org.apache.hadoop.ipc.IdentityProvider parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)>"
Shutting down all AsyncDiskService threads...,info,<org.apache.hadoop.util.AsyncDiskService: void shutdown()>
Metric was emitted with no name.,warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Metric name , name,  was emitted with a null value., ",warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Metric name , name, , value , value,  has no type., ",warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Emitting metric , name, , type , type, , value , value, , slope , <*>,  from hostname , <*>, ",debug,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
Error,debug,<org.apache.hadoop.fs.FsShell: int run(java.lang.String[])>
null file argument.,warn,"<org.apache.hadoop.fs.FileUtil: boolean deleteImpl(java.io.File,boolean)>"
"Failed to delete file or dir , <*>, : it still exists., ",warn,"<org.apache.hadoop.fs.FileUtil: boolean deleteImpl(java.io.File,boolean)>"
Reinit compressor with new compression configuration,debug,<org.apache.hadoop.io.compress.zlib.ZlibCompressor: void reinit(org.apache.hadoop.conf.Configuration)>
"Service , <*>,  is started, ",debug,<org.apache.hadoop.service.AbstractService: void start()>
"getting attribute , <*>,  of , oname,  threw an exception, ",debug,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"getting attribute , <*>,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"getting attribute , <*>,  of , oname,  threw an exception, ",debug,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"getting attribute , <*>,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"getting attribute , <*>,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"getting attribute , <*>,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>"
"Null token ignored for , alias, ",warn,"<org.apache.hadoop.security.Credentials: void addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)>"
"<*>, : , <*>, ",error,"<org.apache.hadoop.ha.FailoverController: void preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)>"
"Service is not ready to become active, but forcing: , <*>, ",warn,"<org.apache.hadoop.ha.FailoverController: void preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)>"
"Current time is , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
"Next refresh is , nextRefresh_, ",debug,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
renewed ticket,debug,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
"No TGT after renewal. Aborting renew thread for , <*>, ",warn,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
Terminating renewal thread,warn,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
"Exception encountered while running the renewal command. Aborting renew thread. , <*>, ",warn,<org.apache.hadoop.security.UserGroupInformation$1: void run()>
"Unregistering , mbeanName, ",debug,<org.apache.hadoop.metrics2.util.MBeans: void unregister(javax.management.ObjectName)>
Stacktrace: ,debug,<org.apache.hadoop.metrics2.util.MBeans: void unregister(javax.management.ObjectName)>
"Error unregistering , mbeanName, ",warn,<org.apache.hadoop.metrics2.util.MBeans: void unregister(javax.management.ObjectName)>
"http://, <*>, /logLevel?log=, <*>, ",process,<org.apache.hadoop.log.LogLevel: void main(java.lang.String[])>
"http://, <*>, /logLevel?log=, <*>, &level=, <*>, ",process,<org.apache.hadoop.log.LogLevel: void main(java.lang.String[])>
"Could not make , path,  in local directories from , dirsProp, ",warn,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.fs.Path getLocalPath(java.lang.String,java.lang.String)>"
"dirsProp, , index_, =, <*>, ",warn,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.fs.Path getLocalPath(java.lang.String,java.lang.String)>"
"Registered source , name, ",debug,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>"
User configured user account update time is less than  minute. Use  minute instead.,info,"<org.apache.hadoop.security.ShellBasedIdMapping: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"Snapshotted source , <*>, ",debug,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)>"
"Auth failed for , <*>, :, <*>,  (, <*>, ), ",warn,<org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
"SASL server context established. Negotiated QoP is , <*>, ",debug,<org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
"SASL server successfully authenticated client: , <*>, ",debug,<org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
"Auth successful for , <*>, ",info,<org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
"Unable to execute , cmd, ",warn,"<org.apache.hadoop.ha.ShellCommandFencer: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"Launched fencing command \', cmd, \' with , <*>_, ",info,"<org.apache.hadoop.ha.ShellCommandFencer: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"Interrupted while waiting for fencing command: , cmd, ",warn,"<org.apache.hadoop.ha.ShellCommandFencer: boolean tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)>"
"The configured checkpoint interval is , <*>,  minutes.,  Using an interval of , <*>,  minutes that is used for deletion instead, ",info,"<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void <init>(org.apache.hadoop.fs.TrashPolicyDefault,org.apache.hadoop.conf.Configuration,long)>"
"Monitoring active leader for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void monitorActiveStatus()>
Falling back to shell based,info,<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback: void <init>()>
"Group mapping impl=, <*>, ",debug,<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback: void <init>()>
Unexpected attribute suffix,error,<org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase: java.lang.Object getAttribute(java.lang.String)>
"unknown metrics type: , <*>, ",error,<org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase: java.lang.Object getAttribute(java.lang.String)>
"Terminating ZK connection for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void terminateConnection()>
"<*>, : disconnecting client , connection, . Number of active connections: , <*>, ",debug,<org.apache.hadoop.ipc.Server$ConnectionManager: boolean close(org.apache.hadoop.ipc.Server$Connection)>
"Ensuring existence of , <*>, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void ensureParentZNode()>
"Successfully created , <*>,  in ZK., ",info,<org.apache.hadoop.ha.ActiveStandbyElector: void ensureParentZNode()>
"startMonitoring failed: , <*>, ",warn,<org.apache.hadoop.metrics.spi.CompositeContext: void startMonitoring()>
\local\ is a deprecated filesystem name. Use \file:///\ instead.,warn,<org.apache.hadoop.fs.FileSystem: java.lang.String fixName(java.lang.String)>
"\, name, \ is a deprecated filesystem name.,  Use \hdfs://, name, /\ instead., ",warn,<org.apache.hadoop.fs.FileSystem: java.lang.String fixName(java.lang.String)>
"Deleting empty destination and renaming , src,  to , dst, ",debug,"<org.apache.hadoop.fs.RawLocalFileSystem: boolean rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
"Falling through to a copy of , src,  to , dst, ",debug,"<org.apache.hadoop.fs.RawLocalFileSystem: boolean rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
"Setting the includes file to , includesFile, ",info,<org.apache.hadoop.util.HostsFileReader: void setIncludesFile(java.lang.String)>
"SASL server DIGEST-MD callback: setting password for client: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
"SASL server DIGEST-MD callback: setting canonicalized client ID: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
name,writeStringField,"<org.apache.hadoop.log.Log4Json: java.io.Writer toJson(java.io.Writer,java.lang.String,long,java.lang.String,java.lang.String,java.lang.String,org.apache.log4j.spi.ThrowableInformation)>"
"Actual length is , <*>, ",debug,<org.apache.hadoop.security.SaslInputStream: int readMoreData()>
"StatNode result: , rc,  for path: , path,  connectionState: , <*>,  for , this, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)>"
"Received stat error from Zookeeper. code:, <*>, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)>"
Lock monitoring failed because session was lost,warn,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)>"
"Return a compressor: , <*>, ",debug,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnCompressor(org.apache.hadoop.io.compress.Compressor)>
Disk Error Exception: ,warn,"<org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: org.apache.hadoop.fs.Path createPath(java.lang.String,boolean)>"
Couldn\'t disconnect ssh channel,warn,<org.apache.hadoop.ha.SshFenceByTcpPort: void cleanup(com.jcraft.jsch.ChannelExec)>
"Unable to determine hostname for interface , strInterface, ",warn,"<org.apache.hadoop.net.DNS: java.lang.String[] getHosts(java.lang.String,java.lang.String)>"
"Token cancelation requested for identifier: , id, ",info,"<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)>"
"CreateNode result: , rc,  for path: , path,  connectionState: , <*>,   for , this, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>"
"Received create error from Zookeeper. code:, <*>,  for path , path, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>"
"Retrying createNode createRetryCount: , <*>, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>"
Lock acquisition failed because session was lost,warn,"<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>"
"Sending sasl message , message, ",debug,<org.apache.hadoop.ipc.Server$Connection: void doSaslReply(com.google.protobuf.Message)>
Compressor obtained from CodecPool already finished(),warn,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Compressor getCompressor()>
"Got a compressor: , <*>, ",debug,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Compressor getCompressor()>
"Failed to parse \, <*>, \, which is the index , i,  element in \, originalString, \, ",warn,"<org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry: int parsePositiveInt(java.lang.String[],int,java.lang.String)>"
"The value , <*>,  <= : it is parsed from the string \, <*>, \ which is the index , i,  element in \, originalString, \, ",warn,"<org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry: int parsePositiveInt(java.lang.String[],int,java.lang.String)>"
Reinit compressor with new compression configuration,debug,<org.apache.hadoop.io.compress.bzip2.Bzip2Compressor: void reinit(org.apache.hadoop.conf.Configuration)>
"key, : , attr, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: javax.management.AttributeList getAttributes(java.lang.String[])>
Unable to clear zk parent znode,error,"<org.apache.hadoop.ha.ZKFailoverController: int formatZK(boolean,boolean)>"
"Connection timed out: couldn\'t connect to ZooKeeper in , connectionTimeoutMs,  milliseconds, ",error,<org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef: void waitForZKConnectionEvent(int)>
"available bytes: , <*>, ",info,<org.apache.hadoop.io.SequenceFile$Reader: void getCurrentValue(org.apache.hadoop.io.Writable)>
"val,  is a zero-length value, ",debug,<org.apache.hadoop.io.SequenceFile$Reader: void getCurrentValue(org.apache.hadoop.io.Writable)>
"Cannot load , <*>,  without native hadoop library!, ",error,<org.apache.hadoop.io.compress.lz4.Lz4Decompressor: void <clinit>()>
Unexpected SecurityException in Configuration,warn,<org.apache.hadoop.conf.Configuration: java.lang.String substituteVars(java.lang.String)>
"Acquired token , token, ",debug,"<org.apache.hadoop.security.SecurityUtil: void setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)>"
"Failed to get token for service , <*>, ",warn,"<org.apache.hadoop.security.SecurityUtil: void setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)>"
"loaded properties from , fname, ",info,"<org.apache.hadoop.metrics2.impl.MetricsConfig: org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String,java.lang.String[])>"
"Cannot locate configuration: tried , <*>, ",warn,"<org.apache.hadoop.metrics2.impl.MetricsConfig: org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String,java.lang.String[])>"
"Unable to gracefully make , svc,  standby (, <*>, ), ",warn,<org.apache.hadoop.ha.FailoverController: boolean tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)>
"Unable to gracefully make , svc,  standby (unable to connect), ",warn,<org.apache.hadoop.ha.FailoverController: boolean tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)>
"submit readahead: , <*>, ",trace,"<org.apache.hadoop.io.ReadaheadPool: org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)>"
"Platform is not supported:, <*>, . Can\'t update user map and group map and,  \'nobody\' will be used for any user and group., ",error,<org.apache.hadoop.security.ShellBasedIdMapping: void updateMaps()>
"Using \', <*>, \' for static UID/GID mapping..., ",info,<org.apache.hadoop.security.ShellBasedIdMapping: void updateMaps()>
"Not doing static UID/GID mapping because \', <*>, \' does not exist., ",info,<org.apache.hadoop.security.ShellBasedIdMapping: void updateMaps()>
"Incorrect header or version mismatch from , <*>, :, <*>,  got version , $i,  expected version , , ",warn,<org.apache.hadoop.ipc.Server$Connection: int readAndProcess()>
"Created SASL server with mechanism = , <*>, ",debug,"<org.apache.hadoop.security.SaslRpcServer: javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)>"
exception in the cleaner thread but it will continue to run,warn,<org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner: void run()>
INSTANCE,<init>,<org.apache.hadoop.util.SignalLogger: void <clinit>()>
"The cluster does not contain node: , <*>, ",warn,"<org.apache.hadoop.net.NetworkTopology: int getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)>"
"The cluster does not contain node: , <*>, ",warn,"<org.apache.hadoop.net.NetworkTopology: int getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)>"
"Problem opening checksum file: , file, .  Ignoring exception: , ",warn,"<org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker: void <init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)>"
"Retrying connect to server: , <*>, . Already tried , curRetries,  time(s); maxRetries=, maxRetries, ",info,"<org.apache.hadoop.ipc.Client$Connection: void handleConnectionTimeout(int,int,java.io.IOException)>"
"Successfully removed SpanReceiver , spanReceiverId,  with class , <*>, ",info,<org.apache.hadoop.tracing.SpanReceiverHost: void removeSpanReceiver(long)>
"Created trash checkpoint: , <*>, ",info,<org.apache.hadoop.fs.TrashPolicyDefault: void createCheckpoint()>
"Have read input token of size , <*>,  for processing by saslServer.unwrap(), ",debug,<org.apache.hadoop.ipc.Server$Connection: void unwrapPacketAndProcessRpcs(byte[])>
"Ignoring failure to deleteOnExit for path , path, ",info,<org.apache.hadoop.fs.FileSystem: void processDeleteOnExit()>
"Starting , <*>, ",info,<org.apache.hadoop.ipc.Server$Listener$Reader: void run()>
"Error closing read selector in , <*>, ",error,<org.apache.hadoop.ipc.Server$Listener$Reader: void run()>
"Error closing read selector in , <*>, ",error,<org.apache.hadoop.ipc.Server$Listener$Reader: void run()>
"<*>, : stopping services, size=, <*>, ",debug,<org.apache.hadoop.service.CompositeService: void serviceStop()>
Using crypto codec {}.,debug,"<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
Crypto codec {} doesn\'t meet the cipher suite {}.,debug,"<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
Crypto codec {} is not available.,debug,"<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
"Script , <*>,  returned , <*>,  values when , <*>,  were expected., ",error,<org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping: java.util.List resolve(java.util.List)>
Trying to re-establish ZK session,info,<org.apache.hadoop.ha.ActiveStandbyElector: void reJoinElection(int)>
Not joining election since service has not yet been reported as healthy.,info,<org.apache.hadoop.ha.ActiveStandbyElector: void reJoinElection(int)>
"Can\'t create(mkdir) trash directory: , <*>, ",warn,<org.apache.hadoop.fs.TrashPolicyDefault: boolean moveToTrash(org.apache.hadoop.fs.Path)>
"Can\'t create trash directory: , <*>, ",warn,<org.apache.hadoop.fs.TrashPolicyDefault: boolean moveToTrash(org.apache.hadoop.fs.Path)>
"from system property: , <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.impl.MetricsSystemImpl$InitMode initMode()>
"from environment variable: , <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.impl.MetricsSystemImpl$InitMode initMode()>
"Could not parse line \', <*>, \'. Lines should be of , the form \'uid|gid remote id local id\'. Blank lines and , everything following a \'#\' on a line will be ignored., ",warn,<org.apache.hadoop.security.ShellBasedIdMapping: org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping parseStaticMap(java.io.File)>
Updating the current master key for generating delegation tokens,info,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: void updateCurrentKey()>
"<*>,  unexpectedly interrupted, ",info,<org.apache.hadoop.ipc.Server$Listener$Reader: void doRunLoop()>
Error in Reader,error,<org.apache.hadoop.ipc.Server$Listener$Reader: void doRunLoop()>
"thread,  terminating on unexpected exception, ",error,"<org.apache.hadoop.net.unix.DomainSocketWatcher$1: void uncaughtException(java.lang.Thread,java.lang.Throwable)>"
No crypto codec classes with cipher suite configured.,debug,"<org.apache.hadoop.crypto.CryptoCodec: java.util.List getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
Class {} is not a CryptoCodec.,debug,"<org.apache.hadoop.crypto.CryptoCodec: java.util.List getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
Crypto codec {} not found.,debug,"<org.apache.hadoop.crypto.CryptoCodec: java.util.List getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>"
Updating attr cache...,debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: int updateAttrCache()>
"Done. # tags & metrics=, numMetrics_, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: int updateAttrCache()>
"Address change detected. Old: , <*>,  New: , <*>, ",warn,<org.apache.hadoop.ipc.Client$Connection: boolean updateAddress()>
"pushMetric failed for , <*>, \n, ",info,<org.apache.hadoop.metrics.util.MetricsTimeVaryingRate: void pushMetric(org.apache.hadoop.metrics.MetricsRecord)>
"Caught exception in callback , <*>, ",warn,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"Unable to make , toSvc,  active (, <*>, ). Failing back., ",error,"<org.apache.hadoop.ha.FailoverController: void failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)>"
"Unable to make , toSvc,  active (unable to connect). Failing back., ",error,"<org.apache.hadoop.ha.FailoverController: void failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)>"
"<*>, . Failback to , fromSvc,  failed (, <*>, ), ",fatal,"<org.apache.hadoop.ha.FailoverController: void failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)>"
"Attempted to update a non-existing znode , <*>, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)>"
"Jetty bound to port , <*>, ",info,<org.apache.hadoop.http.HttpServer: void start()>
HttpServer.start() threw a non Bind IOException,info,<org.apache.hadoop.http.HttpServer: void start()>
HttpServer.start() threw a MultiException,info,<org.apache.hadoop.http.HttpServer: void start()>
"Using callQueue , backingClass, ",info,"<org.apache.hadoop.ipc.CallQueueManager: void <init>(java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"field , field,  with annotation , annotation, ",debug,"<org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>"
"Writing znode , <*>,  to indicate that the local node is the most recent active..., ",info,<org.apache.hadoop.ha.ActiveStandbyElector: void writeBreadCrumbNode(org.apache.zookeeper.data.Stat)>
Error getting localhost name. Using \'localhost\'...,error,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.String getHostname()>
Unable to initialize NativeIO libraries,debug,<org.apache.hadoop.io.nativeio.NativeIO: void <clinit>()>
"Unable to determine pid for , p,  since it is not a UNIXProcess, ",trace,<org.apache.hadoop.ha.ShellCommandFencer: java.lang.String tryGetPid(java.lang.Process)>
"Unable to determine pid for , p, ",trace,<org.apache.hadoop.ha.ShellCommandFencer: java.lang.String tryGetPid(java.lang.Process)>
"Can\'t open BloomFilter: , <*>,  - fallback to MapFile., ",warn,"<org.apache.hadoop.io.BloomMapFile$Reader: void initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>"
"Error invoking method , <*>, ",error,"<org.apache.hadoop.metrics2.lib.MethodMetric$3: void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)>"
Could not flush Keystore..attempting to reset to previous state !!,debug,<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: void resetKeyStoreState(org.apache.hadoop.fs.Path)>
KeyStore resetting to previously flushed state !!,debug,<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: void resetKeyStoreState(org.apache.hadoop.fs.Path)>
Could not reset Keystore to previous state,debug,<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: void resetKeyStoreState(org.apache.hadoop.fs.Path)>
"Sum of weightages can not be more than .; But sum = , sum_, ",warn,<org.apache.hadoop.util.Progress: org.apache.hadoop.util.Progress addPhase(float)>
MetaData Option is ignored during append,info,"<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])>"
"Have read input token of size , <*>,  for processing by saslServer.evaluateResponse(), ",debug,<org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
"<*>,  got value #, <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void receiveRpcResponse()>
Detailed error code not set by server on rpc error,warn,<org.apache.hadoop.ipc.Client$Connection: void receiveRpcResponse()>
"Removing a node: , <*>, ",info,<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void remove(org.apache.hadoop.net.Node)>
"NetworkTopology became:\n, <*>, ",debug,<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void remove(org.apache.hadoop.net.Node)>
"Error retrieving key , keyId,  from ZK, ",error,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int)>
"<*>, : Unable to pump output from , <*>, ",warn,<org.apache.hadoop.ha.StreamPumper$1: void run()>
"<*>,  responds to \', identifier, \', says: \', <*>, \', returns , <*>, ",info,"<org.apache.hadoop.ipc.RefreshRegistry: java.util.Collection dispatch(java.lang.String,java.lang.String[])>"
"Error accessing field , field,  annotated with, annotation, ",warn,"<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void add(java.lang.Object,java.lang.reflect.Field)>"
"Moving bad file , <*>,  to , <*>, ",warn,"<org.apache.hadoop.fs.LocalFileSystem: boolean reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)>"
Ignoring failure of renameTo,warn,"<org.apache.hadoop.fs.LocalFileSystem: boolean reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)>"
Ignoring failure of renameTo,warn,"<org.apache.hadoop.fs.LocalFileSystem: boolean reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)>"
"Error moving bad file , p, : , <*>, ",warn,"<org.apache.hadoop.fs.LocalFileSystem: boolean reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)>"
"Size of protoMap for , rpcKind,  =, <*>, ",debug,"<org.apache.hadoop.ipc.RPC$Server: org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)>"
"RPC Server\'s Kerberos principal name for protocol=, <*>,  is , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: javax.security.sasl.SaslClient createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
"Creating SASL , <*>, (, <*>, ) ,  client to authenticate to service at , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: javax.security.sasl.SaslClient createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
Updating info cache...,debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void updateInfoCache()>
Done,debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void updateInfoCache()>
failed to load SnappyDecompressor,error,<org.apache.hadoop.io.compress.snappy.SnappyDecompressor: void <clinit>()>
"Invalid value , <*>,  for , net.topology.script.number.args, ; must be >= , <*>, ",warn,"<org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping: java.lang.String runResolveCommand(java.util.List,java.lang.String)>"
"Exception running , <*>, ",warn,"<org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping: java.lang.String runResolveCommand(java.util.List,java.lang.String)>"
"Unable to find JAAS classes:, <*>, ",error,<org.apache.hadoop.security.UserGroupInformation: java.lang.Class getOsPrincipalClass()>
"Unexpected EOF reading , <*>,  at entry #, <*>, .  Ignoring., ",warn,<org.apache.hadoop.io.MapFile$Reader: void readIndex()>
"RECEIVED SIGNAL , <*>, : SIG, <*>, ",error,<org.apache.hadoop.util.SignalLogger$Handler: void handle(sun.misc.Signal)>
"Will send , state,  token of size , <*>_,  from saslServer., ",debug,"<org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])>"
"mkdirs false for , $u, , execution will continue, ",debug,"<org.apache.hadoop.fs.FileUtil: java.lang.String[] createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)>"
"FileSystem.Cache.closeAll() threw an exception:\n, <*>, ",info,<org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer: void run()>
"Use , <*>,  authentication for protocol , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth selectSaslClient(java.util.List)>
,info,<org.apache.hadoop.util.SignalLogger: void register(org.apache.commons.logging.Log)>
loginUserFromKeyTab must be done first,<init>,<org.apache.hadoop.security.UserGroupInformation: void reloginFromKeytab()>
"Initiating logout for , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: void reloginFromKeytab()>
"Initiating re-login for , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: void reloginFromKeytab()>
"Old Queue: , <*>, , , Replacement: , <*>, ",info,"<org.apache.hadoop.ipc.CallQueueManager: void swapQueue(java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)>"
,info,"<org.apache.hadoop.util.ReflectionUtils: void logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)>"
"Trash can\'t list homes: , <*>,  Sleeping., ",warn,<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void run()>
"Trash caught: , <*>, . Skipping , <*>, ., ",warn,<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void run()>
RuntimeException during Trash.Emptier.run(): ,warn,<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void run()>
Trash cannot close FileSystem: ,warn,<org.apache.hadoop.fs.TrashPolicyDefault$Emptier: void run()>
"Added filter , name,  (class=, classname, ) to context , <*>, ",info,"<org.apache.hadoop.http.HttpServer: void addFilter(java.lang.String,java.lang.String,java.util.Map)>"
"Added filter , name,  (class=, classname, ) to context , <*>, ",info,"<org.apache.hadoop.http.HttpServer: void addFilter(java.lang.String,java.lang.String,java.util.Map)>"
"Watcher event type: , <*>,  with state:, <*>,  for path:, <*>,  connectionState: , <*>,  for , this, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
Session connected.,info,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
Session disconnected. Entering neutral mode...,info,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
Session expired. Entering neutral mode and rejoining...,info,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
Successfully authenticated to ZooKeeper using SASL.,info,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
"Unexpected node event: , <*>,  for path: , <*>, ",debug,"<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>"
"Invalid CIDR syntax : , hostEntry, ",warn,"<org.apache.hadoop.util.MachineList: void <init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)>"
"this, : , caller,  starting sendCallback for fd , fd, ",trace,"<org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>"
"this, : , caller, : closing fd , fd,  at the request of the handler., ",trace,"<org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>"
"this, : , caller,  : sendCallback processed fd , fd,  in toRemove., ",trace,"<org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>"
"this, : , caller, : sendCallback not , closing fd , fd, ",trace,"<org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>"
Failed to load OpenSSL Cipher.,debug,<org.apache.hadoop.crypto.OpensslCipher: void <clinit>()>
"property , <*>,  unchanged, ",info,"<org.apache.hadoop.conf.ReconfigurationServlet: void applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)>"
"Authorization failed for , user,  for protocol=, protocol, , expected client Kerberos principal is , clientPrincipal_, ",warn,"<org.apache.hadoop.security.authorize.ServiceAuthorizationManager: void authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)>"
"Authorization successful for , user,  for protocol=, protocol, ",info,"<org.apache.hadoop.security.authorize.ServiceAuthorizationManager: void authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)>"
Failed to locate the winutils binary in the hadoop binary path,error,<org.apache.hadoop.util.Shell: java.lang.String getWinUtilsPath()>
"Fatal error occurred:, err, ",fatal,<org.apache.hadoop.ha.ZKFailoverController: void fatalError(java.lang.String)>
"Unable to use , <*>, .  Falling back to , Java SecureRandom., ",info,<org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
"<*>,  authentication enabled for secret manager, ",debug,"<org.apache.hadoop.ipc.Server: java.util.List getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)>"
"Server accepts auth methods:, <*>, ",debug,"<org.apache.hadoop.ipc.Server: java.util.List getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)>"
"Connecting to , <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)>
"Negotiated QOP is :, <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)>
"The libjars file , <*>,  is not on the local , filesystem. Ignoring., ",warn,<org.apache.hadoop.util.GenericOptionsParser: java.net.URL[] getLibJars(org.apache.hadoop.conf.Configuration)>
"rpcKind=, rpcKind, , rpcRequestWrapperClass=, rpcRequestWrapperClass, , rpcInvoker=, rpcInvoker, ",debug,"<org.apache.hadoop.ipc.Server: void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)>"
"<*>,  has a full queue and can\'t consume the given metrics., ",warn,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: boolean putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
"<*>,  couldn\'t fulfill an immediate putMetrics request in time.,  Abandoning., ",warn,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: boolean putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
Initializing the GangliaContext for Ganglia . metrics.,debug,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void init(java.lang.String,org.apache.hadoop.metrics.ContextFactory)>"
"this, : closing, ",debug,<org.apache.hadoop.net.unix.DomainSocketWatcher: void close()>
" got #, <*>, ",debug,<org.apache.hadoop.ipc.Server$Connection: void processOneRpc(byte[])>
"<*>, : starting services, size=, <*>, ",debug,<org.apache.hadoop.service.CompositeService: void serviceStart()>
Trying to load the custom-built native-hadoop library...,debug,<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
Loaded the native-hadoop library,debug,<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
"Failed to load native-hadoop with error: , <*>, ",debug,<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
"java.library.path=, <*>, ",debug,<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable,warn,<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
"No node in path , <*>, , ",error,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)>"
"Protocol , protocolClass,  NOT registered as cannot get protocol version , ",warn,"<org.apache.hadoop.ipc.RPC$Server: void registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)>"
"RpcKind = , rpcKind,  Protocol Name = , <*>,  version=, <*>,  ProtocolImpl=, <*>,  protocolClass=, <*>, ",debug,"<org.apache.hadoop.ipc.RPC$Server: void registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)>"
"Logout failed while disconnecting, error code - , <*>, ",warn,<org.apache.hadoop.fs.ftp.FTPFileSystem: void disconnect(org.apache.commons.net.ftp.FTPClient)>
Exception in getCurrentUser: ,error,"<org.apache.hadoop.fs.FileContext: void <init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.conf.Configuration)>"
"error looking up the name of group , groupId, : , error, ",error,"<org.apache.hadoop.security.JniBasedUnixGroupsMapping: void logError(int,java.lang.String)>"
Initializing the GangliaSink for Ganglia metrics.,debug,<org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink: void init(org.apache.commons.configuration.SubsetConfiguration)>
"parsing input stream , is, ",debug,"<org.apache.hadoop.conf.Configuration: org.w3c.dom.Document parse(javax.xml.parsers.DocumentBuilder,java.io.InputStream,java.lang.String)>"
"Exiting with status , status, ",info,"<org.apache.hadoop.util.ExitUtil: void terminate(int,java.lang.String)>"
Terminate called,fatal,"<org.apache.hadoop.util.ExitUtil: void terminate(int,java.lang.String)>"
"Unknown rpc kind , <*>,  from client , <*>, ",warn,"<org.apache.hadoop.ipc.Server$Connection: void processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,java.io.DataInputStream)>"
"Unable to read call parameters for client , <*>, on connection protocol , <*>,  for rpcKind , <*>, ",warn,"<org.apache.hadoop.ipc.Server$Connection: void processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,java.io.DataInputStream)>"
"Unable to initialize composite metric , contextName, : could not init arity, ",error,"<org.apache.hadoop.metrics.spi.CompositeContext: void init(java.lang.String,org.apache.hadoop.metrics.ContextFactory)>"
"Unexpected item in trash: , <*>, . Ignoring., ",warn,<org.apache.hadoop.fs.TrashPolicyDefault: void deleteCheckpoint()>
"Deleted trash checkpoint: , <*>, ",info,<org.apache.hadoop.fs.TrashPolicyDefault: void deleteCheckpoint()>
"Couldn\'t delete checkpoint: , <*>,  Ignoring., ",warn,<org.apache.hadoop.fs.TrashPolicyDefault: void deleteCheckpoint()>
"Stopping metrics source , <*>, : class=, <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stopSources()>
Using pure-Java version of bzip library,info,<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
"Successfully loaded & initialized native-bzip library , <*>, ",info,<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
"Failed to load/initialize native-bzip library , <*>, , will use pure-Java version, ",warn,<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
Handling deprecation for all properties in config...,debug,<org.apache.hadoop.conf.Configuration: void handleDeprecation()>
"Handling deprecation for , <*>, ",debug,<org.apache.hadoop.conf.Configuration: void handleDeprecation()>
"Connection is closed, will try to reconnect",warn,<org.apache.hadoop.security.LdapGroupsMapping: java.util.List getGroups(java.lang.String)>
"Exception trying to get groups for user , user, : , <*>, ",warn,<org.apache.hadoop.security.LdapGroupsMapping: java.util.List getGroups(java.lang.String)>
"Connection being closed, reconnecting failed, retryCount = , e__, ",warn,<org.apache.hadoop.security.LdapGroupsMapping: java.util.List getGroups(java.lang.String)>
"Exception trying to get groups for user , user, :, <*>, ",warn,<org.apache.hadoop.security.LdapGroupsMapping: java.util.List getGroups(java.lang.String)>
"Error getting metrics from source , <*>, ",error,"<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: java.lang.Iterable getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)>"
Stopping expired delegation token remover thread,debug,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: void stopThreads()>
"Error creating MBean object name: , <*>, ",warn,"<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName getMBeanName(java.lang.String,java.lang.String)>"
"available bytes: , <*>, ",info,<org.apache.hadoop.io.SequenceFile$Reader: java.lang.Object getCurrentValue(java.lang.Object)>
"<*>,  is a zero-length value, ",debug,<org.apache.hadoop.io.SequenceFile$Reader: java.lang.Object getCurrentValue(java.lang.Object)>
"Potential performance problem: getGroups(user=, user, ) , took , deltaMs,  milliseconds., ",warn,<org.apache.hadoop.security.Groups$GroupCacheLoader: java.util.List fetchGroupList(java.lang.String)>
"addJerseyResourcePackage: packageName=, packageName, , pathSpec=, pathSpec, ",info,"<org.apache.hadoop.http.HttpServer: void addJerseyResourcePackage(java.lang.String,java.lang.String)>"
"getting client out of cache: , client_, ",debug,"<org.apache.hadoop.ipc.ClientCache: org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)>"
"FileUtil#symlink: On Windows+Java, copying file instead of creating a symlink. Copying , target,  -> , linkname, ",warn,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
"Parent directory , <*>,  does not exist., ",warn,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
"FileUtil#symlink failed to copy the file with error: , <*>, ",warn,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
Fail to create symbolic links on Windows. The default security settings in Windows disallow non-elevated administrators and all non-administrators from creating symbolic links. This behavior can be changed in the Local Security Policy management console,warn,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
"Command \', <*>, \' failed , <*>,  with: , <*>, ",warn,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
"Error while create symlink , linkname,  to , target, .,  Exception: , <*>, ",debug,"<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>"
Exception while getting login user,fatal,<org.apache.hadoop.security.SecurityUtil: java.lang.Object doAsLoginUserOrFatal(java.security.PrivilegedAction)>
"<*>, : , <*>, ",info,<org.apache.hadoop.ha.StreamPumper: void pump()>
"<*>, : , <*>, ",warn,<org.apache.hadoop.ha.StreamPumper: void pump()>
"dfs.web.ugi should not be used. Instead, use hadoop.http.staticuser.user.",warn,<org.apache.hadoop.http.lib.StaticUserWebFilter: java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration)>
writer in GraphiteSink is closed!,info,<org.apache.hadoop.metrics2.sink.GraphiteSink: void close()>
socket in GraphiteSink is closed!,info,<org.apache.hadoop.metrics2.sink.GraphiteSink: void close()>
socket in GraphiteSink is closed!,info,<org.apache.hadoop.metrics2.sink.GraphiteSink: void close()>
Could not stop Curator Framework,error,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void stopThreads()>
Failed to detect a valid hadoop home directory,debug,<org.apache.hadoop.util.Shell: java.lang.String checkHadoopHome()>
Unable to obtain hostName,info,<org.apache.hadoop.metrics.MetricsUtil: java.lang.String getHostName()>
,process,"<org.apache.hadoop.log.LogLevel$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
,process,"<org.apache.hadoop.log.LogLevel$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
\n<br /><hr /><h>Get / Set</h>\n<form>Log: <input type=\'text\' size=\'\' name=\'log\' /> <input type=\'submit\' value=\'Get Log Level\' /></form>\n<form>Log: <input type=\'text\' size=\'\' name=\'log\' /> Level: <input type=\'text\' name=\'level\' /> <input type=\'submit\' value=\'Set Log Level\' /></form>,println,"<org.apache.hadoop.log.LogLevel$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"truncating long string: , <*>,  chars, starting with , <*>, ",warn,<org.apache.hadoop.io.UTF8: void set(java.lang.String)>
Falling back to shell based,debug,<org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: void <init>()>
"Group mapping impl=, <*>, ",debug,<org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: void <init>()>
"<*>,  metrics system not yet started!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stop()>
"<*>,  metrics system stopped (again), ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stop()>
"Stopping , <*>,  metrics system..., ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stop()>
"<*>,  metrics system stopped., ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stop()>
"Removing ZKDTSMDelegationKey_, <*>, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
"Attempted to delete a non-existing znode , <*>, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
"<*>,  znode could not be removed!!, ",debug,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Failed to reset renewer,warn,<org.apache.hadoop.fs.DelegationTokenRenewer: void reset()>
"Registered sink , name, ",info,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)>"
"<*>,  not supported by BuiltInZlibDeflater., ",warn,<org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater: void reinit(org.apache.hadoop.conf.Configuration)>
Reinit compressor with new compression configuration,debug,<org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater: void reinit(org.apache.hadoop.conf.Configuration)>
"Stopping server on , <*>, ",info,<org.apache.hadoop.ipc.Server: void stop()>
"Error changing ownership of , item, ",debug,<org.apache.hadoop.fs.FsShellPermissions$Chown: void processPath(org.apache.hadoop.fs.shell.PathData)>
"<*>, , , finalDesc, ",debug,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>"
"Could not read \', <*>, \', , <*>, ",warn,<org.apache.hadoop.util.VersionInfo: void <init>(java.lang.String)>
"Sending sasl message , message, ",debug,"<org.apache.hadoop.security.SaslRpcClient: void sendSaslMessage(java.io.DataOutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>"
"Unable to create metrics context , contextName, ",error,"<org.apache.hadoop.metrics.MetricsUtil: org.apache.hadoop.metrics.MetricsContext getContext(java.lang.String,java.lang.String)>"
interrupted while sleeping,warn,<org.apache.hadoop.util.ThreadUtil: void sleepAtLeastIgnoreInterrupts(long)>
"Added global filter \', name, \' (class=, classname, ), ",info,"<org.apache.hadoop.http.HttpServer: void addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)>"
"Sink , <*>,  started, ",info,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void start()>
topology.script.file.name,<init>,<org.apache.hadoop.conf.Configuration: void <clinit>()>
topology.script.number.args,<init>,<org.apache.hadoop.conf.Configuration: void <clinit>()>
hadoop.configured.node.mapping,<init>,<org.apache.hadoop.conf.Configuration: void <clinit>()>
topology.node.switch.mapping.impl,<init>,<org.apache.hadoop.conf.Configuration: void <clinit>()>
"DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively",warn,<org.apache.hadoop.conf.Configuration: void <clinit>()>
Unable to initialize NativeIO libraries,debug,<org.apache.hadoop.io.nativeio.NativeIO$Windows: void <clinit>()>
"<*>,  KeyStore: , <*>, ",debug,<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
"<*>,  Loaded KeyStore: , <*>, ",debug,<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
"<*>,  TrustStore: , <*>, ",debug,<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
"<*>,  Loaded TrustStore: , <*>, ",debug,<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
"The property \', <*>, \' has not been set, , no TrustStore will be loaded, ",debug,<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
"Caught InterruptedException, returning low priority queue",warn,<org.apache.hadoop.ipc.DecayRpcScheduler: int cachedOrComputedPriorityLevel(java.lang.Object)>
AsyncDiskService awaitTermination timeout.,warn,<org.apache.hadoop.util.AsyncDiskService: boolean awaitTermination(long)>
All AsyncDiskService threads are terminated.,info,<org.apache.hadoop.util.AsyncDiskService: boolean awaitTermination(long)>
"Proceeding with manual HA state management even though\nautomatic failover is enabled for , target, ",warn,<org.apache.hadoop.ha.HAAdmin: boolean checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)>
Ignoring socket shutdown exception,debug,<org.apache.hadoop.ipc.Server$Connection: void close()>
"Ignoring stale result from old client with sessionId , <*>, ",warn,<org.apache.hadoop.ha.ActiveStandbyElector: boolean isStaleClient(java.lang.Object)>
Error while closing the input stream,warn,<org.apache.hadoop.util.Shell: void runCommand()>
Error while closing the error stream,warn,<org.apache.hadoop.util.Shell: void runCommand()>
Error while closing the input stream,warn,<org.apache.hadoop.util.Shell: void runCommand()>
Error while closing the error stream,warn,<org.apache.hadoop.util.Shell: void runCommand()>
Unexpected Exception while clearing selector : ,info,"<org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool: int select(java.nio.channels.SelectableChannel,int,long)>"
Unexpected Exception while clearing selector : ,info,"<org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool: int select(java.nio.channels.SelectableChannel,int,long)>"
Unexpected Exception while clearing selector : ,info,"<org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool: int select(java.nio.channels.SelectableChannel,int,long)>"
"this, : starting with interruptCheckPeriodMs = , <*>, ",debug,<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
"this, : adding fd , <*>, ",trace,<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
"<*>,  thread terminating., ",debug,<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
"<*>,  terminating on InterruptedException, ",info,<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
"<*>,  terminating on exception, ",error,<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
"Initialized cache for IDs to User/Group mapping with a  cache timeout of , <*>,  seconds., ",debug,<org.apache.hadoop.io.nativeio.NativeIO$POSIX: void <clinit>()>
Unable to initialize NativeIO libraries,debug,<org.apache.hadoop.io.nativeio.NativeIO$POSIX: void <clinit>()>
"Recursively deleting , <*>,  from ZK..., ",info,<org.apache.hadoop.ha.ActiveStandbyElector: void clearParentZNode()>
"Successfully deleted , <*>,  from ZK., ",info,<org.apache.hadoop.ha.ActiveStandbyElector: void clearParentZNode()>
"poking parent \', <*>, \' for key: , key, ",debug,<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.Object getProperty(java.lang.String)>
"returning \', <*>, \' for key: , key, ",debug,<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.Object getProperty(java.lang.String)>
"No KEY found for persisted identifier , <*>, ",warn,"<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)>"
"Loaded truststore \', <*>, \', ",debug,<org.apache.hadoop.security.ssl.ReloadingX509TrustManager: javax.net.ssl.X509TrustManager loadTrustManager()>
"Error changing permissions of , item, ",debug,<org.apache.hadoop.fs.FsShellPermissions$Chmod: void processPath(org.apache.hadoop.fs.shell.PathData)>
"Metrics intern cache overflow at , <*>,  for , e, ",warn,<org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys$1: boolean removeEldestEntry(java.util.Map$Entry)>
"Exception while changing ops : , <*>, ",warn,<org.apache.hadoop.ipc.Server$Responder: void doAsyncWrite(java.nio.channels.SelectionKey)>
"Error getting groups for , user, ",debug,<org.apache.hadoop.security.JniBasedUnixGroupsMapping: java.util.List getGroups(java.lang.String)>
"Error getting groups for , user, : , <*>, ",info,<org.apache.hadoop.security.JniBasedUnixGroupsMapping: java.util.List getGroups(java.lang.String)>
Health monitor failed,fatal,"<org.apache.hadoop.ha.HealthMonitor$MonitorDaemon$1: void uncaughtException(java.lang.Thread,java.lang.Throwable)>"
"Local node , <*>,  is already active. , No need to failover. Returning success., ",info,<org.apache.hadoop.ha.ZKFailoverController: void doGracefulFailover()>
"Asking , <*>,  to cede its active state for , timeout, ms, ",info,<org.apache.hadoop.ha.ZKFailoverController: void doGracefulFailover()>
"Successfully became active. , <*>, ",info,<org.apache.hadoop.ha.ZKFailoverController: void doGracefulFailover()>
"Should fence: , target, ",info,<org.apache.hadoop.ha.ZKFailoverController: void doFence(org.apache.hadoop.ha.HAServiceTarget)>
"Successfully transitioned , target,  to standby , state without fencing, ",info,<org.apache.hadoop.ha.ZKFailoverController: void doFence(org.apache.hadoop.ha.HAServiceTarget)>
"Couldn\'t fence old active , target, ",error,<org.apache.hadoop.ha.ZKFailoverController: void doFence(org.apache.hadoop.ha.HAServiceTarget)>
"pushMetric failed for , <*>, \n, ",info,<org.apache.hadoop.metrics.util.MetricsTimeVaryingInt: void pushMetric(org.apache.hadoop.metrics.MetricsRecord)>
"When stopping the service , <*>,  : , <*>, ",warn,"<org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)>"
Using JniBasedUnixGroupsNetgroupMapping for Netgroup resolution,debug,<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping: void <clinit>()>
hadoop login,debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean login()>
"Call: , <*>,  , callTime, ",debug,"<org.apache.hadoop.ipc.WritableRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
POST,info,"<org.apache.hadoop.conf.ReconfigurationServlet: void doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Yielding from election,info,<org.apache.hadoop.ha.ActiveStandbyElector: void quitElection(boolean)>
"<*>, : task running, ",debug,<org.apache.hadoop.ipc.Server$ConnectionManager$1: void run()>
Refreshing hosts (include/exclude) list,info,<org.apache.hadoop.util.HostsFileReader: void refresh()>
"Interrupted while shutting down thread - , <*>, ",warn,"<org.apache.hadoop.util.ShutdownThreadsHelper: boolean shutdownThread(java.lang.Thread,long)>"
"Would have joined master election, but this node is prohibited from doing so for , <*>,  more ms, ",info,<org.apache.hadoop.ha.ZKFailoverController: void recheckElectability()>
"Ensuring that , <*>,  does not , participate in active master election, ",info,<org.apache.hadoop.ha.ZKFailoverController: void recheckElectability()>
"Quitting master election for , <*>,  and marking that fencing is necessary, ",info,<org.apache.hadoop.ha.ZKFailoverController: void recheckElectability()>
"servlet path: , <*>, ",info,<org.apache.hadoop.conf.ReconfigurationServlet: org.apache.hadoop.conf.Reconfigurable getReconfigurable(javax.servlet.http.HttpServletRequest)>
"getting attribute: conf.servlet.reconfigurable., <*>, ",info,<org.apache.hadoop.conf.ReconfigurationServlet: org.apache.hadoop.conf.Reconfigurable getReconfigurable(javax.servlet.http.HttpServletRequest)>
"<*>, : Call -> , <*>, : , <*>,  {, <*>, }, ",trace,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"<*>, : Exception <- , <*>, : , <*>,  {, <*>, }, ",trace,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"Call: , <*>,  took , e_, ms, ",debug,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"<*>, : Response <- , <*>, : , <*>,  {, <*>, }, ",trace,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
KeyStore loaded successfully !!,debug,"<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
"Metrics intern cache overflow at , <*>,  for , e, ",warn,<org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys$2: boolean removeEldestEntry(java.util.Map$Entry)>
"ZK Election indicated that , <*>,  should become standby, ",info,<org.apache.hadoop.ha.ZKFailoverController: void becomeStandby()>
"Successfully transitioned , <*>,  to standby state, ",info,<org.apache.hadoop.ha.ZKFailoverController: void becomeStandby()>
"Couldn\'t transition , <*>,  to standby state, ",error,<org.apache.hadoop.ha.ZKFailoverController: void becomeStandby()>
"Exception encountered while connecting to the server : , <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection$1: java.lang.Object run()>
"Couldn\'t setup connection for , <*>,  to , <*>, ",warn,<org.apache.hadoop.ipc.Client$Connection$1: java.lang.Object run()>
"Exception encountered while connecting to the server : , <*>, ",warn,<org.apache.hadoop.ipc.Client$Connection$1: java.lang.Object run()>
Refreshing hosts (include/exclude) list,info,"<org.apache.hadoop.util.HostsFileReader: void refresh(java.io.InputStream,java.io.InputStream)>"
"Pushing record , <*>, ., <*>, ., <*>,  to , <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
Done,debug,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
Stopping client,debug,<org.apache.hadoop.ipc.Client: void stop()>
"Server at , addr,  not available yet, Zzzzz..., ",info,"<org.apache.hadoop.ipc.RPC: org.apache.hadoop.ipc.ProtocolProxy waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)>"
"Problem connecting to server: , addr, ",info,"<org.apache.hadoop.ipc.RPC: org.apache.hadoop.ipc.ProtocolProxy waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)>"
"No route to host for server: , addr, ",info,"<org.apache.hadoop.ipc.RPC: org.apache.hadoop.ipc.ProtocolProxy waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)>"
HttpServer Acceptor: isRunning is false. Rechecking.,warn,<org.apache.hadoop.http.HttpServer$SelectChannelConnectorWithSafeStartup: boolean isRunning()>
"HttpServer Acceptor: isRunning is , <*>, ",warn,<org.apache.hadoop.http.HttpServer$SelectChannelConnectorWithSafeStartup: boolean isRunning()>
Deompressor obtained from CodecPool already finished(),warn,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Decompressor getDecompressor()>
"Got a decompressor: , <*>, ",debug,<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Decompressor getDecompressor()>
"Stopping service #, i_, : , service, ",debug,"<org.apache.hadoop.service.CompositeService: void stop(int,boolean)>"
"Jetty bound to port , <*>, ",info,<org.apache.hadoop.http.HttpServer2: void openListeners()>
"Call: protocol=, protocol, , method=, <*>, ",info,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"Unknown method , <*>,  called on , protocol,  protocol., ",warn,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"Served: , <*>,  queueTime= , qTime,  procesingTime= , processingTime_, ",debug,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"<*>,  exception= , <*>, ",debug,"<org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"Error serializing call response for call , call, ",warn,"<org.apache.hadoop.ipc.Server: void setupResponse(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)>"
"Failed to renew token, action=, action_, ",warn,<org.apache.hadoop.fs.DelegationTokenRenewer: void run()>
running sort pass,debug,<org.apache.hadoop.io.SequenceFile$Sorter: int sortPass(boolean)>
"Interrupted while canceling token for , <*>, filesystem, ",error,<org.apache.hadoop.fs.DelegationTokenRenewer: void removeRenewAction(org.apache.hadoop.fs.FileSystem)>
,debug,<org.apache.hadoop.fs.DelegationTokenRenewer: void removeRenewAction(org.apache.hadoop.fs.FileSystem)>
"<*>_, ZKDTSMDelegationToken_, <*>, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean)>"
The connection is not in the closed state,error,<org.apache.hadoop.ipc.Client$Connection: void close()>
A connection is closed for no cause and calls are not empty,warn,<org.apache.hadoop.ipc.Client$Connection: void close()>
"closing ipc connection to , <*>, : , <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void close()>
"<*>, : closed, ",debug,<org.apache.hadoop.ipc.Client$Connection: void close()>
"truncating long string: , <*>,  chars, starting with , <*>, ",warn,"<org.apache.hadoop.io.UTF8: int writeString(java.io.DataOutput,java.lang.String)>"
"Interface , childInterface,  ignored because it does not extend VersionedProtocol, ",warn,<org.apache.hadoop.ipc.RPC: java.lang.Class[] getSuperInterfaces(java.lang.Class[])>
"<*>,  thread interrupted., ",info,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void publishMetricsFromQueue()>
"Got sink exception, retry in , awhile, ms, ",error,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void publishMetricsFromQueue()>
"<*>,  thread interrupted while waiting for retry, ",info,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void publishMetricsFromQueue()>
"Got sink exception and over retry limit, suppressing further error messages",error,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void publishMetricsFromQueue()>
"<*>, ..., ",info,<org.apache.hadoop.ipc.WritableRpcEngine$Server: void log(java.lang.String)>
"this, : adding notificationSocket , <*>, , connected to , <*>, ",trace,"<org.apache.hadoop.net.unix.DomainSocketWatcher: void addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)>"
"Server connection from , <*>, ; # active connections: , <*>, ; # queued calls: , <*>, ",debug,<org.apache.hadoop.ipc.Server$ConnectionManager: org.apache.hadoop.ipc.Server$Connection register(java.nio.channels.SocketChannel)>
"Served: , <*>,  queueTime= , <*>,  procesingTime= , <*>, ",debug,"<org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
Unexpected throwable object ,error,"<org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"<*>,  exception= , <*>, ",debug,"<org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>"
"parsing URL , url, ",debug,"<org.apache.hadoop.conf.Configuration: org.w3c.dom.Document parse(javax.xml.parsers.DocumentBuilder,java.net.URL)>"
"<*>, :Exception in closing listener socket. , <*>, ",info,<org.apache.hadoop.ipc.Server$Listener: void doStop()>
Ignoring re-entrant call to stop(),debug,<org.apache.hadoop.service.AbstractService: void stop()>
"Looking for process running on port , <*>, ",info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
"Successfully killed process that was listening on port , <*>, ",info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
Indeterminate response from trying to kill service. Verifying whether it is running using nc...,info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
Unable to fence - it is running but we cannot kill it,warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
Verified that the service is down.,info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
"rc: , <*>, ",info,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
Interrupted while trying to fence via ssh,warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
Unknown failure while trying to fence via ssh,warn,"<org.apache.hadoop.ha.SshFenceByTcpPort: boolean doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)>"
"Error getting users for netgroup , netgroup, ",debug,<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping: java.util.List getUsersForNetgroup(java.lang.String)>
"Error getting users for netgroup , netgroup, : , <*>, ",info,<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping: java.util.List getUsersForNetgroup(java.lang.String)>
"Error while changing permission : , filename,  Exception: , <*>, ",debug,"<org.apache.hadoop.fs.FileUtil: int chmod(java.lang.String,java.lang.String,boolean)>"
Error caching groups,warn,<org.apache.hadoop.security.Groups: void cacheGroupsAdd(java.util.List)>
"The ping interval is , <*>,  ms., ",debug,"<org.apache.hadoop.ipc.Client$Connection: void <init>(org.apache.hadoop.ipc.Client,org.apache.hadoop.ipc.Client$ConnectionId,int)>"
"ShutdownHook \', <*>, \' failed, , <*>, ",warn,<org.apache.hadoop.util.ShutdownHookManager$1: void run()>
"Detected a loopback TCP socket, disconnecting it",info,"<org.apache.hadoop.net.NetUtils: void connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)>"
Unable to get host interfaces,error,"<org.apache.hadoop.net.NetUtils: java.util.List getIPs(java.lang.String,boolean)>"
"Unable to close SpanReceiver correctly: , <*>, ",warn,<org.apache.hadoop.tracing.SpanReceiverHost: void closeReceivers()>
Unable to determine local hostname -falling back to \localhost\,warn,<org.apache.hadoop.net.DNS: java.lang.String resolveLocalHostname()>
Metric was emitted with no name.,warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Metric name , name,  was emitted with a null value., ",warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Metric name , name, , value , value,  has no type., ",warn,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Emitting metric , name, , type , type, , value , value, , slope , <*>,  from hostname , <*>, ",debug,"<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>"
"Adding service , <*>, ",debug,<org.apache.hadoop.service.CompositeService: void addService(org.apache.hadoop.service.Service)>
"Could not load truststore (keep using existing one) : , <*>, ",warn,<org.apache.hadoop.security.ssl.ReloadingX509TrustManager: void run()>
KeyStore initialized anew successfully !!,debug,"<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
"Can\'t map group , group, . Use its string hashcode:, <*>, ",info,<org.apache.hadoop.security.ShellBasedIdMapping: int getGidAllowingUnknown(java.lang.String)>
"Failed to set setXIncludeAware(true) for parser , <*>, :, <*>, ",error,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"parsing File , <*>, ",debug,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
bad conf file: top-level element not <configuration>,fatal,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
bad conf file: element not <property>,warn,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"error parsing conf , name_, ",fatal,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"error parsing conf , name_, ",fatal,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"error parsing conf , name_, ",fatal,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"error parsing conf , name_, ",fatal,"<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>"
"Got , <*>_,  , name_,  for ID , id,  from the native implementation, ",debug,"<org.apache.hadoop.io.nativeio.NativeIO$POSIX: java.lang.String getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)>"
"Failed to connect to server: , <*>, : , <*>, ",warn,"<org.apache.hadoop.ipc.Client$Connection: void handleConnectionFailure(int,java.io.IOException)>"
"Retrying connect to server: , <*>, . Already tried , curRetries,  time(s); retry policy is , <*>, ",info,"<org.apache.hadoop.ipc.Client$Connection: void handleConnectionFailure(int,java.io.IOException)>"
Interrupted while waiting for clientExecutorto stop,error,<org.apache.hadoop.ipc.Client$ClientExecutorServiceFactory: java.util.concurrent.ExecutorService unrefAndCleanup()>
"Becoming standby for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void becomeStandby()>
"<*>, : starting, having connections , <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void run()>
"Unexpected error reading responses on connection , this, ",warn,<org.apache.hadoop.ipc.Client$Connection: void run()>
"<*>, : stopped, remaining connections , <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection: void run()>
"Error while stopping listener for webapp, <*>, ",error,<org.apache.hadoop.http.HttpServer2: void stop()>
"Error while stopping web app context for webapp , <*>, ",error,<org.apache.hadoop.http.HttpServer2: void stop()>
"Error while stopping web server for webapp , <*>, ",error,<org.apache.hadoop.http.HttpServer2: void stop()>
Lexicographical comparer selected for byte aligned system architecture,trace,<org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
Unsafe comparer selected for byte unaligned system architecture,trace,<org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
,trace,<org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
Lexicographical comparer selected,trace,<org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
net.topology.table.file.name not configured. ,warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"Line does not have two columns. Ignoring. , <*>, ",warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"<*>,  cannot be read., ",warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"<*>,  cannot be read., ",warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"<*>,  cannot be read., ",warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"<*>,  cannot be read., ",warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
"name, , , description, ",debug,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSink register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)>"
"Sink , name,  already exists!, ",warn,"<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSink register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)>"
"Found checksum error: b, off, , , <*>, =, <*>, ",info,"<org.apache.hadoop.fs.FSInputChecker: int readChecksumChunk(byte[],int,int)>"
"\n, header, <*>, ",warn,"<org.apache.hadoop.security.ShellBasedIdMapping: void reportDuplicateEntry(java.lang.String,java.lang.Integer,java.lang.String,java.lang.Integer,java.lang.String)>"
Failed to read topology table. /default-rack will be used for all nodes.,warn,<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.List resolve(java.util.List)>
"No TokenRenewer defined for token kind , <*>, ",warn,<org.apache.hadoop.security.token.Token: org.apache.hadoop.security.token.TokenRenewer getRenewer()>
failed to register any UNIX signal loggers: ,warn,"<org.apache.hadoop.util.StringUtils: void startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.commons.logging.Log)>"
Already in election. Not re-connecting.,info,<org.apache.hadoop.ha.ActiveStandbyElector: void joinElection(byte[])>
"Attempting active election for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void joinElection(byte[])>
"The mapping provider, , name,  does not have a valid class, ",error,<org.apache.hadoop.security.CompositeGroupsMapping: void loadMappingProviders()>
"PrivilegedAction as:, this,  from:, <*>, ",debug,"<org.apache.hadoop.security.UserGroupInformation: void logPrivilegedAction(javax.security.auth.Subject,java.lang.Object)>"
"Error while stopping listener for webapp, <*>, ",error,<org.apache.hadoop.http.HttpServer: void stop()>
"Error while destroying the SSLFactory, <*>, ",error,<org.apache.hadoop.http.HttpServer: void stop()>
"Error while stopping web app context for webapp , <*>, ",error,<org.apache.hadoop.http.HttpServer: void stop()>
"Error while stopping web server for webapp , <*>, ",error,<org.apache.hadoop.http.HttpServer: void stop()>
Checking for any old active which needs to be fenced...,info,<org.apache.hadoop.ha.ActiveStandbyElector: org.apache.zookeeper.data.Stat fenceOldActive()>
No old node to fence,info,<org.apache.hadoop.ha.ActiveStandbyElector: org.apache.zookeeper.data.Stat fenceOldActive()>
"Old node exists: , <*>, ",info,<org.apache.hadoop.ha.ActiveStandbyElector: org.apache.zookeeper.data.Stat fenceOldActive()>
"But old node has our own data, so don\'t need to fence it.",info,<org.apache.hadoop.ha.ActiveStandbyElector: org.apache.zookeeper.data.Stat fenceOldActive()>
Using JniBasedUnixGroupsMapping for Group resolution,debug,<org.apache.hadoop.security.JniBasedUnixGroupsMapping: void <clinit>()>
"this, : error writing to notificationSockets, ",error,<org.apache.hadoop.net.unix.DomainSocketWatcher: void kick()>
"refCount=, <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: boolean shutdown()>
Redundant shutdown,debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: boolean shutdown()>
Error stopping the metrics system,warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: boolean shutdown()>
"<*>,  metrics system shutdown complete., ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: boolean shutdown()>
"Adding Kerberos (SPNEGO) filter to , name, ",info,"<org.apache.hadoop.http.HttpServer2: void addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean)>"
reading next wrapped RPC packet,debug,<org.apache.hadoop.security.SaslRpcClient$WrappedInputStream: void readNextRpcPacket()>
"unwrapping token of length:, <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient$WrappedInputStream: void readNextRpcPacket()>
"FairCallQueue is in use with , <*>,  queues., ",info,"<org.apache.hadoop.ipc.FairCallQueue: void <init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"Added filter , name,  (class=, classname, ) to context , <*>, ",info,"<org.apache.hadoop.http.HttpServer2: void addFilter(java.lang.String,java.lang.String,java.util.Map)>"
"Added filter , name,  (class=, classname, ) to context , <*>, ",info,"<org.apache.hadoop.http.HttpServer2: void addFilter(java.lang.String,java.lang.String,java.util.Map)>"
"No node in path , <*>, , ",error,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: org.apache.hadoop.security.token.delegation.DelegationKey getKeyFromZK(int)>
"Trying to make , <*>,  active..., ",info,<org.apache.hadoop.ha.ZKFailoverController: void becomeActive()>
"Successfully transitioned , <*>,  to active state, ",info,<org.apache.hadoop.ha.ZKFailoverController: void becomeActive()>
"Couldn\'t make , <*>,  active, ",fatal,<org.apache.hadoop.ha.ZKFailoverController: void becomeActive()>
"Ignoring failure to deleteOnExit for path , path, ",warn,<org.apache.hadoop.fs.FileContext: void processDeleteOnExit()>
Closing proxy or invocation handler caused exception,error,<org.apache.hadoop.ipc.RPC: void stopProxy(java.lang.Object)>
"RPC.stopProxy called on non proxy: class=, <*>, ",error,<org.apache.hadoop.ipc.RPC: void stopProxy(java.lang.Object)>
hadoop login commit,debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"using existing subject:, <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"using kerberos user:, <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"using local user:, <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"Using user: \, user_, \ with name , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
,<init>,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"User entry: \, <*>, \, ",debug,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"Can\'t find user in , <*>, ",error,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
Can\'t find user name,<init>,<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
"setsid exited with exit code , <*>_, ",debug,<org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
setsid is not available on this machine. So not using it.,debug,<org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
"setsid exited with exit code , <*>_, ",debug,<org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
"setsid exited with exit code , <*>_, ",debug,<org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
"Login successful for user , <*>,  using keytab file , <*>, ",info,"<org.apache.hadoop.security.UserGroupInformation: void loginUserFromKeytab(java.lang.String,java.lang.String)>"
"Computing capacity for map , mapName, ",info,"<org.apache.hadoop.util.LightWeightGSet: int computeCapacity(long,double,java.lang.String)>"
"VM type       = , <*>, -bit, ",info,"<org.apache.hadoop.util.LightWeightGSet: int computeCapacity(long,double,java.lang.String)>"
"percentage, % max memory , <*>,  = , <*>, ",info,"<org.apache.hadoop.util.LightWeightGSet: int computeCapacity(long,double,java.lang.String)>"
"capacity      = ^, <*>_,  = , <*>,  entries, ",info,"<org.apache.hadoop.util.LightWeightGSet: int computeCapacity(long,double,java.lang.String)>"
"writing intermediate results to , <*>, ",debug,<org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge()>
"name,  not found, ",info,<org.apache.hadoop.conf.Configuration: java.io.Reader getConfResourceAsReader(java.lang.String)>
"found resource , name,  at , <*>, ",info,<org.apache.hadoop.conf.Configuration: java.io.Reader getConfResourceAsReader(java.lang.String)>
"<*>,  metrics system timer already stopped!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stopTimer()>
"Error retrieving tokenInfo , <*>,  from ZK, ",error,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
,debug,<org.apache.hadoop.ipc.metrics.RpcDetailedMetrics: void <init>(int)>
"error getting users for netgroup , netgroup, ",warn,<org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping: java.lang.String execShellGetUserForNetgroup(java.lang.String)>
,warn,<org.apache.hadoop.crypto.JceAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
"method , method,  with annotation , annotation, ",debug,"<org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>"
"Exception while trying to get password for alias , alias, : , <*>, ",warn,"<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: java.lang.String getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>"
"<*>, : responding to , call_, ",debug,"<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>"
"<*>, , call , call_, : output error, ",warn,"<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>"
"<*>, : responding to , call_,  Wrote , <*>,  bytes., ",debug,"<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>"
"<*>, : responding to , call_,  Wrote partial , <*>,  bytes., ",debug,"<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>"
"<*>, , call , call_, : output error, ",warn,"<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>"
Connecting to ZooKeeper with SASL/Kerberosand using \'sasl\' ACLs,info,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
Connecting to ZooKeeper without authentication,info,<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
"Could not connect to local service at , <*>, : , <*>, ",warn,<org.apache.hadoop.ha.HealthMonitor: void tryConnect()>
Starting reconfiguration task.,info,<org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread: void run()>
"Property , <*>,  is not reconfigurable, ",info,<org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread: void run()>
"Change property: , <*>,  from \, <*>_, \ to \, <*>_, \., ",info,<org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread: void run()>
"urls: , <*>, ",debug,"<org.apache.hadoop.util.ApplicationClassLoader: void <init>(java.net.URL[],java.lang.ClassLoader,java.util.List)>"
"system classes: , systemClasses, ",debug,"<org.apache.hadoop.util.ApplicationClassLoader: void <init>(java.net.URL[],java.lang.ClassLoader,java.util.List)>"
"system classes: , <*>, ",info,"<org.apache.hadoop.util.ApplicationClassLoader: void <init>(java.net.URL[],java.lang.ClassLoader,java.util.List)>"
"options parsing failed: , <*>, ",warn,"<org.apache.hadoop.util.GenericOptionsParser: void parseGeneralOptions(org.apache.commons.cli.Options,org.apache.hadoop.conf.Configuration,java.lang.String[])>"
Failed to reload the topology table.  The cached mappings will not be cleared.,error,<org.apache.hadoop.net.TableMapping$RawTableMapping: void reloadCachedMappings()>
login must be done first,<init>,<org.apache.hadoop.security.UserGroupInformation: void reloginFromTicketCache()>
"Initiating logout for , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: void reloginFromTicketCache()>
"Initiating re-login for , <*>, ",debug,<org.apache.hadoop.security.UserGroupInformation: void reloginFromTicketCache()>
"Stopping metrics sink , <*>, : class=, <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stopSinks()>
Stopping HealthMonitor thread,info,<org.apache.hadoop.ha.HealthMonitor: void shutdown()>
"emitRecord failed: , <*>, ",warn,"<org.apache.hadoop.metrics.spi.CompositeContext: void emitRecord(java.lang.String,java.lang.String,org.apache.hadoop.metrics.spi.OutputRecord)>"
"Added global filter \', name, \' (class=, classname, ), ",info,"<org.apache.hadoop.http.HttpServer2: void addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)>"
"I/O error finding interface , strInterface, : , <*>, ",warn,"<org.apache.hadoop.net.DNS: java.lang.String[] getIPs(java.lang.String,boolean)>"
"Found tgt , ticket, ",debug,<org.apache.hadoop.security.UserGroupInformation: javax.security.auth.kerberos.KerberosTicket getTGT()>
"<*>,  metrics system already initialized!, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
"<*>,  metrics system started (again), ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
"Metrics system not started: , <*>, ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
Stacktrace: ,debug,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
"prefix,  metrics system started in standby mode, ",info,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
"Failed to find datanode (scope=\, <*>, \ excludedScope=\, <*>, \)., ",<init>,"<org.apache.hadoop.net.NetworkTopology: org.apache.hadoop.net.Node chooseRandom(java.lang.String,java.lang.String)>"
Could not get disk usage information,warn,<org.apache.hadoop.fs.DU$DURefreshThread: void run()>
Unexpected exception while closing selector : ,warn,<org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo: void close()>
,debug,<org.apache.hadoop.metrics2.lib.MutableRates: void init(java.lang.Class)>
"Error creating rate metrics for , <*>, ",error,<org.apache.hadoop.metrics2.lib.MutableRates: void init(java.lang.Class)>
WeightedRoundRobinMultiplexer is being used.,info,"<org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer: void <init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"Storing ZKDTSMDelegationKey_, <*>, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>"
"Key with path , <*>,  already exists.. Updating !!, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>"
"Updating non existent Key path , <*>, .. Adding new !!, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>"
"<*>,  znode already exists !!, ",debug,"<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>"
 null,sortByDistance,"<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)>"
"<*>, : doAsyncWrite threw exception , <*>, ",info,<org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
Checking for old call responses.,debug,<org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
Out of Memory in server select,warn,<org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
Exception in Responder,warn,<org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
"Namenode trash configuration: Deletion interval = , <*>,  minutes, Emptier interval = , <*>,  minutes., ",info,"<org.apache.hadoop.fs.TrashPolicyDefault: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>"
"Invalid propertylist for , <*>, ",error,<org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink: void loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType)>
"got exception trying to get groups for user , user, : , <*>, ",warn,<org.apache.hadoop.security.ShellBasedUnixGroupsMapping: java.util.List getUnixGroups(java.lang.String)>
"Adding saslServer wrapped token of size , <*>,  as call response., ",debug,"<org.apache.hadoop.ipc.Server: void wrapWithSasl(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$Call)>"
No MBeanServer could be found.,error,"<org.apache.hadoop.jmx.JMXJsonServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Caught an exception while processing JMX request,error,"<org.apache.hadoop.jmx.JMXJsonServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Caught an exception while processing JMX request,error,"<org.apache.hadoop.jmx.JMXJsonServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"SASL client callback: setting username: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
SASL client callback: setting userPassword,debug,<org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
"SASL client callback: setting realm: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
"Running cmd: , cmd, ",debug,"<org.apache.hadoop.ha.SshFenceByTcpPort: int execCommand(com.jcraft.jsch.Session,java.lang.String)>"
"No unit for , name, (, <*>, ) assuming , unit, ",warn,"<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>"
"flush failed: , <*>, ",warn,<org.apache.hadoop.metrics.spi.CompositeContext: void flush()>
"Can\'t find group name for gid , gid, . Use default group name , unknown, ",warn,"<org.apache.hadoop.security.ShellBasedIdMapping: java.lang.String getGroupName(int,java.lang.String)>"
failed to load SnappyCompressor,error,<org.apache.hadoop.io.compress.snappy.SnappyCompressor: void <clinit>()>
"Drop the response. Current retryCount == , retryCount, ",debug,"<org.apache.hadoop.io.retry.LossyRetryInvocationHandler: java.lang.Object invokeMethod(java.lang.reflect.Method,java.lang.Object[])>"
"retryCount == , <*>, . It\'s time to normally process the response, ",debug,"<org.apache.hadoop.io.retry.LossyRetryInvocationHandler: java.lang.Object invokeMethod(java.lang.reflect.Method,java.lang.Object[])>"
"Error accessing field , field, ",warn,<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.lib.MetricsRegistry initRegistry(java.lang.Object)>
"Listing beans for , qry, ",debug,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"getting attribute , prs_,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"getting attribute , prs_,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"getting attribute , prs_,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"getting attribute , prs_,  of , oname,  threw an exception, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"Problem while trying to process JMX query: , qry,  with MBean , oname, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"Problem while trying to process JMX query: , qry,  with MBean , oname, ",error,"<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>"
"recommended=, recommended_length, , actual=, <*>, ",debug,<org.apache.hadoop.util.LightWeightGSet: void <init>(int)>
"Failed to add SpanReceiver , <*>,  with configuration , <*>, ",info,<org.apache.hadoop.tracing.SpanReceiverHost: long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo)>
"Failed to add SpanReceiver , <*>,  with configuration , <*>, ",info,<org.apache.hadoop.tracing.SpanReceiverHost: long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo)>
"Successfully added SpanReceiver , <*>,  with configuration , <*>, ",info,<org.apache.hadoop.tracing.SpanReceiverHost: long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo)>
"adding path spec: , c#, ",info,"<org.apache.hadoop.http.HttpServer: void <init>(java.lang.String,java.lang.String,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.AccessControlList,org.mortbay.jetty.Connector,java.lang.String[])>"
rechecking for electability from bad state,debug,<org.apache.hadoop.ha.ZKFailoverController: void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)>
"Local service , <*>,  has changed the serviceState to , changedState, . Expected was , <*>, . Quitting election marking fencing necessary., ",error,<org.apache.hadoop.ha.ZKFailoverController: void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)>
"RETRY , retries, ) policy=, <*>, , exception=, e_, ",debug,"<org.apache.hadoop.io.retry.RetryUtils$1: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>"
HttpServer.start() threw a non Bind IOException,info,<org.apache.hadoop.http.HttpServer2: void start()>
HttpServer.start() threw a MultiException,info,<org.apache.hadoop.http.HttpServer2: void start()>
"changing property , property,  to , newVal, ",info,"<org.apache.hadoop.conf.ReconfigurableBase: java.lang.String reconfigureProperty(java.lang.String,java.lang.String)>"
"attribute, : , a, ",debug,<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: java.lang.Object getAttribute(java.lang.String)>
Update cache now,info,<org.apache.hadoop.security.ShellBasedIdMapping: void checkAndUpdateMaps()>
"Can\'t update the maps. Will use the old ones, which can potentially cause problem.",error,<org.apache.hadoop.security.ShellBasedIdMapping: void checkAndUpdateMaps()>
"Exception while notifying listeners of , this, : , <*>, ",warn,<org.apache.hadoop.service.AbstractService: void notifyListeners()>
"adding path spec: , c#, ",info,"<org.apache.hadoop.http.HttpServer2: void initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])>"
"Created new connection for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: void createConnection()>
Metric was emitted with no name.,warn,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>"
"Metric name , name,  was emitted with a null value., ",warn,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>"
"Metric name , name, , value , value,  has no type., ",warn,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>"
"Emitting metric , name, , type , type, , value , value,  from hostname, <*>, ",debug,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>"
"Metric name , name, , value , value,  had \'null\' units, ",warn,"<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>"
Operation failed,debug,<org.apache.hadoop.ha.HAAdmin: int run(java.lang.String[])>
"Token renewal for identifier: , id, ; total currentTokens , <*>, ",info,"<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: long renewToken(org.apache.hadoop.security.token.Token,java.lang.String)>"
"<*>,  sending #, <*>, ",debug,<org.apache.hadoop.ipc.Client$Connection$3: void run()>
"Becoming active for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: boolean becomeActive()>
Exception handling the winning of election,warn,<org.apache.hadoop.ha.ActiveStandbyElector: boolean becomeActive()>
,<init>,<org.apache.hadoop.security.UserGroupInformation: org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject)>
"Kerberos principal name is , <*>, ",debug,<org.apache.hadoop.security.SaslRpcServer: void <init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)>
"No groups available for user , <*>, ",warn,<org.apache.hadoop.security.UserGroupInformation: java.lang.String[] getGroupNames()>
"Removing a node: , <*>, ",info,<org.apache.hadoop.net.NetworkTopology: void remove(org.apache.hadoop.net.Node)>
"NetworkTopology became:\n, <*>, ",debug,<org.apache.hadoop.net.NetworkTopology: void remove(org.apache.hadoop.net.Node)>
"Error creating sink \', sinkName, \', ",warn,<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void configureSinks()>
"Unknown metrics type: , <*>, ",warn,"<org.apache.hadoop.metrics.ganglia.GangliaContext: void emitRecord(java.lang.String,java.lang.String,org.apache.hadoop.metrics.spi.OutputRecord)>"
"Remove leading / off , name, ",debug,<org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)>
"getResource(, name, )=, url_, ",debug,<org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)>
"Halt with status , status,  Message: , msg, ",info,"<org.apache.hadoop.util.ExitUtil: void halt(int,java.lang.String)>"
Halt called,fatal,"<org.apache.hadoop.util.ExitUtil: void halt(int,java.lang.String)>"
"<*>, : starting, ",info,<org.apache.hadoop.ipc.Server$Responder: void run()>
"Stopping , <*>, ",info,<org.apache.hadoop.ipc.Server$Responder: void run()>
"Couldn\'t close write selector in , <*>, ",error,<org.apache.hadoop.ipc.Server$Responder: void run()>
"Stopping , <*>, ",info,<org.apache.hadoop.ipc.Server$Responder: void run()>
"Couldn\'t close write selector in , <*>, ",error,<org.apache.hadoop.ipc.Server$Responder: void run()>
"name,  not found, ",info,<org.apache.hadoop.conf.Configuration: java.io.InputStream getConfResourceAsInputStream(java.lang.String)>
"found resource , name,  at , <*>, ",info,<org.apache.hadoop.conf.Configuration: java.io.InputStream getConfResourceAsInputStream(java.lang.String)>
"Problem opening checksum file: , file, .  Ignoring exception: , ",warn,"<org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker: void <init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)>"
"Bad checksum at , <*>, . Skipping entries., ",warn,<org.apache.hadoop.io.SequenceFile$Reader: void handleChecksumException(org.apache.hadoop.fs.ChecksumException)>
"version: , <*>, ",debug,<org.apache.hadoop.util.VersionInfo: void main(java.lang.String[])>
Not able to close a socket,warn,<org.apache.hadoop.ipc.Client$Connection: void closeConnection()>
HttpServer Acceptor: isRunning is false. Rechecking.,warn,<org.apache.hadoop.http.HttpServer2$SelectChannelConnectorWithSafeStartup: boolean isRunning()>
"HttpServer Acceptor: isRunning is , <*>, ",warn,<org.apache.hadoop.http.HttpServer2$SelectChannelConnectorWithSafeStartup: boolean isRunning()>
"Unable to wrap exception of type , <*>, : it has no (String) constructor, ",warn,"<org.apache.hadoop.net.NetUtils: java.io.IOException wrapWithMessage(java.io.IOException,java.lang.String)>"
Serialization class not found: ,warn,"<org.apache.hadoop.io.serializer.SerializationFactory: void add(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"mlocking , identifier, ",info,"<org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator: void mlock(java.lang.String,java.nio.ByteBuffer,long)>"
"Adding Kerberos (SPNEGO) filter to , name, ",info,"<org.apache.hadoop.http.HttpServer: void addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean)>"
GET,info,"<org.apache.hadoop.conf.ReconfigurationServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Shutting down all AsyncDiskService threads immediately...,info,<org.apache.hadoop.util.AsyncDiskService: java.util.List shutdownNow()>
"Service health check failed for , <*>, : , <*>, ",warn,<org.apache.hadoop.ha.HealthMonitor: void doHealthChecks()>
"Transport-level exception trying to monitor health of , <*>, : , <*>, ",warn,<org.apache.hadoop.ha.HealthMonitor: void doHealthChecks()>
"Establishing zookeeper connection for , this, ",debug,<org.apache.hadoop.ha.ActiveStandbyElector: boolean reEstablishSession()>
"Starting expired delegation token remover thread, tokenRemoverScanInterval=, <*>,  min(s), ",info,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover: void run()>
Master key updating failed: ,error,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover: void run()>
"ExpiredTokenRemover received , <*>, ",error,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover: void run()>
ExpiredTokenRemover thread received unexpected exception,error,<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover: void run()>
Could not load LogJLogger class,debug,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
"Http request log for , <*>,  could not be created, ",warn,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
"Http request log for , <*>,  is not defined, ",info,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
,setFilename,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
,setRetainDays,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
"Jetty request log for , <*>,  was of the wrong class, ",warn,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
Jetty request log can only be enabled using Logj,warn,<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
"Error invoking method , <*>, ",error,"<org.apache.hadoop.metrics2.lib.MethodMetric$2: void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)>"
Failed to load Openssl SecureRandom,error,<org.apache.hadoop.crypto.random.OpensslSecureRandom: void <clinit>()>
Successfully loaded & initialized native-zlib library,info,<org.apache.hadoop.io.compress.zlib.ZlibFactory: void <clinit>()>
Failed to load/initialize native-zlib library,warn,<org.apache.hadoop.io.compress.zlib.ZlibFactory: void <clinit>()>
<*>_,debug,<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.String getClassName(java.lang.String)>
"Adding Rpc request clientId , <*>, <*>,  callId , <*>,  to retryCache, ",trace,<org.apache.hadoop.ipc.RetryCache: org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)>
"Exception while invoking , <*>, ., <*>,  over , <*>, . Not retrying because , <*>, ",warn,"<org.apache.hadoop.io.retry.RetryInvocationHandler: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"msg_, . Trying to fail over , <*>, ",info,"<org.apache.hadoop.io.retry.RetryInvocationHandler: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"Exception while invoking , <*>,  of class , <*>,  over , <*>, . Retrying , <*>, ",debug,"<org.apache.hadoop.io.retry.RetryInvocationHandler: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
A failover has occurred since the start of this method invocation attempt.,warn,"<org.apache.hadoop.io.retry.RetryInvocationHandler: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>"
"noteFailure , exception, ",debug,<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
"Service , <*>,  failed in state , <*>, ; cause: , exception, ",info,<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
"using plugin jars: , <*>, ",debug,<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.ClassLoader getPluginLoader()>
"addJerseyResourcePackage: packageName=, packageName, , pathSpec=, pathSpec, ",info,"<org.apache.hadoop.http.HttpServer2: void addJerseyResourcePackage(java.lang.String,java.lang.String)>"
"Metrics cache overflow at , <*>,  for , eldest, ",warn,<org.apache.hadoop.metrics2.util.MetricsCache$RecordCache: boolean removeEldestEntry(java.util.Map$Entry)>
Stop interrupted,warn,<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void stop()>
Deleting bread-crumb of active node...,info,<org.apache.hadoop.ha.ActiveStandbyElector: void tryDeleteOwnBreadCrumbNode()>
"Unable to delete our own bread-crumb of being active at , <*>, : , <*>, . , Expecting to be fenced by the next active., ",warn,<org.apache.hadoop.ha.ActiveStandbyElector: void tryDeleteOwnBreadCrumbNode()>
"Entering state , newState, ",info,<org.apache.hadoop.ha.HealthMonitor: void enterState(org.apache.hadoop.ha.HealthMonitor$State)>
"Get kerberos info proto:, <*>,  info:, <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
"getting serverKey: , <*>,  conf value: , <*>,  principal: , <*>, ",debug,<org.apache.hadoop.security.SaslRpcClient: java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
"Got brand-new decompressor , <*>, , ",info,<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)>
Got recycled decompressor,debug,<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)>
"Setting the excludes file to , excludesFile, ",info,<org.apache.hadoop.util.HostsFileReader: void setExcludesFile(java.lang.String)>
"Interrupted while joining on: , t, ",warn,<org.apache.hadoop.util.Shell: void joinThread(java.lang.Thread)>
"Service: , <*>,  entered state , <*>, ",debug,<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
"stopping client from cache: , client, ",debug,<org.apache.hadoop.ipc.ClientCache: void stopClient(org.apache.hadoop.ipc.Client)>
"removing client from cache: , client, ",debug,<org.apache.hadoop.ipc.ClientCache: void stopClient(org.apache.hadoop.ipc.Client)>
"stopping actual client because no more references remain: , client, ",debug,<org.apache.hadoop.ipc.ClientCache: void stopClient(org.apache.hadoop.ipc.Client)>
"Registered , <*>, ",debug,"<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName register(java.lang.String,java.lang.String,java.lang.Object)>"
"Failed to register MBean \, <*>, \, ",trace,"<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName register(java.lang.String,java.lang.String,java.lang.Object)>"
"Failed to register MBean \, <*>, \: Instance already exists., ",warn,"<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName register(java.lang.String,java.lang.String,java.lang.Object)>"
"Failed to register MBean \, <*>, \, ",warn,"<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName register(java.lang.String,java.lang.String,java.lang.Object)>"
Not attempting to re-login since the last re-login was attempted less than  seconds before.,warn,<org.apache.hadoop.security.UserGroupInformation: boolean hasSufficientTimeElapsed(long)>
"Got UserName , <*>,  for UID , <*>,  from the native implementation, ",info,<org.apache.hadoop.io.nativeio.NativeIO: java.lang.String getOwner(java.io.FileDescriptor)>
"Adding , <*>,  to the list of , type,  hosts from , filename, ",info,"<org.apache.hadoop.util.HostsFileReader: void readFileToSetWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Set)>"
"Updating layout version from , <*>,  to , <*>,  for storage , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Upgrading storage directory , <*>, .\n   old LV = , <*>, ; old CTime = , <*>, .\n   new LV = , <*>, ; new CTime = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Upgrade of , <*>,  is complete, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"replaying edit log: , op, ",trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
"<*>, : , <*>,  numblocks : , <*>,  clientHolder , <*>,  clientMachine , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
Reopening an already-closed file for append,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
"<*>, : , <*>,  numblocks : , <*>,  clientHolder , <*>,  clientMachine , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
"<*>, : , <*>,  numblocks : , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
"<*>, : , <*>,  new block id : , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
"Found corruption while reading , file, . Error repairing corrupt blocks. Bad blocks remain., ",info,"<org.apache.hadoop.hdfs.DFSClient: void reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
error closing blockReader,error,<org.apache.hadoop.hdfs.DFSInputStream: void closeCurrentBlockReader()>
"Inconsistent number of corrupt replicas for , blk,  blockMap has , <*>,  but corrupt replicas map has , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlock createLocatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long)>"
No header found in log,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>
EOF while reading layout flags from log,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>
Could not find ip address of \default\ inteface.,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String newBlockPoolID()>
"Analyzing storage directories for bpid , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"snapRoot, snapName",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logCreateSnapshot(java.lang.String,java.lang.String,boolean)>"
"snapRoot, snapName",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logCreateSnapshot(java.lang.String,java.lang.String,boolean)>"
Triggering a rollback fsimage for rolling upgrade.,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Triggering checkpoint because there have been , <*>,  txns since the last checkpoint, which , exceeds the configured threshold , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Triggering checkpoint because it has been , <*>,  seconds since the last checkpoint, which , exceeds the configured interval , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
But skipping this checkpoint since we are about to failover!,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Checkpoint was cancelled: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Interrupted during checkpointing,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Exception in doCheckpoint,error,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Processing previouly queued message , rbi, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processQueuedMessages(java.lang.Iterable)>
Encountered exception during format: ,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
"Upgrading to sequential block IDs. Generation stamp for new blocks set to , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
"load last allocated InodeId from fsimage:, <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Old layout version doesn\'t have inode id. Will assign new id for each inode.,debug,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
"Loading image file , curFile,  using , stampAtIdSwitch_, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
"Number of files = , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
"Image file , curFile,  of size , <*>,  bytes loaded in , <*>,  seconds., ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
"Failed to delete replica , <*>, : ReplicaInfo not found., ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
"Received non-NN/SNN/administrator request for image or edits from , <*>,  at , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>"
"Received an invalid request file transfer request from a secondary with storage info , theirStorageInfoString, ",warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>"
No edits directories configured!,error,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initJournals(java.util.List)>
"namenodes  = , namenodes, ",info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>"
"parameters = , p, ",info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>"
"this, : cache cleaner running at , <*>, ",debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
"CacheCleaner: purging , replica, : , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
"this, : finishing cache cleaner run started at , <*>, .  Demoted , <*>,  mmapped replicas; , purged , numPurged_,  replicas., ",debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
Updating block keys,info,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: boolean updateKeys()>
Expected edit log to be open for read,<init>,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int downloadImage(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol)>"
"removing shm , shm, ",debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm)>
,setTransactionId,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp decodeOp()>
"Checking block access token for block \', <*>, \' with mode \', mode, \', ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
"Block token verification failed: op=, op, , remoteAddress=, <*>, , message=, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
Cluster console encounters a not handled situtation.,warn,<org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: void getDecommissionNodeClusterState(java.util.Map)>
"Overwriting existing file , f,  with file downloaded from , url, ",warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
"Unable to download file , f, ",warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
"Queueing reported block , block,  in state , reportedState,  from datanode , <*>,  for later processing because , reason, ., ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String)>"
", <*>, path, <*>, <*>",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logMkDir(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode)>"
"Getting groups for user , user, ",debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: java.lang.String[] getGroupsForUser(java.lang.String)>
"Disabling journal , j, ",error,<org.apache.hadoop.hdfs.server.namenode.JournalSet: void disableAndReportErrorOnJournals(java.util.List)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEditLogProto)>
"Got journal, state = , <*>, ; firstTxId = , firstTxId, ; numTxns = , numTxns, ",trace,"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void journal(long,int,byte[])>"
,logGenerationStampV1,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: long nextGenerationStamp(boolean)>
,logGenerationStampV2,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: long nextGenerationStamp(boolean)>
"Block with id {}, pool {} already exists in the FsDatasetCache with state {}",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>"
"Initiating caching for Block with id {}, pool {}",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>"
Closing all peers.,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void closeAllPeers()>
"image loading failed at offset , <*>, ",error,<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer: void go()>
"Problem connecting to server: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>
"Removing replica , bpid, :, <*>,  on failed volume , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>
"Removed , removedBlocks_,  out of , totalBlocks_, (took , mlsec,  millisecs), ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>
Exiting on user request.,error,<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void quit()>
"id, expiryTime",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRenewDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
"Enabled trash for bpid , bpid, ",info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void enableTrash(java.lang.String)>
"The threshold value should\'t be greater than , threshold: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"dfs.namenode.safemode.threshold-pct = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"dfs.namenode.safemode.min.datanodes = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"dfs.namenode.safemode.extension     = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"LazyWriter was interrupted, exiting",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>
Ignoring exception in LazyWriter:,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>
"The volume list has been changed concurrently, retry to remove volume: , newVolume, ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
"Added new volume: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
"Failed to getReplicaVisibleLength from datanode , datanode,  for block , <*>, ",debug,<org.apache.hadoop.hdfs.DFSInputStream: long readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock)>
Timed out waiting for response from loggers,<init>,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: boolean hasSomeData()>
No KeyProvider found.,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"Found KeyProvider: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Enabling async auditlog,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"fsLock is fair:, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"fsOwner             = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"supergroup          = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"isPermissionEnabled = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"Determined nameservice ID: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"HA Enabled: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"Configured NNs:\n, <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"Append Enabled: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"<*>,  initialization failed., ",error,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"<*>,  initialization failed., ",error,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
"entry=, entry, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void check()>
"Start loading edits file , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Edits file , <*>,  of size , <*>,  edits # , <*>,  loaded in , <*>,  seconds, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Interrupted waiting to join on checkpointer thread,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Exception shutting down SecondaryNameNode,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Exception while closing CheckpointStorage,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Encountered exception while exiting state ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stop()>
"this, : trying to create ShortCircuitReplicaInfo., ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
"this, : allocShmSlot used up our previous socket , <*>, .  Allocating a new one..., ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
"this, : closing stale domain peer , peer_, ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
"this, : I/O error requesting file descriptors.  , Disabling domain socket , <*>, ",warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
"Block Pool , <*>,  is not alive, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void run()>
"Decided to move , this, ",debug,"<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: boolean markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.StorageType)>"
"Unsupported protocol found when creating the proxy connection to NameNode: , <*>_, ",error,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createNonHAProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean,java.util.concurrent.atomic.AtomicBoolean)>"
"BUG: Namespace quota violation in image for , <*>,  quota = , <*>,  < consumed = , child#, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>"
"BUG: Diskspace quota violation in image for , <*>,  quota = , <*>,  < consumed = , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>"
"Lease renewer daemon for , <*>,  with renew id , <*>,  started, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
"Lease renewer daemon for , <*>,  with renew id , <*>,  exited, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
"<*>,  is interrupted., ",debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
"Lease renewer daemon for , <*>,  with renew id , <*>,  exited, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
"Lease renewer daemon for , <*>,  with renew id , <*>,  exited, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
"Couldn\'t report bad block , block,  to , actor, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void reportRemoteBadBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>"
"Exiting , <*>,  due to an exception, ",error,<org.apache.hadoop.hdfs.server.mover.Mover: void main(java.lang.String[])>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrEditLogProto)>
"Purging old edit log , log, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeLog(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)>
"createNameNode , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.namenode.NameNode createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration)>"
All journals failed to abort,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void abortCurrentLogSegment()>
"Block token params received from NN: for block pool , blockPoolId,  keyUpdateInterval=, <*>,  min(s), tokenLifetime=, <*>,  min(s), ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void registerBlockPoolWithSecretManager(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String)>"
"Refresh request received for nameservices: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void refreshNamenodes(org.apache.hadoop.conf.Configuration)>
"Received non-NN/JN request for edits from , <*>, ",warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
.empty,renameSelf,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile: void moveAsideEmptyFile()>
"Beginning to copy stream , stream,  to shared edits, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
"copying op: , <*>, ",trace,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
,logEdit,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
"ending log segment because of END_LOG_SEGMENT op in , stream, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
"ending log segment because of end of stream in , stream, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
<*> L,closeAllStreams,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
"BLOCK* addToExcessReplicate: (, dn, , , block, ) is added to excessReplicateMap, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToExcessReplicate(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)>"
Nothing to flush,info,<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void flushAndSync(boolean)>
"DatanodeCommand action: DNA_CACHE for , <*>,  of , <*>, , ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"DatanodeCommand action: DNA_UNCACHE for , <*>,  of , <*>, , ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"Got finalize command for block pool , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_ACCESSKEYUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"Updating balance throttler bandwidth from , <*>,  bytes/s , to: , <*>,  bytes/s., ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"Unknown DatanodeCommand action: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"leaseToCheck_,  has expired hard limit, ",info,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
"Lease recovery for , p,  is complete. File closed., ",debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
"Started block recovery , p,  lease , leaseToCheck_, ",debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
"Cannot release the path , p,  in the lease , leaseToCheck_, ",error,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
"Unable to release hard-limit expired lease: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
"Failed to append to , <*>, , m=, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void append(long,long,long)>"
"url=, <*>, ",trace,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>"
"Waited , <*>, ms to read from , <*>, ; spawning hedged read, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: void hedgedFetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
"Failed getting node for hedged read: , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: void hedgedFetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
"Need to move , <*>,  to make the cluster balanced., ",info,<org.apache.hadoop.hdfs.server.balancer.Balancer: org.apache.hadoop.hdfs.server.balancer.Balancer$Result runOneIteration()>
"Will move , <*>,  in this iteration, ",info,<org.apache.hadoop.hdfs.server.balancer.Balancer: org.apache.hadoop.hdfs.server.balancer.Balancer$Result runOneIteration()>
"Received an RBW replica for , storedBlock,  on , dn, : ignoring it, since it is , complete with the same genstamp, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Unexpected replica state , reportedState,  for block: , storedBlock,  on , dn,  size , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Exiting balancer due an exception,error,<org.apache.hadoop.hdfs.server.balancer.Balancer: void main(java.lang.String[])>
"src, timestamp",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logDelete(java.lang.String,long,boolean)>"
"src, timestamp",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logDelete(java.lang.String,long,boolean)>"
"*BLOCK* NameNode.cacheReport: from , nodeReg,  , <*>,  blocks, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)>"
"Finalizing upgrade for journal , <*>, ., <*>_, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void doFinalize()>
"Number of files under construction = , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFilesUnderConstruction(java.io.DataInput,boolean,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>"
"BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block , block,  from priority queue , priLevel, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>"
"BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block , block,  from priority queue , i_, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>"
FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdown()>
"dfs.block.access.token.enable=, <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
Security is enabled but block access tokens (via dfs.block.access.token.enable) aren\'t enabled. This may cause issues when clients attempt to talk to a DataNode.,error,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
"dfs.block.access.key.update.interval=, <*>,  min(s), , dfs.block.access.token.lifetime, =, <*>,  min(s), , dfs.encrypt.data.transfer.algorithm, =, <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
"Starting a new period : work left in prev period : , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void startNewPeriod()>
"this,  beginning handshake with NN, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
"Problem connecting to server: , <*>,  :, <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
"Problem connecting to server: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
"Block pool , this,  successfully registered with NN, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
Setting block keys,info,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void addKeys(org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys)>
this cycle terminating immediately because \'shouldRun\' has been deactivated,warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
Exception during DirectoryScanner execution - will continue next cycle,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
System Error during DirectoryScanner execution - permanently terminating periodic scanner,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
"caught exception initializing , this, ",error,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
"skipping , skipAmt,  bytes at the end , of edit log  \', <*>, \': reached txid , <*>,  out of , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
"No block pool scanner found for block pool id: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Directive {}: not scanning file {} because bytesNeeded for pool {} is {}, but the pool\'s limit is {}",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
"Directive {}: can\'t cache block {} because it is in state {}, not COMPLETE.",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Directive {}: setting replication for block {} to {},trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Directive {}: caching {}: {}/{} bytes,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
,logReassignLease,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease reassignLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Failed to parse options,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>
Failed to start secondary namenode,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>
"Unable to abort file , <*>, ",warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>
"Unable to delete tmp file during abort , <*>, ",warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>
"Server using cipher suite , <*>, ",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(java.io.OutputStream,java.io.InputStream,java.util.Map,javax.security.auth.callback.CallbackHandler)>"
Error converting file to URI,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory getStorageDirectory(java.net.URI)>
"Storage directory , <*>,  does not exist, ",warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
"<*>,  does not exist. Creating ..., ",info,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
"<*>, is not a directory, ",warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
"Cannot access storage directory , <*>, ",warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
"Cannot access storage directory , <*>, ",warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
"this, : the DfsClientShmManager isclosed., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,org.apache.hadoop.hdfs.ExtendedBlockId,java.lang.String)>"
"Get corrupt file blocks returned error: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.lang.String getCorruptFiles()>
"FSDirectory.verifyMaxDirItems: , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxDirItems(org.apache.hadoop.hdfs.server.namenode.INode[],int)>"
"this, : the DfsClientShmManager has been closed., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
"this, : shared memory segment access is disabled., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
"this, : waiting for loading to finish..., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
"this, : the UNIX domain socket associated with , this short-circuit memory closed before we could make , use of the shm., ",debug,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
"<*>,  interrupted: , <*>, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void run()>
"Cannot find BPOfferService for reporting block received for bpid=, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void closeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>"
"BLOCK* processIncrementalBlockReport is received from dead or unregistered node , nodeID, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"Unknown block status code reported by , nodeID, : , rdbi, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"BLOCK* block , <*>, : , <*>,  is received from , nodeID, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"*BLOCK* NameNode.processIncrementalBlockReport: from , nodeID,  receiving: , receiving_, , ,  received: , received_, , ,  deleted: , deleted_, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"Added volume - , <*>, , StorageType: , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(java.util.Collection,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>"
Reached EOF when reading log header,<init>,"<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: int readLogVersion(java.io.DataInputStream,boolean)>"
"Unexpected version of the file system log file: , <*>, . Current version = , <*>, ., ",<init>,"<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: int readLogVersion(java.io.DataInputStream,boolean)>"
"Unable to rename checkpoint in , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>"
"BLOCK* removeDeadDatanode: lost heartbeat from , d_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDeadDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
"demoteOldEvictable: demoting , <*>, : , <*>_, : , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: int demoteOldEvictableMmaped(long)>
"getAdditionalDatanode: src=, src, , fileId=, fileId, , blk=, blk, , existings=, <*>, , excludes=, <*>, , numAdditionalNodes=, numAdditionalNodes, , clientName=, clientName, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)>"
"Processed cache report from {}, blocks: {}, processing time: {} msecs",debug,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List)>"
Image has not changed. Will not download image.,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>
Image has changed. Downloading updated image from NN.,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>
"<*>, ",logFinalizeRollingUpgrade,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void finalizeRollingUpgrade()>
"Storage directory , <*>,  does not contain previous fs state., ",info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: boolean canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int)>"
"Could not sync enough journals to persistent storage due to , <*>, . , Unsynced transactions: , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>
"Could not sync enough journals to persistent storage. Unsynced transactions: , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>
,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void linkAllBlocks(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.io.File,java.io.File)>"
AsyncDiskService has already shut down.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
Shutting down all async disk service threads,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
All async disk service threads have been shut down,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
"DIR* FSDirectory.removeBlock: , path,  with , block,  block is removed from the file system, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRemoveBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>"
"namenodes = , namenodes, ",info,"<org.apache.hadoop.hdfs.server.mover.Mover: int run(java.util.Map,org.apache.hadoop.conf.Configuration)>"
Nothing to flush,info,<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void flushAndSync(boolean)>
"BLOCK* removeDatanode: , node,  does not exist, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
"readNextPacket: dataPlusChecksumLen = , dataPlusChecksumLen,  headerLen = , $i, ",trace,"<org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: void doRead(java.nio.channels.ReadableByteChannel,java.io.InputStream)>"
"Updating lastPromisedEpoch from , <*>,  to , newEpoch,  for client , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void updateLastPromisedEpoch(long)>
GOT EXCEPITION,trace,<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
INTERNAL_SERVER_ERROR,warn,<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
"<*>, .wipeDatanode(, node, ): storage , <*>,  is removed from datanodeMap., ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
"BLOCK* addBlock: logged info for , <*>,  of , i$__,  reported., ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"BLOCK* addBlock: block , b_#,  on , <*>,  size , <*>,  does not belong to any file, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Periodic Block Verification scan disabled because , reason#__, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataBlockScanner(org.apache.hadoop.conf.Configuration)>
"Incompatible build versions: active name-node BV = , <*>, ; backup node BV = , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol)>
"Rollback of , <*>,  is complete., ",info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Successfully connected to , <*>,  for , <*>, ",info,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to , <*>,  : , <*>, ",info,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
"Failed to connect to , <*>,  for block, , add to deadNodes and continue. , <*>, ",warn,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
"response.isCommitted()=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.StreamFile: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"About to load edits:\n  , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Reading , editIn,  expecting start txid #, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Will rename reserved path , <*>,  to , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: void setRenameReservedMapInternal(java.lang.String)>
Web server init done,info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
"Checkpoint Period   :, <*>,  secs , (, <*>,  min), ",info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
"Log Size Trigger    :, <*>,  txns, ",info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
"Will roll logs on active node at , <*>,  every , <*>,  seconds., ",info,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
Not going to trigger log rolls on active node because dfs.ha.log-roll.period is negative.,info,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"logRollPeriodMs=, <*>,  sleepTime=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"ACCESS CHECK: , this, , doCheckOwner=, doCheckOwner, , ancestorAccess=, ancestorAccess, , parentAccess=, parentAccess, , access=, access, , subAccess=, subAccess, , ignoreEmptyDir=, ignoreEmptyDir, , resolveLink=, resolveLink, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkPermission(java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSDirectory,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean,boolean)>"
"Adding an already existing block , block, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"<*>, Re-checking all blocks for replication, since they should now be replicated cross-rack, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"<*>, Not checking for mis-replicated blocks because this NN is not yet processing repl queues., ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Storage directory , dataDir,  does not exist, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Storage directory , dataDir,  is not formatted for , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"BLOCK* blockReceived: , block,  is expected to be removed from an unrecorded node , delHint, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,java.lang.String)>"
"this, : can\'t fetchOrCreate , key,  because the cache is closed., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>"
"this, : retrying , <*>, ",debug,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>"
"Lease renewer daemon for , <*>,  with renew id , id,  executed, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
"Failed to renew lease for , <*>,  for , <*>,  seconds.  Aborting ..., ",warn,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
"Failed to renew lease for , <*>,  for , <*>,  seconds.  Will retry shortly ..., ",warn,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
"Lease renewer daemon for , <*>,  with renew id , id,  is not current, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
"Lease renewer daemon for , <*>,  with renew id , id,  expired, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
"DIR* completeFile: request from , holder,  to complete inode , fileId, (, src_, ) which is already closed. But, it appears to be , an RPC retry. Returning success, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFileInternal(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long)>"
Exception shutting down key updater thread,warn,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void close()>
"Loading , <*>,  strings, ",info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.lang.String[] loadStringTable(java.io.InputStream)>
"Closing journal storage for , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void close()>
"LazyWriter: Start persisting RamDisk block: block pool Id: , <*>,  block id: , <*>,  on target volume , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
"Exception saving replica , block_, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
"Failed to save replica , block_, . re-enqueueing it., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
"Failed to save replica , block_, . re-enqueueing it., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
"DataNode.handleDiskError: Keep Running: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>
"DataNode is shutting down: , errMsgr, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>
"Unable to stop existing writer for block , b,  after , writerStopMs,  miniseconds., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline createTemporary(org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>"
Interrupted while waiting for replicationQueueInitializer. Returning..,warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void stopReplicationInitializer()>
"Renaming , <*>,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void bumpReplicaGS(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,long)>"
"Error reported on storage directory , sd, ",error,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"current list of storage dirs:, <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"About to remove corresponding storage: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Unable to unlock bad storage directory: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"at the end current list of storage dirs:, <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"UnresolvedPathException  path: , <*>,  preceding: , <*>,  count: , count_,  link: , <*>,  target: , <*>,  remainder: , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.INodesInPath: org.apache.hadoop.hdfs.server.namenode.INodesInPath resolve(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,byte[][],int,boolean)>"
id,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logCancelDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier)>
"Synchronizing log , <*>, : no current segment in place, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
"Synchronizing log , <*>, : old segment , <*>,  is not the right length, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
"<*>, ",alwaysAssert,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
"Skipping download of log , <*>, : already have up-to-date logs, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
"Accepted recovery for segment , <*>, : , <*>, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
", <*> <*>",<init>,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
"Checkpointer about to load edits from , <*>,  stream(s)., ",info,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
"Starting standby checkpoint thread...\nCheckpointing active NN at , <*>, \n, Serving checkpoints at , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void start()>
"Added filter \', <*>, \' (class=, <*>, ), ",info,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration)>
"!!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \, propertyName, \ in hdfs-site.xml;, \n\t\t- use Backup Node as a persistent and up-to-date storage , of the file system meta-data., ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection getStorageDirs(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"<*>, : seqno=, seqno,  waiting for local datanode to finish write., ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet waitForAckHead(long)>
"*DIR* NameNode.create: file , src,  for , clientName,  at , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>"
"take(): poll() returned null, sleeping for {} ms",debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch take()>
"Failed to cache block with id , blockId, , pool , bpid, : ReplicaInfo not found., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Failed to cache block with id , blockId, , pool , bpid, : replica is not finalized; it is in state , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Failed to cache block with id , blockId, , pool , bpid, : volume not found., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Failed to cache block with id , blockId, : volume was not an instance of FsVolumeImpl., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Caching not supported on block with id , blockId,  since the volume is backed by RAM., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Removing non-existent lease! holder=, holder,  src=, src, ",warn,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(java.lang.String,java.lang.String)>"
Wait for lease checker to terminate,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void interruptAndJoin()>
"STATE* Safe mode is ON, <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enterSafeMode(boolean)>
"Error reported on file , f, ... exiting, ",fatal,<org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter: void reportErrorOnFile(java.io.File)>
"Request for token received with no authentication from , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"Exception while renewing token. Re-throwing. s=, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"Is namenode in safemode? , <*>, ; uri=, uri, ",debug,<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
"Got an exception for uri=, uri, ",debug,<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
Failed to read previous verification times.,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean assignInitialVerificationTimes()>
initializing replication queues,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void initializeReplQueues()>
Successfully saved namespace for preparing rolling upgrade.,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startRollingUpgradeInternalForNonHA(long)>
"Clients are to use , <*>,  to access,  this namenode/service., ",info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void initialize(org.apache.hadoop.conf.Configuration)>
,add,<org.apache.hadoop.hdfs.server.balancer.Dispatcher: java.util.List init()>
"Going to check the following volumes disk space: , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: java.util.Collection getVolumesLowOnSpace()>
"Unexpected error trying to , <*>_,  block , <*>,  , <*>,  at file , <*>, . Ignored., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>
"Deleted , <*>,  , <*>,  file , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>
,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts parseArgs(java.lang.String[])>
"recoverBlocks FAILED: , b, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$4: void run()>
"Retrieving token from: , <*>, ",debug,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: org.apache.hadoop.security.Credentials getDTfromRemote(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,java.lang.String,java.lang.String)>"
"concat , <*>,  to , target, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concat(java.lang.String,java.lang.String[])>"
"Removed volume: , target, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
"The volume list has been changed concurrently, retry to remove volume: , target, ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
"Volume , target,  does not exist or is removed by others., ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
DFS Read,warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readWithStrategy(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int)>"
"Waiting for ack for: , seqno, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>
"Slow waitForAckedSeqno took , <*>, ms (threshold=, <*>, ms), ",warn,<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>
"Checking access for user=, userId, , block=, block, , access mode=, mode,  using , <*>, ",debug,"<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
"Could not send read status (, statusCode, ) to datanode , <*>, : , <*>, ",info,"<org.apache.hadoop.hdfs.RemoteBlockReader: void sendReadResult(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
"*BLOCK* NameNode.abandonBlock: , b,  of file , src, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
there are no corrupt file blocks.,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>"
"list corrupt file blocks returned: , count_, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>"
"Skipping jas , jas,  since it\'s disabled, ",info,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>"
"Unable to determine input streams from , <*>, . Skipping., ",warn,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>"
"Last block locations not available. Datanodes might not have reported blocks completely. Will retry for , retriesForLastBlockLength_,  times, ",warn,<org.apache.hadoop.hdfs.DFSInputStream: void openInfo()>
"block , block, : , <*>, ",trace,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"Total number of blocks            = , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"Number of invalid blocks          = , nrInvalid_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"Number of under-replicated blocks = , nrUnderReplicated_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"Number of  over-replicated blocks = , nrOverReplicated_, <*>_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"Number of blocks being written    = , nrUnderConstruction_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in , <*>,  msec, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Interrupted while processing replication queues.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"writeTransactionIdToStorage failed on , sd, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void writeTransactionIdFileToStorage(long)>
"Not able to receive block , <*>,  from , <*>,  because threads , quota is exceeded., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Connecting to datanode , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Moved , block,  from , <*>, , delHint=, delHint, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Error writing reply back to , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"opReplaceBlock , block,  received exception , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Error writing reply back to , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Request for token received with no authentication from , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"Sending token: {, <*>, ,, <*>, }, ",info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Exception while sending token. Re-throwing ,info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"<*>, <*>, . Throwing a BlockMissingException, ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
"No node available for , <*>, ",info,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
"Could not obtain , <*>,  from any node: , <*>, <*>, . Will get new block locations from namenode and retry..., ",info,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
"DFS chooseDataNode: got # , <*>,  IOException, will wait for , <*>,  msec., ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
"CopyOnWrite for block , this, ",info,<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: boolean unlinkBlock(int)>
"Connecting to datanode , <*>,  addr=, <*>, ",debug,"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean,org.apache.hadoop.hdfs.protocol.LocatedBlock)>"
"this, : trimEvictionMaps is purging , replica_, <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void trimEvictionMaps()>
"poll(): lastReadTxid is -, reading current txid from NN",debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>
poll(): read no edits from the NN when requesting edits after txid {},debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>
"this, : unregisterSlot , slotIdx, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void unregisterSlot(int)>
"persistBlocks: , path,  with , <*>,  blocks is persisted to,  the file system, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
Stopping ReplicationMonitor.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
ReplicationMonitor received an exception while shutting down.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
Stopping ReplicationMonitor for testing.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
ReplicationMonitor thread received Runtime exception. ,fatal,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
"Update , oldBlock,  (len = , <*>, ) to an older state: , newBlock,  (len = , <*>, ), ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean)>"
,persistBlocks,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean)>"
"src, replication",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSetReplication(java.lang.String,short)>"
"Removed bpid=, blockPoolId,  from blockPoolScannerMap, ",info,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void removeBlockPool(java.lang.String)>
"loaded , <*>,  bytes into bounce , buffer from offset , oldDataPos,  of , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: boolean fillDataBuf(boolean)>
Uncaching {} now that it is no longer in use by any clients.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
Forcibly uncaching {} after {} because client(s) {} refused to stop using it.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
Replica {} still can\'t be uncached because some clients continue to use it.  Will wait for {},info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
<*> <*> <*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void flushAndSync(boolean)>
Error encountered requiring NN shutdown. Shutting down immediately.,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: void doImmediateShutdown(java.lang.Throwable)>
"selectInputStream manifests:\n, <*>, ",debug,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>"
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void closeResponder()>
"DIR* NameSystem.startFile: added , src,  inode , <*>,  , holder, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>"
"DIR* NameSystem.startFile: , src,  , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>"
"who,  calls recoverBlock(, <*>, , targets=, <*>, , , newGenerationStamp=, <*>, ), ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void logRecoverBlock(java.lang.String,org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>"
"Will connect to NameNode at , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.net.URL getInfoServer()>
"Edit log file , <*>,  appears to be empty. , Moving it aside..., ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>
"getSegmentInfo(, segmentTxId, ): , <*>,  -> , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>
"DIR* NameSystem.completeFile: , srcArg,  for , holder, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
"DIR* completeFile: , srcArg,  is closed by , holder, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
"dfs.namenode.startup.delay.block.deletion.sec is set to , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>
"The block deletion will start around , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>
"data:, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void applyEdits(long,int,byte[])>"
"Sending OOB to peer: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void sendOOB()>
SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>"
Connection closed by client,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>"
"Failed to delete block file , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
"Failed to delete meta file , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
"Unexpected meta-file version for , name, : version in file is , $i,  but expected version is , , ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: org.apache.hadoop.util.DataChecksum readDataChecksum(java.io.DataInputStream,java.lang.Object)>"
"Invalid namespaceID in journal request - expected , <*>,  actual , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>
"Invalid clusterId in journal request - expected , <*>,  actual , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>
"Adjusting safe-mode totals for deletion.decreasing safeBlocks by , numRemovedSafe_, , totalBlocks by , numRemovedComplete_, ",debug,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo)>
"Caught exception after reading , numValid_,  ops from , in,  while determining its valid length., Position was , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"After resync, position is , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"Reloading namespace from , file, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void reloadFromImageFile(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
"Prepared recovery for segment , segmentTxId, : , <*>, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long)>"
"Cached location of block , blk,  as , <*>, ",debug,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockPathInfo(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.security.token.Token,boolean,org.apache.hadoop.hdfs.StorageType)>"
"BLOCK* markBlockReplicasAsCorrupt: mark block replica, b_,  on , <*>,  as corrupt because the dn is not in the new committed , storage list., ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockReplicasAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,long,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>"
"Namenode , actor,  trying to claim ACTIVE state with , txid=, <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
"NN , actor,  tried to claim ACTIVE state at txid=, <*>,  but there was already a more recent claim at txid=, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
"Acknowledging ACTIVE Namenode , actor, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
"Namenode , actor,  taking over ACTIVE state from , <*>,  at higher txid=, <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
"Namenode , actor,  relinquishing ACTIVE state with , txid=, <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
"Update block keys every , <*>, ",info,"<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void <init>(org.apache.hadoop.hdfs.server.balancer.KeyManager,long)>"
"*DIR* NameNode.mkdirs: , src, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>"
The resolve call returned null!,error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
"Unresolved topology mapping for host , <*>, ",<init>,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Unexpected safe mode action,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction)>
"Downloaded file , <*>,  size , <*>,  bytes., ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash handleUploadImageRequest(javax.servlet.http.HttpServletRequest,long,org.apache.hadoop.hdfs.server.common.Storage,java.io.InputStream,long,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
"Image upload with txid , txid,  conflicted with a previous image upload to the , same NameNode. Continuing..., ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>"
"Uploaded image with txid , txid,  to namenode at , fsName,  in , <*>,  seconds, ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>"
"Caught exception after reading , numValid_,  ops from , in,  while determining its valid length., Position was , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"After resync, position is , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
,mergeManifest,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto)>
"Removing lazyPersist file , <*>,  with no replicas., ",warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void clearCorruptLazyPersistFiles()>
Log not rolled,checkNameNodeSafeMode,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.CheckpointSignature rollEditLog()>
"Roll Edit Log from , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.CheckpointSignature rollEditLog()>
Unable to drop cache on file close,warn,<org.apache.hadoop.hdfs.server.datanode.BlockSender: void close()>
"Failed to get the storage policy of file , fullPath, ",warn,"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processFile(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus)>"
"Failed to check the status of , parent, . Ignore it and continue., ",warn,"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processRecursively(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus)>"
"Clusterid mismatch - current clusterid: , <*>, , Ignoring given clusterid: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>"
"Using clusterid: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>"
"Cannot find BPOfferService for reporting block received for bpid=, <*>, ",error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>"
DatanodeCommand action from standby: DNA_ACCESSKEYUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"Got a command from standby NN - ignoring command:, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"Unknown DatanodeCommand action: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
"<*>, :DataXceiverServer.kill(): , ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void kill()>
"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold = , <*>, , , dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,  = , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is greater than . but should be in the range . - .,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is less than . so volumes with less available disk space will receive more block allocations,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
"Failed to delete block file , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>
"Failed to delete meta file , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>
"Generated and persisted new Datanode UUID , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDatanodeUuid()>
SKIP_UNTIL,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State: void <clinit>()>
OK,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State: void <clinit>()>
STREAM_FAILED,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State: void <clinit>()>
STREAM_FAILED_RESYNC,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State: void <clinit>()>
EOF,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State: void <clinit>()>
"Error parsing protocol buffer of EZ XAttr , <*>,  dir:, <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void addEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields,org.apache.hadoop.hdfs.server.namenode.XAttrFeature)>"
"Received , <*>,  size , <*>,  from , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void finalizeBlock(long)>
"Connecting to datanode , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>"
"Send buf size , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>"
Could not get full path. Corresponding file might have deleted already.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INode[] getRelativePathINodes(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
"Created , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>
"<*>,  already exists., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>
"Block , blockFile,  does not have a metafile!, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: long getGenerationStampFromFile(java.io.File[],java.io.File)>"
editLog must be initialized,<init>,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>"
Save namespace ...,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>"
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
"Failed to send success response back to the client.  Shutting down socket for , <*>, ., ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
"Failed to send success response back to the client.  Shutting down socket for , <*>, ., ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
"Failed to send success response back to the client.  Shutting down socket for , <*>, ., ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
"Received exception in Datanode#join: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void join()>
"DIR* NameSystem.delete: , src, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInt(java.lang.String,boolean,boolean)>"
OP_ADD,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_RENAME_OLD,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_DELETE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_MKDIR,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_REPLICATION,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_DATANODE_ADD,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_DATANODE_REMOVE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_PERMISSIONS,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_OWNER,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_CLOSE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_GENSTAMP_V,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_NS_QUOTA,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_CLEAR_NS_QUOTA,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_TIMES,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_QUOTA,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_RENAME,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_CONCAT_DELETE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SYMLINK,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_GET_DELEGATION_TOKEN,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_RENEW_DELEGATION_TOKEN,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_CANCEL_DELEGATION_TOKEN,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_UPDATE_MASTER_KEY,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_REASSIGN_LEASE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_END_LOG_SEGMENT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_START_LOG_SEGMENT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_UPDATE_BLOCKS,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_CREATE_SNAPSHOT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_DELETE_SNAPSHOT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_RENAME_SNAPSHOT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ALLOW_SNAPSHOT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_DISALLOW_SNAPSHOT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_GENSTAMP_V,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ALLOCATE_BLOCK_ID,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ADD_BLOCK,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ADD_CACHE_DIRECTIVE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_REMOVE_CACHE_DIRECTIVE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ADD_CACHE_POOL,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_MODIFY_CACHE_POOL,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_REMOVE_CACHE_POOL,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_MODIFY_CACHE_DIRECTIVE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_ACL,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ROLLING_UPGRADE_START,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_ROLLING_UPGRADE_FINALIZE,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_XATTR,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_REMOVE_XATTR,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_SET_STORAGE_POLICY,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
OP_INVALID,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes: void <clinit>()>
"<*>, \n, <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: void stopWriter(long)>
"nextDomainPeer: reusing existing peer , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextDomainPeer()>
"DIR* NameSystem.concat: , <*>,  to , target, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concatInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String[],boolean)>"
"ServicePlugin , p,  could not be stopped, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopCommonServices()>
"Server using encryption algorithm , <*>, ",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream)>"
"DIR* FSDirectory.unprotectedDelete: , <*>,  is removed, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>"
"Finalizing upgrade for storage directory , <*>, .\n   cur LV = , <*>, ; cur CTime = , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doFinalize(java.io.File)>
"<*>_, Verification succeeded for , block, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"block,  is no longer in the dataset, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Verification failed for , block,  - may be due to race with write, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"<*>_, Verification failed for , block, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"closeFile: , path,  with , <*>,  blocks is persisted to the file system, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void closeFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
"Using NN principal: , <*>, ",debug,<org.apache.hadoop.hdfs.tools.GetGroups: void setConf(org.apache.hadoop.conf.Configuration)>
"Formatting , this,  with namespace info: , nsInfo, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair createStreamPair(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherOption,java.io.OutputStream,java.io.InputStream,boolean)>"
"Starting log segment at , segmentTxId, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void startLogSegment(long,boolean)>"
"Syntax error in URI , s, . Please check hdfs configuration., ",error,<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>
"Path , s,  should be specified as a URI , in configuration files. Please update hdfs configuration., ",warn,<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>
"Storage directory , <*>,  has been successfully formatted., ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Failed to report to name-node.,error,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void stop()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Allocating new block,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
"Append to block , <*>, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
"DataStreamer block , <*>,  sending packet , e__, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
DataStreamer Exception,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
"*DIR* Namenode.delete: src=, src, , recursive=, recursive, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean delete(java.lang.String,boolean)>"
"Failed to save replica , <*>, . re-enqueueing it., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onFailLazyPersist(java.lang.String,long)>"
"skip , n, ",debug,<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: long skip(long)>
"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}",debug,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
"DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}",debug,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
"creating , <*>, (shmId=, shmId, , mmappedLength=, <*>, , baseAddress=, <*>, , slots.length=, <*>, ), ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void <init>(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$ShmId,java.io.FileInputStream)>"
"Unable to delete cancelled checkpoint in , sd, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImage: void deleteCancelledCheckpoint(long)>
"deleteBlockPool command received for block pool , blockPoolId, , force=, force, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>"
"The block pool , blockPoolId,  is still running, cannot be deleted., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>"
"Adding new volumes: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
"Storage directory is loaded: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
"Deactivating volumes: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
"DIR* FSDirectory.renameTo: , src,  to , dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean renameTo(java.lang.String,java.lang.String,long)>"
Got error when sending OOB message.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>
Interrupted when sending OOB message.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>
"Name checkpoint time is newer than edits, not loading edits.",debug,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestEditsFiles()>
,mergeManifest,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto)>
,setHttpPort,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto)>
"BLOCK* getBlocks: Asking for blocks from an unrecorded node , datanode, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations getBlocksWithLocations(org.apache.hadoop.hdfs.protocol.DatanodeID,long)>"
"*DIR* NameNode.rename: , src,  to , dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean rename(java.lang.String,java.lang.String)>"
"BLOCK markBlockAsCorrupt: , b,  cannot be marked as corrupt as it does not belong to any file, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"storageInfo,  failed., ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateFailedStorage(java.util.Set)>
"DFSClient readChunk got header , $u, ",debug,"<org.apache.hadoop.hdfs.RemoteBlockReader: int readChunk(long,byte[],int,int,byte[])>"
"A packet was last sent , diff,  milliseconds ago., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: boolean packetSentInTime()>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogManifestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogManifestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogManifestProto)>
"Resolved path is , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: java.lang.String constructRemainingPath(java.lang.String,byte[][],int)>"
Failed to send data:,trace,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
"Failed to send data: , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
BlockSender.sendChunks() exception: ,error,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
"DIR* Namesystem.delete: , <*>,  is removed, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInternal(java.lang.String,boolean,boolean,boolean)>"
Starting services required for standby state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startStandbyServices(org.apache.hadoop.conf.Configuration)>
"Using UGI token: , <*>, ",debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>
"Fetched new token: , <*>, ",debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>
"DIR* FSDirectory.unprotectedRenameTo: , rename destination cannot be the root, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"DIR* FSDirectory.unprotectedRenameTo: , src,  is renamed to , dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"DIR* FSDirectory.unprotectedRenameTo: failed to rename , src,  to , dst, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"Registration IDs mismatched: the , <*>,  ID is , <*>,  but the expected ID is , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifyRequest(org.apache.hadoop.hdfs.server.protocol.NodeRegistration)>
"modifyDirective of , idString,  failed: , ",warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
modifyDirective of {} successfully applied {}.,info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
"Removing backup journal , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void releaseBackupStream(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>
"Block token params received from NN: update interval=, <*>, , token lifetime=, <*>, ",info,"<org.apache.hadoop.hdfs.server.balancer.KeyManager: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol,boolean,org.apache.hadoop.conf.Configuration)>"
"No excess replica can be found. excessTypes: , excessTypes, . moreThanOne: , <*>, . exactlyOne: , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.util.List chooseReplicasToDelete(java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Reported block , block,  on , <*>,  size , <*>,  replicaState = , reportedState, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>"
"In memory blockUCState = , <*>, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>"
"Could not send read status (, statusCode, ) to datanode , <*>, : , <*>, ",info,<org.apache.hadoop.hdfs.RemoteBlockReader2: void sendReadResult(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
"recoverLease: , <*>, , src=, src,  from client , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"startFile: recover , <*>, , src=, src,  client , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"Initializing journal in directory , <*>, ",info,"<org.apache.hadoop.hdfs.qjournal.server.JournalNode: org.apache.hadoop.hdfs.qjournal.server.Journal getOrCreateJournal(java.lang.String,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Error when dealing remote token:,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>"
"rethrowing exception from HTTP request: , <*>, ",info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>"
"Ignore the lease of file , p,  for checkpoint since the file is not under construction, ",warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map getINodesUnderConstruction()>
"this, : registerSlot , slotIdx, : allocatedSlots=, <*>, <*>, ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)>"
"Verifying QOP, requested QOP = {}, negotiated QOP = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: void checkSaslComplete(org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant,java.util.Map)>"
"BLOCK* NameSystem.abandonBlock: , b, of file , src, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
"BLOCK* NameSystem.abandonBlock: , b,  is removed from pendingCreates, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
"Starting BPOfferServices for nameservices: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
"Stopping BPOfferServices for nameservices: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
"Refreshing list of NNs for nameservices: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
"nextTcpPeer: reusing existing peer , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
"nextTcpPeer: created newConnectedPeer , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
"nextTcpPeer: failed to create newConnectedPeer connected to , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
"ListPathsServlet - Path , p,  does not exist, ",warn,<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet$2: java.lang.Void run()>
dfs.namenode.max.op.size ,setMaxOpSize,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Planning to load edit log stream: , elis_, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
No edit log streams selected.,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Failed to load image from , i$_, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
<*> toAtLeastTxId_ recovery ,closeAllStreams,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Error processing datanode Command,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: boolean processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[])>
"this, : can\'t create client mmap for , replica,  because we failed to , create one just , delta, ms ago., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getOrCreateClientMmap(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica,boolean)>"
"this, : retrying client mmap for , replica, , , delta,  ms after the previous failure., ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getOrCreateClientMmap(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica,boolean)>"
"Receiving one packet for block , <*>, : , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Slow BlockReceiver write packet to mirror took , duration_, ms (threshold=, <*>, ms), ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Receiving an empty packet or the end of the block , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"receivePacket for , <*>, : previous write did not end at the chunk boundary.,  onDiskLen=, <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Slow BlockReceiver write data to disk cost:, <*>, ms (threshold=, <*>, ms), ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Writing out partial crc for data len , <*>, , skip=, , ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Number of storages reported in heartbeat=, <*>, ; Number of storages in storageMap=, <*>, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
"Removed storage , len$,  from DataNode, this, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
"Deferring removal of stale storage , len$,  with , <*>,  blocks, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
"Starting web server as: , <*>, ",info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
"Starting Web-server for , name,  at: , <*>, ",info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
"Starting Web-server for , name,  at: , <*>, ",info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
"Got: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkReadAccess(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Failed to create RPC proxy to NameNode,error,<org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider: org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo getProxy()>
Received null remoteUser while authorizing access to getImage servlet,warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
SecondaryNameNode principal could not be added,debug,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"ImageServlet allowing checkpointer: , remoteUser, ",info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"ImageServlet allowing administrator: , remoteUser, ",info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"ImageServlet rejecting: , remoteUser, ",info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
"SASL server doing encrypted handshake for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in unsecured configuration for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in secured configuration for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server doing general handshake for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"invalidateCorruptReplicas error in deleting bad block , blk,  on , node, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>
Received exception: ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void rollVerificationLogs()>
"Couldn\'t connect to , <*>, , assuming security is disabled, ",warn,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
Exception getting delegation token,debug,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
"Got dt for , <*>, ;t.service=, <*>, ",debug,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
"Moved , blockFile,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
"Moved , metaFile,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
"Changing meta file offset of block , b,  from , <*>,  to , newPos, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)>"
"Performing recovery in , <*>,  and , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>
"Unable to delete dir , <*>,  before rename, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>
*DIR* reportBadBlocks,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])>
Dispatcher thread failed,warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher: long dispatchBlockMoves()>
"Block pool ID needed, but service not yet registered with NN",warn,<org.apache.hadoop.hdfs.server.datanode.BPOfferService: java.lang.String getBlockPoolId()>
"Standby Checkpointer should only attempt a checkpoint when NN is in standby mode, but the edit logs are in an unexpected state",<init>,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>
"A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid , <*>, . Skipping..., ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>
"this, : loading , key, ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
"this, : failed to load , key, ",warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
"this, : successfully loaded , <*>, ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
"this, : could not load , key,  due to InvalidToken , exception., ",warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
"this, : failed to load , key, ",warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
"<*>, : about to release , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
"<*>, : released , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
"<*>, : failed to release , short-circuit shared memory slot , <*>,  by sending , ReleaseShortCircuitAccessRequestProto to , <*>, .  Closing shared memory segment., ",error,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
"this,  can\'t register a slot because the , ShortCircuitRegistry is not enabled., ",trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>"
"this, : registered , blockId,  with slot , slotId,  (isCached=, isCached, ), ",trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>"
"Formatting journal , <*>,  with nsid: , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
"Recovering , lease, , src=, src, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"DIR* NameSystem.internalReleaseLease: attempt to release a create lock on , src,  but file is already closed., ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"DIR* NameSystem.internalReleaseLease: Failed to release lease for file , src, . Committed blocks are waiting to be minimally replicated.,  Try again later., ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
BLOCK* internalReleaseLease: Removed empty last block and closed file.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"DIR* NameSystem.internalReleaseLease: File , src,  has not been closed.,  Lease recovery is in progress. , RecoveryId = , <*>,  for block , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"Restarting previously-stopped writes to , <*>,  in segment starting at txid , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10: java.lang.Void call()>
Both short-circuit local reads and UNIX domain socket are disabled.,debug,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
"feature#__,  cannot be used because , <*>, ",warn,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
"feature#__,  is enabled., ",debug,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
"Generated new storageID , <*>,  for directory , <*>, <*>_, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: boolean createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,boolean)>"
"HTTP , <*>, : , op, , , path, , ugi=, ugi, , , username, , , doAsUser, <*>, ",trace,"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>"
"src, mtime, atime",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logTimes(java.lang.String,long,long)>"
Problem getting block size,warn,<org.apache.hadoop.hdfs.DFSClient: long getBlockSize(java.lang.String)>
"BlockReader failed to seek to , targetPos, . Instead, it seeked to , <*>, ., ",warn,<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>
"Exception while seek to , targetPos,  from , <*>,  of , <*>,  from , <*>, ",debug,<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>
"trg, srcs, timestamp",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logConcat(java.lang.String,java.lang.String[],long,boolean)>"
"trg, srcs, timestamp",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logConcat(java.lang.String,java.lang.String[],long,boolean)>"
"Sending DataTransferOp , <*>, : , proto, ",trace,"<org.apache.hadoop.hdfs.protocol.datatransfer.Sender: void send(java.io.DataOutputStream,org.apache.hadoop.hdfs.protocol.datatransfer.Op,com.google.protobuf.Message)>"
Tried to read from deleted or moved edit log segment,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
Tried to read from deleted edit log segment,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"Failed to initialize storage directory , locationString, . Exception details: , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>
"Failed to initialize storage directory , locationString, . Exception details: , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>
"Validating log segment , <*>,  about to be , finalized, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long)>"
"Restoring trash failed for storage directory , sd, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void restoreTrash()>
"Failed to choose remote rack (location = ~, <*>, ), fallback to local rack, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: void chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Periodic Directory Tree Verification scan is disabled because , reason#__, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDirectoryScanner(org.apache.hadoop.conf.Configuration)>
"Using a threshold of , <*>, ",info,<org.apache.hadoop.hdfs.server.balancer.Balancer$Cli: org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters parse(java.lang.String[])>
"Failed to choose from the next rack (location = , <*>, ), retry choosing ramdomly, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseFromNextRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Error response from HTTP request=, <*>, ;ec=, ie, ;em=, exceptionMsg, ",info,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
failed to create object of this class,warn,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
"Exception from HTTP response=, <*>, ",info,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
dfs.blockreport.initialDelay is greater than dfs.blockreport.intervalMsec. Setting initial delay to  msec:,info,<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configuration)>
"BLOCK* NameSystem.getDatanode: , <*>, ",fatal,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor getDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
,setStartTxId,<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto)>
,setEndTxId,<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto)>
,setIsInProgress,<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder: org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto)>
"Synchronizing log , <*>,  from , url, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: java.io.File syncLog(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
"Unable to delete tmp file , <*>, ",warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void close()>
"Failed to delete temporary file , <*>, ",warn,<org.apache.hadoop.hdfs.qjournal.server.Journal$1: java.lang.Void run()>
"Periodic Directory Tree Verification scan starting at , firstScanTime,  with interval , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void start()>
"Invalid file name. Skipping , fName, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
"Deleting , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
"Failed to delete image file: , $u, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
Edit log tailer thread exited with an exception,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void stop()>
"Stop Decommissioning , node, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Closing log when already closed,debug,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
Error closing journalSet,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
Error closing journalSet,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
over-utilized,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
above-average,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
below-average,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
underutilized,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
"src, dst, timestamp, options",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRename(java.lang.String,java.lang.String,long,boolean,org.apache.hadoop.fs.Options$Rename[])>"
"src, dst, timestamp, options",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRename(java.lang.String,java.lang.String,long,boolean,org.apache.hadoop.fs.Options$Rename[])>"
"Setting up storage: nsid=, <*>, ;bpid=, <*>, ;lv=, <*>, ;nsInfo=, nsInfo, ;dnuuid=, <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initStorage(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
"Rolling back storage directory , <*>, .\n   target LV = , <*>, ; target CTime = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Rollback of , <*>,  is complete, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
This is a rare failure scenario!!!,error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
"Image checkpoint time , <*>,  > edits checkpoint time , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
"Although short-circuit local reads are configured, they are disabled because you didn\'t configure dfs.domain.socket.path",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.net.DomainPeerServer getDomainPeerServer(org.apache.hadoop.conf.Configuration,int)>"
"Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning , <*>,  hdfsTimeout , <*>, ",info,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Could not complete , <*>,  retrying..., ",info,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Exception while stopping httpserver,error,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopHttpServer()>
"Failed to analyze storage directories for block pool , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.util.List loadBpStorageDirectories(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Invalid dfs.datanode.data.dir , <*>,  : , ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List checkStorageLocations(java.util.Collection,org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker)>"
Marking all datandoes as stale,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void markAllDatanodesStale()>
"Finalize upgrade for , <*>,  failed., ",error,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>
"Finalize upgrade for , <*>,  is complete., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>
"DIR* FSNamesystem.concat to , target, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void unprotectedConcat(java.lang.String,java.lang.String[],long)>"
"Unrecognized section , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void loadInternal(java.io.RandomAccessFile,java.io.FileInputStream)>"
Registered FSNamesystemState MBean,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void registerMBean()>
"computePacketChunkSize: src=, <*>, , chunkSize=, chunkSize, , chunksPerPacket=, <*>, , packetSize=, <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void computePacketChunkSize(int,int)>"
"End checkpoint for , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>"
"bpid,  has some block files, cannot delete unless forced, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void deleteBlockPool(java.lang.String,boolean)>"
"src, dst, timestamp",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRename(java.lang.String,java.lang.String,long,boolean)>"
"src, dst, timestamp",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRename(java.lang.String,java.lang.String,long,boolean)>"
"Backup node , bnReg,  re-registers, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
"Registering new backup node: , bnReg, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
"Removing failed volume , fsv, : , ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: java.util.List checkDirs()>
"Completed checkDirs. Removed , <*>,  volumes. Current volumes: , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: java.util.List checkDirs()>
FSDirectory.addChildNoQuotaCheck - unexpected,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean addLastINodeNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode)>"
Loading inode references,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>
"Loaded , counter_,  inode references, ",info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>
"NameNode started a new log segment at txid , txid, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
"NN started new log segment at txid , txid, , but BN had only written up to txid , <*>, in the log segment starting at , <*>, . Aborting this , log segment., ",warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
Stopped applying edits to prepare for checkpoint.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
"Setting fs.defaultFS to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>"
"lastTxnId: , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
Edits tailer failed to find any streams. Will try again later.,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
"edit streams to load from: , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
genstamp,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logGenerationStampV1(long)>
"redirectURI=, <*>, ",trace,"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: java.net.URI redirectURI(org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>"
"write to , <*>, : , <*>, , block=, <*>, ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"set bytesPerCRC=, bytesPerCRC_, , crcPerBlock=, crcPerBlock_, ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"got reply from , <*>, : md=, $u, ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"Got access token error in response to OP_BLOCK_CHECKSUM for file , src,  for block , <*>,  from datanode , <*>, . Will retry the block once., ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"src=, src, , datanodes, j_, =, <*>, ",warn,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"Finalizing upgrade for local dirs. , <*>_, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void finalizeUpgrade(boolean)>
"DIR* NameSystem.renameTo: , srcArg,  to , dstArg, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean renameToInt(java.lang.String,java.lang.String,boolean)>"
"Writing txid , firstTxnId, -, <*>, ",trace,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>"
"Sync of transaction range , firstTxnId, -, <*>,  took , <*>, ms, ",warn,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>"
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto)>
"src, : masked=, absPermission_, ",debug,"<org.apache.hadoop.hdfs.DFSClient: boolean primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>"
directive,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAddCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)>"
directive,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAddCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)>"
UNINIT,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$State: void <clinit>()>
OPEN,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$State: void <clinit>()>
CLOSED,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$State: void <clinit>()>
Uncaching of {} completed. usedBytes = {},debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>
Deferred uncaching of {} completed. usedBytes = {},debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>
"Cannot find BPOfferService for reporting block receiving for bpid=, <*>, ",error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivingBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
"The file , <*>,  is not under construction but has lease., ",warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>
"Number of blocks under construction: , numUCBlocks_, ",info,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>
<*> <*>,addWriteEndToEndLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteRpcLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
"Took , <*>, ms to send a batch of , <*>,  edits (, <*>,  bytes) to , remote journal , <*>, ",warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
"Remote journal , <*>,  failed to , write txns , <*>, -, <*>, . Will try to write to this JN again after the next , log roll., ",warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteEndToEndLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteRpcLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
"Took , <*>, ms to send a batch of , <*>,  edits (, <*>,  bytes) to , remote journal , <*>, ",warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
"Loaded image for txid , <*>,  from , curFile, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImage(java.io.File,org.apache.hadoop.io.MD5Hash,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean)>"
"Upgrade process renamed reserved path , oldPath,  to , path_, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: java.lang.String renameReservedPathsOnUpgrade(java.lang.String,int)>"
"Started plug-in , p, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>
"ServicePlugin , p,  could not be started, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>
"No block pool scanner found for block pool id: , poolId, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.Block)>"
"DirectoryScanner: shutdown has been called, but periodic scanner not started",warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
DirectoryScanner: shutdown has been called,warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
interrupted while waiting for masterThread to terminate,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
interrupted while waiting for reportCompileThreadPool to terminate,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
"Locking is disabled for , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>
"Cannot lock storage , <*>, . The directory is already locked, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>
"addCachePool of , info,  failed: , ",info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
addCachePool of {} successful.,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
ERROR in FSDirectory.verifyINodeName,error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxComponentLength(byte[],java.lang.Object,int)>"
,logMkDir,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsRecursively(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,long)>"
"mkdirs: created directory , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsRecursively(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,long)>"
"Cancelling , <*>, ",info,"<org.apache.hadoop.hdfs.DFSClient$Renewer: void cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)>"
"Unresolved topology mapping. Using /default-rack for host , <*>, ",error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocationWithFallBackToDefaultLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
"Log file , file,  has no valid header, ",warn,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(java.io.File)>
"nodes are empty for write pipeline of block , <*>, ",info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
"pipeline = , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
Exception in createBlockOutputStream,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to , <*>,  : , <*>, ",info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
"Waiting for the datanode to be restarted: , <*>, ",info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
Interrupted while waiting for CacheReplicationMonitor rescan,warn,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void waitForRescanIfNeeded()>
"Request for token received with no authentication from , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Exception while cancelling token. Re-throwing. ,info,"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"this, : createNewShm: created , <*>, ",trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
"this, : datanode does not support short-circuit , shared memory access: , <*>_, ",info,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
"this, : error requesting short-circuit shared memory , access: , <*>_, ",warn,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
"Unable to perform a zero-copy read from offset , curPos,  of , <*>, ; , length_,  bytes left in block.  , blockPos=, blockPos, ; curPos=, curPos, ; curEnd=, curEnd, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"Reducing read length from , maxLength,  to , length_,  to avoid going more than one byte , past the end of the block.  blockPos=, blockPos, ; curPos=, curPos, ; curEnd=, curEnd, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"Unable to perform a zero-copy read from offset , curPos,  of , <*>, ; -bit MappedByteBuffer limit , exceeded.  blockPos=, blockPos, , curEnd=, curEnd, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"Reducing read length from , maxLength,  to , length_,  to avoid -bit limit.  , blockPos=, blockPos, ; curPos=, curPos, ; curEnd=, curEnd, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"unable to perform a zero-copy read from offset , curPos,  of , <*>, ; BlockReader#getClientMmap returned , null., ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"readZeroCopy read , length_,  bytes from offset , curPos,  via the zero-copy read , path.  blockEnd = , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"LazyWriter failed to async persist RamDisk block pool id: , <*>, block Id: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask: void run()>
"Error while processing URI: , name, ",error,<org.apache.hadoop.hdfs.server.common.Util: java.util.List stringCollectionAsURIs(java.util.Collection)>
"Copied , srcMeta,  to , <*>,  and calculated checksum, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>"
"Copied , srcFile,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>"
"Deleted a metadata file without a block , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Removed block , blockId,  from memory with missing block file on the disk, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Deleted a metadata file for the deleted block , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Added missing block to memory , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Failed to delete , diskFile, . Will retry on next scan, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Block file in volumeMap , <*>,  does not exist. Updating it to the file found during scan , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Updating generation stamp for block , blockId,  from , <*>,  to , <*>_, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Metadata file in memory , <*>,  does not match file found by scan , <*>_, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Updating generation stamp for block , blockId,  from , <*>,  to , <*>_, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Updating size of block , blockId,  from , <*>,  to , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Reporting the block , corruptBlock_,  as corrupt due to length mismatch, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Failed to repot bad block , corruptBlock_, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Checkpointer got exception,warn,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.BackupNode)>"
"SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
Error while reading edits from disk. Will try again.,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
Unknown error encountered while tailing edits. Shutting down standby NN.,fatal,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
Edit log tailer interrupted,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
"BLOCK* BlockInfoUnderConstruction.initLeaseRecovery: No blocks found, lease removed.",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>
"BLOCK* , this,  recovery started, primary=, primary_, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>
"Loading edits into backupnode to try to catch up from txid , <*>,  to , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Logs rolled while catching up to current segment,debug,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
"Unable to find stream starting with , <*>, . This indicates that there is an error in synchronization in BackupImage, ",warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
"Going to finish converging with remaining , editStreamsAll,  txns from in-progress stream , stream_, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
", <*> <*>",closeAllStreams,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
", <*> <*>",closeAllStreams,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
"Successfully synced BackupNode with NameNode at txnid , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Data from remote NameNode,<init>,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void <init>(org.apache.hadoop.conf.Configuration)>
"src,  not found in lease.paths (=, <*>, ), ",debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>"
"lease,  not found in sortedLeases, ",error,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>"
pool,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAddCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)>"
pool,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAddCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)>"
"BLOCK* processOverReplicatedBlock: Postponing , block,  since storage , storage,  does not yet have up-to-date information., ",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlock(org.apache.hadoop.hdfs.protocol.Block,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Adding new storage ID , <*>,  for DN , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo updateStorage(org.apache.hadoop.hdfs.server.protocol.DatanodeStorage)>
"Invalid BlockPoolId , <*>,  in HeartbeatResponse. Expected , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void handleRollingUpgradeStatus(org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse)>
key,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
"Failed to delete temporary edits file: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void deleteTempEdits()>
"Loaded FSImage in , <*>,  seconds., ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void load(java.io.File)>
STATE* Safe mode is already OFF,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void leaveSafeMode()>
"Starting upgrade of edits directory: .\n   old LV = , <*>, ; old CTime = , <*>, .\n   new LV = , <*>, ; new CTime = , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void doUpgrade(org.apache.hadoop.hdfs.server.common.StorageInfo)>
Failed to get snapshottable directories. Ignore and continue.,warn,<org.apache.hadoop.hdfs.server.mover.Mover$Processor: void getSnapshottableDirs()>
failed to load misc.Unsafe,error,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: sun.misc.Unsafe safetyDance()>
"<*>,  method=, <*>,  op=, <*>,  target=, path_, ",info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler: org.jboss.netty.channel.ChannelFuture handleOperation(org.jboss.netty.channel.MessageEvent)>
"Caching file names occuring more than , <*>,  times, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
poolName,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRemoveCachePool(java.lang.String,boolean)>"
poolName,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRemoveCachePool(java.lang.String,boolean)>"
"Could not delete , file, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void deleteOrWarn(java.io.File)>
Client using encryption algorithm {},debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey)>"
"No shared edits directory configured for namespace , <*>,  namenode , <*>, ",fatal,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
", , ",formatNonFileJournals,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not initialize shared edits dir,error,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
"closed , this, suffix_, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: void close()>
"Must specify a valid cluster ID after the , <*>,  flag, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Must specify a valid cluster ID after the , <*>,  flag, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Must specify a valid cluster ID after the , <*>,  flag, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Unknown upgrade flag , clusterId_, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Must specify a rolling upgrade startup option , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Invalid argument: , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
"Reconfiguring , property,  to , newVal, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void reconfigurePropertyImpl(java.lang.String,java.lang.String)>"
Caught interrupted exception ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void join()>
"Cannot list edit logs in , fjm, ",warn,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
"Found gap in logs at , j#_, : , not returning previous logs in manifest., ",debug,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
"Generated manifest for logs since , fromTxId, :, $u, ",debug,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
"Checkpoint Period : , <*>,  secs , (, <*>,  min), ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>
"Transactions count is  : , <*>, , to trigger checkpoint, ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto)>
"Starting upgrade of storage directory , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doPreUpgrade(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>"
"Block {}: can\'t add new cached replicas, because there is no record of this block on the NameNode.",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"Block {}: can\'t cache this block, because it is not yet complete.",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"Block {}: DataNode {} is not a valid possibility because the block has size {}, but the DataNode only has {}bytes of cache remaining ({} pending bytes, {} already cached.",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
Block {}: added to PENDING_CACHED on DataNode {},trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
Block {}: we only have {} of {} cached replicas. {} DataNodes have insufficient cache capacity.,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
,mergeSignature,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto)>
"Reporting bad , block, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Cannot report bad , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Downloaded file , <*>,  size , <*>,  bytes., ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash downloadImageToStorage(java.net.URL,long,org.apache.hadoop.hdfs.server.common.Storage,boolean)>"
"blockId=, blockId, , f=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File validateBlockFile(java.lang.String,long)>"
"Refreshing all user-to-groups mappings. Requested by user: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshUserToGroupsMappings()>
"Total time to add all replicas to map: , arr$#, ms, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>"
"Connecting to datanode , <*>,  addr=, <*>, ",debug,"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: void <init>(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>"
"Removing , <*>,  from FsDataset., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void removeVolumes(java.util.Collection)>
"path, snapOldName, snapNewName",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean)>"
"path, snapOldName, snapNewName",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean)>"
txId layoutVersion,waitForWriteQuorum,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>"
"BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block , b,  no longer found , in block map., ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block , b, , result is , <*>, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
"DFSClient writeChunk allocating new packet seqno=, <*>, , src=, <*>, , packetSize=, <*>, , chunksPerPacket=, <*>, , bytesCurBlock=, <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>"
"DFSClient writeChunk packet full seqno=, <*>, , src=, <*>, , bytesCurBlock=, <*>, , blockSize=, <*>, , appendChunk=, <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>"
"SASL client doing encrypted handshake for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client doing general handshake for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"*BLOCK* NameNode.addBlock: file , src,  fileId=, fileId,  for , clientName, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[])>"
,mergeJid,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto)>
,setSinceTxId,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto)>
,setInProgressOk,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto)>
"BUG: Inconsistent diskspace for directory , <*>, . Cached = , <*>,  != Computed = , computed, ",error,"<org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature: void checkDiskspace(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,long)>"
createNewMemorySegment: ShortCircuitRegistry is not enabled.,trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>"
"createNewMemorySegment: created , <*>, ",trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>"
,logStartRollingUpgrade,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo startRollingUpgrade()>
"writeTo blockfile is , <*>,  of size , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>"
"writeTo metafile is , <*>,  of size , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>"
"Start moving , this, ",debug,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
"Successfully moved , this, ",info,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
"Failed to move , this, : , <*>, ",warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
"Block deletion is delayed during NameNode startup. The deletion will start after , <*>,  ms., ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: java.util.List invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Failed to preserve last modified date from\', srcFile, \' to \', destFile, \', ",debug,"<org.apache.hadoop.hdfs.server.common.Storage: void nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean)>"
"BLOCK* checkFileProgress: , block,  has not reached minimal replication , $i, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
"BLOCK* checkFileProgress: , <*>,  has not reached minimal replication , $i, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
Exception while getting block list,warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: void dispatchBlocks()>
"this, : found waitable for , key, ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
"this, : interrupted while waiting for , key, ",info,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
"this, : could not get , key,  due to InvalidToken , exception., ",warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
"this, : failed to get , key, ",warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
"this, : got stale replica , <*>, .  Removing , this replica from the replicaInfoMap and retrying., ",info,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
ListPathServlet encountered InterruptedException,warn,"<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Cache report from datanode {} has block {},trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Added block {}  to cachedBlocks,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Added block {} to CACHED list.,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Removed block {} from PENDING_CACHED list.,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
"Allowing manual HA control from , <*>,  even though automatic HA is enabled, because the user , specified the force flag, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)>
"processReport x, <*>, : removing zombie storage , <*>, , which no longer exists on the DataNode., ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>"
"processReport x, <*>, : removed , <*>,  replicas from storage , <*>, , which no longer exists on the DataNode., ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>"
"Failed to delete old dfsUsed file in , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>
"Failed to write dfsUsed to , $u, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>
AsyncLazyPersistService has already shut down.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
Shutting down all async lazy persist service threads,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
All async lazy persist service threads have been shut down,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
"Block , b,  unfinalized and removed. , ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void unfinalizeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"Loading , <*>,  INodes., ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: void loadINodeSection(java.io.InputStream)>
"Failed to renew lease for , <*>,  for , <*>,  seconds (>= hard-limit =, L,  seconds.) , Closing all files being written ..., ",warn,<org.apache.hadoop.hdfs.DFSClient: boolean renewLease()>
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
"BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src=, src_,  lastBlock=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
"BLOCK* allocateBlock: caught retry for allocation of a new block in , src_, . Returning previously allocated block , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
error in renew over HTTP,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>"
"rethrowing exception from HTTP request: , <*>, ",info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>"
Refreshing SuperUser proxy group mapping list ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshSuperUserGroupsConfiguration()>
"Lock on , <*>,  acquired by nodename , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
"It appears that another namenode, <*>_,  has already locked the storage directory, ",error,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
"Failed to acquire lock on , <*>, . If this storage directory is mounted via NFS, , ensure that the appropriate nfs lock services are running., ",error,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
"Performing upgrade of storage directory , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>"
"Unable to rename temp to previous for , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>"
encountered exception ,warn,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy$LocalDatanodeInfo: org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol getDatanodeProxy(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,boolean)>"
"Number of failed storage changes from , <*>,  to , volFailures, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateHeartbeatState(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int)>"
"Received an invalid request file transfer request from , <*>, : , <*>, ",warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"Replaced expired token: , <*>, ",debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: boolean replaceExpiredDelegationToken()>
"Sending receipt verification byte for slot , slot, ",trace,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"this, : error creating ShortCircuitReplica., ",warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"short-circuit read access is disabled for DataNode , <*>, .  reason: , <*>, ",warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"short-circuit read access for the file , <*>,  is disabled for DataNode , <*>, .  reason: , <*>, ",warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"this, :, <*>, ",debug,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"this, : unknown response code , <*>,  while attempting to set up short-circuit access. , <*>, ",warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
"Connecting to datanode , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair connectToDN(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.protocol.LocatedBlock)>"
"Unexpectedly low genstamp on , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
"Unexpectedly short length on , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
"Unexpectedly short length on , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
"Discarding , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
"automatically choosing , firstChoice, ",info,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>"
"I\'m sorry, I cannot understand your response.\n",error,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>"
"this, : freeing empty stale , shm, ",trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
"this, : shutting down UNIX domain socket for , empty , shm, ",trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
"End checkpoint at txid , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void endCheckpoint(org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>
error closing TcpPeerServer: ,error,<org.apache.hadoop.hdfs.net.TcpPeerServer: void close()>
"Deleting , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>
"Failed to delete , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>
"DIR* NameSystem.append: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"Starting upgrade of edits directory , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>
"Failed to move aside pre-upgrade storage in image directory , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>
"Slow PacketResponder send ack to upstream took , <*>, ms (threshold=, <*>, ms), , <*>, , replyAck=, $u, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
"<*>, , replyAck=, $u, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
"Stopped plug-in , p, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
"ServicePlugin , p,  could not be stopped, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Exception shutting down DataNode,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
"Waiting for threadgroup to exit, active threads is , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Received exception in BlockPoolManager#shutDownAll: ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
"Exception when unlocking storage: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Shutdown complete.,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
"Cannot find BPOfferService for reporting block deleted for bpid=, <*>, ",error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeDeletedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
"File descriptor passing is disabled because , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
File descriptor passing is enabled.,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"Configured hostname is , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"this, : trying to construct a BlockReaderLocal , for short-circuit reads., ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
"this, : , <*>,  is not , usable for short circuit; giving up on BlockReaderLocal., ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
"this, : got InvalidToken exception while trying to , construct BlockReaderLocal via , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
"this, : failed to get , ShortCircuitReplica. Cannot construct , BlockReaderLocal via , <*>, ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
"removeCachePool of , poolName,  failed: , ",info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>
"removeCachePool of , poolName,  successful., ",info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>
Reading empty packet at end of read,trace,<org.apache.hadoop.hdfs.RemoteBlockReader2: void readTrailingEmptyPacket()>
"Storage directory , sd,  contains no VERSION file. Skipping..., ",warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector readAndInspectDirs(java.util.EnumSet,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"truncateBlock: blockFile=, blockFile, , metaFile=, metaFile, , oldlen=, oldlen, , newlen=, newlen, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void truncateBlock(java.io.File,java.io.File,long,long)>"
"Existing client context \', <*>, \' does not match , requested configuration.  Existing: , <*>, , Requested: , <*>, ",warn,<org.apache.hadoop.hdfs.ClientContext: void printConfWarningIfNeeded(org.apache.hadoop.hdfs.DFSClient$Conf)>
"opReadBlock , block,  received exception , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"Client , <*>,  did not send a valid status code after reading. , Will close connection., ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
Error reading client status response. Will close connection.,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"<*>, :Ignoring exception while serving , block,  to , <*>, ",trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"<*>, :Got exception while serving , block,  to , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
BUG: unexpected exception ,error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void updateCountNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,int,long,long)>"
"ServicePlugin , p,  could not be started, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
"<*>,  RPC up at: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
"<*>,  service RPC up at: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
"<*>_, uccessfully sent block report x, <*>, ,  containing , <*>,  storage report(s), of which we sent , i$_, .,  The reports had , totalBlockCount_,  total blocks and used , kvPair#_,  RPC(s). This took , <*>,  msec to generate and , <*>,  msecs for RPC and NN processing.,  Got back , <*>_, ., ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>
"<*>_, uccessfully sent block report x, <*>, ,  containing , <*>,  storage report(s), of which we sent , i$_, .,  The reports had , totalBlockCount_,  total blocks and used , kvPair#_,  RPC(s). This took , brCreateCost,  msec to generate and , brSendCost,  msecs for RPC and NN processing.,  Got back , <*>_, ., ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>
"State transition , <*>,  -> , newState, ",debug,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState)>
"BLOCK NameSystem.addToCorruptReplicasMap: , <*>,  added as corrupt on , dn,  by , <*>, reasonText_, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>"
"BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for , <*>,  to add as corrupt , on , dn,  by , <*>, reasonText_, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>"
"Error report from , <*>_, : , msg, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
"Disk error on , <*>_, : , msg, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
"Fatal disk error on , <*>_, : , msg, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
"Error report from , <*>_, : , msg, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
"DIR* NameSystem.mkdirs: , srcArg, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)>"
"this,  received versionRequest response: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
"Problem connecting to server: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
"Problem connecting to server: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
"BLOCK* processReport: discarded non-initial block report from , nodeID,  because namenode still in startup phase, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"BLOCK* processReport: Received first block report from , storage,  after starting up or becoming active. Its block , contents are no longer considered stale, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"processReport x, <*>, : no zombie storages found., ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"processReport x, <*>, : , <*>,  more RPCs remaining in this report., ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"BLOCK* processReport: , staleBefore,  on , <*>,  size , <*>,  does not belong to any file, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"BLOCK* processReport: from storage , <*>,  node , nodeID, , blocks: , <*>, , hasStaleStorages: , <*>, , processing time: , <*>,  msecs, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"Failed to , <*>_,  inode , <*>, ",error,<org.apache.hadoop.hdfs.DFSClient: void closeAllFilesBeingWritten(boolean)>
"Unexpected exception occurred while checking disk error  , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>
InterruptedException in check disk error thread,debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>
"initReplicaRecovery: , block, , recoveryId=, recoveryId, , replica=, <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
"initReplicaRecovery: update recovery id for , block,  from , <*>,  to , recoveryId, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
"initReplicaRecovery: changing replica state for , block,  from , <*>,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
,logAllocateBlockId,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: long nextBlockId()>
"Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"Cancelling caching for block with id {}, pool {}.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"{} is anchored, and can\'t be uncached now.  Scheduling it for uncaching in {} ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
{} has been scheduled for immediate uncaching.,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"Block with id {}, pool {} does not need to be uncached, because it is in state {}.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"defaultReplication         = , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"maxReplication             = , $i, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"minReplication             = , $i, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"maxReplicationStreams      = , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"replicationRecheckInterval = , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"encryptDataTransfer        = , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
"maxNumBlocksToLog          = , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void stopMonitor()>
"this, : added no-checksum anchor to slot , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>
"this, : could not add no-checksum anchor to slot , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto)>
"Opening connection to , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash getFileClient(java.net.URL,java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean)>"
"src, username, groupname",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSetOwner(java.lang.String,java.lang.String,java.lang.String)>"
"<*>, :Exception writing , <*>,  to mirror , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void handleMirrorOutError(java.io.IOException)>
"Skipped stale nodes for recovery : , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[] handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int)>"
,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void purge(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
"<*>, :DataXceiverServer: , ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
"<*>, :DataXceiverServer: , ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
DataNode is out of memory. Will retry in  seconds.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
"<*>, :DataXceiverServer: Exiting due to: , ",error,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
"<*>,  :DataXceiverServer: close exception, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
Shutting down DataXceiverServer before restart,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
"this, : failed to munmap, ",warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>
"this, : freed, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>
"<*>,  , name, : , items, ",info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollection(java.lang.String,java.util.Collection)>"
,checkState,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: java.util.Collection selectInputStreams(long,long,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean)>"
,mergeReqInfo,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto)>
,setStartTxId,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto)>
,setEndTxId,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto)>
"nextValidOp: got exception while reading , this, ",error,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextValidOp()>
"Scanning storage , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
"Latest log is , latestLog, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
"Latest log , latestLog,  has no transactions. , moving it aside and looking for previous log, ",warn,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
"No files in , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
"closing file , <*>, , but there are still , unreleased ByteBuffers allocated by read().  , Please release , <*>, ., ",warn,<org.apache.hadoop.hdfs.DFSInputStream: void close()>
"this, : allocAndRegisterSlot , <*>, : allocatedSlots=, <*>, <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)>
"DIR* FSDirectory.delete: , src, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long delete(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>"
NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.inotify.EventBatchList getEditsFromTxid(long)>
"Logic error: we\'re trying to uncache more replicas than actually exist for , cachedBlock, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingUncached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
"DIR* FSDirectory.unprotectedRenameTo: , rename source cannot be the root, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
"DIR* addFile: failed to add , path, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>"
"DIR* addFile: , path,  is added, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>"
"Preallocated , total_,  bytes at the end of , the edit log (offset , <*>, ), ",debug,<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void preallocate()>
"passing over , elf,  because it is in progress , and we are ignoring in-progress logs., ",debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
"got IOException while trying to validate header of , elf, .  Skipping., ",error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
"passing over , elf,  because it ends at , <*>, , but we only care about transactions , as new as , fromTxId, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
,<init>,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
"selecting edit log stream , elf, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
"ACLs enabled? , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
"XAttrs enabled? , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
"Maximum size of an xattr: , <*>, <*>_, ",info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
"Removed , bpos, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>
"Couldn\'t remove BPOS , t,  from bpByNameserviceId map, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>
,visitOp,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
"Got IOException at position , <*>, ",error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Got IOException while reading stream!  Resyncing.,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
"Got RuntimeException at position , <*>, ",error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Got RuntimeException while reading stream!  Resyncing.,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Encountered exception loading fsimage,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>
"Finished loading FSImage in , ioe,  msecs, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>
,mergeReqInfo,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto)>
,setMinTxIdToKeep,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto)>
path,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAllowSnapshot(java.lang.String)>
" Could not read or failed to veirfy checksum for data at offset , <*>,  for block , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void readChecksum(byte[],int,int)>"
Loading inode directory section,info,"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>"
"Loaded , counter_,  directories, ",info,"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>"
PendingReplicationMonitor checking Q,debug,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
"PendingReplicationMonitor timed out , block, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
"Appending to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline append(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
"Starting to scan blockpool: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
"All remaining blocks were processed recently, so this run is complete",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
"Done scanning block pool: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
RuntimeException during BlockPoolScanner.scan(),warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
"Done scanning block pool: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
"Recovering unfinalized segments in , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
"Deleting zero-length edit log file , elf, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
"Moving aside edit log file that seems to have zero transactions , elf, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
,finalizeLogSegment,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
"remove datanode , nodeInfo, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"dfs.client.use.legacy.blockreader.local = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
"dfs.client.read.shortcircuit = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
"dfs.client.domain.socket.data.traffic = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
"dfs.domain.socket.path = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
The storage directory is in an inconsistent state,warn,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Failed to move aside pre-upgrade storage in image directory , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Adding replicas to map for block pool , <*>,  on volume , <*>, ..., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
"Time to add replicas to map for block pool , <*>,  on volume , <*>, : , timeTaken, ms, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
"Caught exception while adding replicas from , <*>, . Will throw later., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFileInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],boolean)>"
"No block pool is up, going to wait",warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>
"Received exception: , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>
"Access token was invalid when connecting to , targetAddr,  : , ex, ",info,"<org.apache.hadoop.hdfs.DFSInputStream: boolean tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)>"
Stopping services started for standby state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopStandbyServices()>
"Processing , <*>,  messages from DataNodes , that were previously queued during standby state, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAllPendingDNMessages()>
"modifyCachePool of , info,  failed: , ",info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
modifyCachePool of {} successful; {},info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
,setTxid,<org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto)>
trying to get DT with no secret manager running,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
Starting CheckDiskError Thread,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDiskErrorAsync()>
"Couldn\'t create proxy provider , failoverProxyProviderClass_, ",debug,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean)>"
Validating directive {} pool maxRelativeExpiryTime {},trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: long validateExpiryTime(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,long)>"
"metadata returned: , <*>, ",trace,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.BlockStorageLocation[] getBlockStorageLocations(java.util.List)>
sync_file_range error,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2: void run()>
"Fsck: can\'t copy the remains of , <*>,  to , lost+found, because , <*>,  already exists., ",warn,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
"Fsck: could not copy block , <*>,  to , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
"Fsck: there were errors copying the remains of the corrupted file , <*>,  to /lost+found, ",warn,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
"Fsck: copied the remains of the corrupted file , <*>,  to /lost+found, ",info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
"copyBlocksToLostFound: error processing , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
Starting services required for active state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Catching up to latest edits from old active before taking over writer role in edits logs,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Reprocessing replication and invalidation queues,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
"NameNode metadata after re-processing replication and invalidation queues during failover:\n, <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
"Will take over writing edit logs at txnid , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
"<*>,  got , $u, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
"Relaying an out of band ack of type , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
"Calculated invalid ack time: , oobStatus, ns., ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
The downstream error might be due to congestion in upstream including this node. Propagating the error: ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
"<*>, : Thread is interrupted., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
IOException in BlockReceiver.run(): ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
"<*>,  terminating, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
"Renewing , <*>, ",info,<org.apache.hadoop.hdfs.DFSClient: long renewDelegationToken(org.apache.hadoop.security.token.Token)>
"Recover failed append to , b, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverAppend(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
"rollingUpgrade , action, ",info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)>
"this, : selecting input streams starting at , fromTxId, <*>_, from among , <*>,  candidate file(s), ",debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>"
Interrupted while processing replication queues.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>
Error while processing replication queues async,error,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>
"Failed to read expected encryption handshake from client at , <*>, . Perhaps the client , is running an older version of Hadoop which does not support , encryption, ",info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :Number of active connections is: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"Cached , <*>,  closing after , opsProcessed_,  ops, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :Number of active connections is: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :DataXceiver error processing , <*>_,  operation ,  src: , <*>,  dst: , <*>, ",trace,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, ; , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :DataXceiver error processing , <*>_,  operation ,  src: , <*>,  dst: , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :Number of active connections is: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"<*>, :Number of active connections is: , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
"Error while resolving the link : , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean isInSnapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>
Rolling edit logs,info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: long rollEditLog()>
Exception while adding a block,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
"Waiting for replication for , <*>,  seconds, ",info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
"NotReplicatedYetException sleeping , <*>,  retries left , retries_, ",warn,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
Caught exception ,warn,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
"detachFile failed to delete temporary file , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: void unlinkFile(java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
Error in setting outputbuffer capacity,error,<org.apache.hadoop.hdfs.server.namenode.JournalSet: void setOutputBufferCapacity(int)>
"this, : returning new legacy block reader local., ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
"this, : returning new block reader local., ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
"this, : returning new remote block reader using , UNIX domain socket on , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
"path, <*>_, <*>",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAddBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
"Failed to create trash directory , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
"Moving files , <*>,  and , <*>,  to trash., ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
"Unable to start log segment , txid,  at , <*>, : , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>"
"Set non-null progress callback on DFSOutputStream , src, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void <init>(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.util.Progressable,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.util.DataChecksum)>"
"<*>, : , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
"<*>, : , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
"<*>, : , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
"Retrying connect to namenode: , <*>, . Already tried , retry,  time(s); retry policy is , <*>, , delay , <*>, ms., ",info,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>"
Original exception is ,warn,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>"
"addFinalizedBlock: Moved , <*>,  to , <*>,  and , srcfile,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File moveBlockFiles(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
"shutdownDatanode command received (upgrade=, forUpgrade, ). Shutting down Datanode..., ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdownDatanode(boolean)>
"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.",error,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>
"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.keytab\' is not set.",error,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>
"Removing block pool , bpid, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdownBlockPool(java.lang.String)>
"DIR* NameSystem.appendFile: src=, srcArg, , holder=, holder, , clientMachine=, clientMachine, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>"
"DIR* NameSystem.appendFile: file , <*>,  for , holder,  at , clientMachine,  block , <*>,  block size , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>"
"BLOCK* invalidateBlock: , b,  on , dn, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"BLOCK* invalidateBlocks: postponing invalidation of , b,  on , dn,  because , <*>,  replica(s) are located on nodes , with potentially out-of-date block reports, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"BLOCK* invalidateBlocks: , b,  on , dn,  listed for deletion., ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"BLOCK* invalidateBlocks: , b,  on , dn,  is the only copy and was not deleted, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
Cannot use /lost+found : a regular file with this name exists.,warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>
Cannot initialize /lost+found .,warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>
New namespace image has been created,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void saveNamespace()>
"DIR* NameSystem.createSymlink: target=, target,  link=, linkArg, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void createSymlinkInt(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
BUG: removeSnapshot increases namespace usage.,error,"<org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature: org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot removeSnapshot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
"No KEY found for persisted identifier , <*>, ",warn,"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
Same delegation token being added twice; invalid entry in fsimage or editlogs,<init>,"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
"Log file , file,  has no valid header, ",warn,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(java.io.File)>
"Caught exception after scanning through , numValid_,  ops from , $u,  while determining its valid length. Position was , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(java.io.File)>
"After resync, position is , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(java.io.File)>
,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(java.io.File)>
"NameNode is being shutdown, exit SafeModeMonitor thread",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeMonitor: void run()>
"Could not find a target for file , src,  with favored node , favoredNode, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>"
"Failed to choose with favored nodes (=, favoredNodes, ), disregard favored nodes hint and retry., ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>"
"Unable to rename checkpoint in , sd, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(long,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,boolean)>"
"Configuration key , key,  is deprecated! Ignoring...,  Instead please specify a value for , dfs.namenode.checkpoint.txns, ",warn,<org.apache.hadoop.hdfs.server.namenode.CheckpointConf: void warnForDeprecatedConfigs(org.apache.hadoop.conf.Configuration)>
"<*>,  DN: , dnReg, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"<*>, . Note: This is normal during a rolling upgrade., ",info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"Failed to cache , <*>, : could not reserve , <*>,  more bytes in the cache: , dfs.datanode.max.locked.memory,  of , <*>,  exceeded., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Failed to cache , <*>, : Underlying blocks are not backed by files., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Failed to cache , <*>, : failed to find backing , files., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Failed to cache , <*>, : failed to open file, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Failed to cache , <*>, : checksum verification failed., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Failed to cache , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
"Caching of , <*>,  was cancelled., ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Successfully cached {}.  We are now caching {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Edits log must not be open.,<init>,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
"Starting upgrade of local storage directories.\n   old LV = , <*>, ; old CTime = , <*>, .\n   new LV = , <*>, ; new CTime = , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
"Failed to move aside pre-upgrade storage in image directory , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
req,logAddCachePool,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
"<*>, .findLease: prefix=, prefix, ",debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map findLeaseWithPrefixPath(java.lang.String,java.util.SortedMap)>"
Exception in doCheckpoint: ,error,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>
Throwable Exception in doCheckpoint: ,error,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>
<*>,visitOp,"<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader: void endElement(java.lang.String,java.lang.String,java.lang.String)>"
"Can perform rollback for , sd_, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Can perform rollback for shared edit log.,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
"Rolling back storage directory , <*>, .\n   new LV = , <*>, ; new CTime = , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
"BLOCK* fsync: , src,  for , clientName, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void fsync(java.lang.String,long,java.lang.String,long)>"
"BLOCK* removeStoredBlock: , block,  from , node, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"BLOCK* removeStoredBlock: , block,  has already been removed from node , node, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"BLOCK* removeStoredBlock: , block,  is removed from excessBlocks, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Unresolved dependency mapping for host , <*>, . Continuing with an empty dependency list, ",error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependenciesWithDefault(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"block,  is already in the recovery queue, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void addBlockToBeRecovered(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>
Continuing,info,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void editLogLoaderPrompt(java.lang.String,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,java.lang.String)>"
"HTTP , <*>, : , op, , , path, , ugi=, ugi, <*>, ",trace,"<org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>"
Starting recovery process for unclosed journal segments...,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
"Successfully started new epoch , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
"newEpoch(, <*>, ) responses:\n, <*>, ",debug,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
Acquiring write lock to replay edit log,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
,check203UpgradeFailure,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"We failed to read txId , expectedTxId_, ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"There appears to be a gap in the edit log.  We expected txid , expectedTxId_, , but got txid , <*>, ., ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"There appears to be an out-of-order edit in the edit log.  We expected txid , expectedTxId_, , but got txid , <*>, ., ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"op=, <*>, , startOpt=, startOpt, , numEdits=, numEdits_, , totalEdits=, <*>, ",trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Encountered exception on operation , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Failed to apply edit log operation , <*>, : error , <*>, ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"replaying edit log: , deltaTxId, /, <*>,  transactions completed. (, <*>, %), ",info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Stopped at OP_START_ROLLING_UPGRADE for rollback.,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Stopped reading edit log at , <*>, /, <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
replaying edit log finished,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
replaying edit log finished,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"path, <*>",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logUpdateBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
"path, <*>",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logUpdateBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
,debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void accept(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheVisitor)>
Exception shutting down access key updater thread,warn,<org.apache.hadoop.hdfs.server.balancer.KeyManager: void close()>
"Data dir states:\n  , <*>, ",trace,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"Storage directory , <*>,  is not formatted., ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"opWriteBlock: stage=, stage, , clientname=, clientname, \n  block  =, block, , newGs=, latestGenerationStamp, , bytesRcvd=, minBytesRcvd, , , maxBytesRcvd, , \n  targets=, <*>, ; pipelineSize=, pipelineSize, , srcDataNode=, srcDataNode, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"isDatanode=, isDatanode, , isClient=, isClient, , isTransfer=, isTransfer, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"writeBlock receive buf size , <*>,  tcp no delay , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Receiving , block,  src: , <*>,  dest: , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Connecting to datanode , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Datanode , <*>,  got response for connect ack ,  from downstream datanode with firstbadlink as , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"<*>, :Exception transfering block , block,  to mirror , <*>, : , <*>, ",error,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"<*>, :Exception transfering , block,  to mirror , <*>, - continuing without the mirror, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Datanode , <*>,  forwarding connect ack to upstream firstbadlink is , firstBadLink_, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
TRANSFER: send close-ack,trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Received , block,  src: , <*>,  dest: , <*>,  of size , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"opWriteBlock , block,  received exception , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Invalid hostname , hostStr_,  in hosts file, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.protocol.DatanodeID parseDNFromHostsEntry(java.lang.String)>
"Starting DataNode with maxLockedMemory = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"dnUserName = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"supergroup = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"<*>, .addDatanode: , node , node,  is added to datanodeMap., ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Error: , status,  failed for required journal (, jas, ), ",fatal,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
"Error: , status,  failed for (journal , jas, ), ",error,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
"Error: , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
"getDatanodeListForReport with includedNodes = , <*>, , excludedNodes = , <*>, , foundNodes = , <*>, , nodes = , <*>, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)>
"getBlockLocalPathInfo successful block=, block,  blockfile , <*>,  metafile , <*>, ",trace,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"getBlockLocalPathInfo for block=, block,  returning null, ",trace,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"LazyWriter schedule async task to persist RamDisk block pool id: , bpId,  block id: , blockId, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
"LazyWriter failed to create , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
Edit log tailer thread exited with an exception,warn,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void stop()>
Failed to read next line.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RollingLogsImpl$Reader: java.lang.String next()>
Refreshing call queue.,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshCallQueue()>
"Opened IPC server at , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initIpcServer(org.apache.hadoop.conf.Configuration)>
"Edits file , f,  has improperly formatted , transaction ID, ",error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
"In-progress edits file , f,  has improperly , formatted transaction ID, ",error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
"In-progress stale edits file , f,  has improperly , formatted transaction ID, ",error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
"Error report from , registration, : , msg, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)>"
Exception in doCheckpoint,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
"Merging failed , <*>,  times., ",fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
Throwable Exception in doCheckpoint,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
"BPOfferService , this,  interrupted while , stateString, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void sleepAndLogInterrupts(int,java.lang.String)>"
Cannot load customized ssl related configuration. Fallback to system-generic settings.,debug,<org.apache.hadoop.hdfs.web.URLConnectionFactory: org.apache.hadoop.hdfs.web.URLConnectionFactory newDefaultURLConnectionFactory(org.apache.hadoop.conf.Configuration)>
"Start Decommissioning , node,  , storage,  with , <*>,  blocks, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Checksum error in block , <*>,  from , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
"report corrupt , <*>,  from datanode , <*>,  to namenode, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
"Failed to report bad , <*>,  from datanode , <*>,  to namenode, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
"Failed to delete restart meta file: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean)>"
"src, permissions",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSetPermissions(java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>"
"Finalize upgrade for , <*>,  failed, ",error,<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>
"Finalize upgrade for , <*>,  is complete, ",info,<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>
Stopping services started for active state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopActiveServices()>
"Failed to delete , <*>, ",warn,<org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: void close()>
"lastAckedSeqno = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void addDatanode2ExistingPipeline()>
"Ignoring unknown CryptoProtocolVersion provided by client: , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[])>"
"this, : error shutting down shm: got IOException calling , shutdown(SHUT_RDWR), ",warn,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void shutdown(org.apache.hadoop.hdfs.shortcircuit.DfsClientShm)>
editLog must be initialized,<init>,<org.apache.hadoop.hdfs.server.namenode.FSImage: void openEditLogForWrite()>
"Balancing bandwith is , bandwidth,  bytes/s, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>"
"Number threads for balancing is , maxThreads, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>"
"Failed to report bad block , block,  to namenode : ,  Exception, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,org.apache.hadoop.hdfs.StorageType)>"
"Closing old block , <*>, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void endBlock()>
"Can\'t send invalid block , block, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>"
"<*>,  Starting thread to transfer , block,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>"
,mergeReqInfo,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto)>
,setTxid,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto)>
,setLayoutVersion,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto)>
"updateReplica: , oldBlock, , recoveryId=, recoveryId, , length=, newlength, , replica=, <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String updateReplicaUnderRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
"<*>,  had lastBlockReportId x, <*>, , but curBlockReportId = x, <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: java.util.List removeZombieStorages()>
"Service RPC server is binding to , bindHost_, :, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>"
"RPC server is binding to , serviceHandlerCount_, :, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>"
Clearing encryption key,debug,<org.apache.hadoop.hdfs.DFSClient: void clearDataEncryptionKey()>
"fs.defaultFS is , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>
"Clients are to use , <*>,  to access,  this namenode/service., ",info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>
"Completing previous upgrade for storage directory , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Recovering storage directory , <*>,  from previous upgrade, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Completing previous rollback for storage directory , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Recovering storage directory , <*>,  from previous rollback, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Completing previous finalize for storage directory , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Completing previous checkpoint for storage directory , <*>, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"Recovering storage directory , <*>,  from failed checkpoint, ",info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
"<*>, .changelease: ,  src=, src, , dest=, dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>"
"changeLease: replacing , oldpath,  with , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>"
,mergeJournalInfo,<org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto)>
,setTxid,<org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto)>
,setEpoch,<org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto)>
"Loading , <*>,  inodes., ",info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
Sorting inodes,debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
Finished sorting inodes,debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
Error compiling report,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: java.util.Map getDiskReport()>
"Excluding datanode , dn, : , <*>, , , <*>, , , <*>, , , notIncluded, ",trace,<org.apache.hadoop.hdfs.server.balancer.Dispatcher: boolean shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"Failed to choose from local rack (location = , <*>, ), retry with the rack of the next replica (location = , <*>, ), ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Failed to choose from local rack (location = , <*>, ); the second replica is not found, retry choosing ramdomly, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Unable to abort stream , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream: void abort()>
"this, : , <*>,  no longer contains , replica, .  refCount , <*>,  -> , <*>, <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
"this, : replica  refCount , <*>,  -> , <*>, <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
"Decommission complete for , node, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: boolean checkDecommissionState(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
"Reported NameNode version \', <*>, \' does not match , DataNode version \', <*>, \' but is within acceptable , limits. Note: This is normal during a rolling upgrade., ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
<*>,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRemoveCacheDirectiveInfo(java.lang.Long,boolean)>"
<*>,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logRemoveCacheDirectiveInfo(java.lang.Long,boolean)>"
"snapRoot, snapName",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logDeleteSnapshot(java.lang.String,java.lang.String,boolean)>"
"snapRoot, snapName",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logDeleteSnapshot(java.lang.String,java.lang.String,boolean)>"
"addDirective of , info,  failed: , ",warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
addDirective of {} successful.,info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
"*BLOCK* NameNode.blockReceivedAndDeleted: from , nodeReg,  , <*>,  blocks., ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])>"
"Recover RBW replica , b, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>"
"Recovering , rbw, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>"
"close(filename=, <*>, , block=, <*>, ), ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: void close()>
"got IOException closing stale peer , <*>, , which is , ageMs,  ms old, ",warn,"<org.apache.hadoop.hdfs.PeerCache: org.apache.hadoop.hdfs.net.Peer get(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean)>"
"DIR* FSDirectory.renameTo: , src,  to , dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void renameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
"LazyPersistFileScrubber was interrupted, exiting",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
Ignoring exception in LazyPersistFileScrubber:,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
"LazyWriter: Finish persisting RamDisk block:  block pool Id: , bpId,  block id: , blockId,  to block file , <*>,  and meta file , <*>,  on target volume , targetVolume, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
"Restoring , blockFile,  to , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.lang.String getRestoreDirectory(java.io.File)>
,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.util.Collection getVolumeInfo()>
"BLOCK* chooseExcessReplicates: (, chosen, , , b, ) is added to invalidated blocks set, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processChosenExcessReplica(java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block)>"
,mergeUnknownFields,<org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto)>
"Directory , <*>,  does not exist., ",info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Finalize upgrade for , <*>,  is not required., ",info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Finalizing upgrade of storage directory , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Finalize upgrade for , <*>,  is complete., ",info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"initialized with , <*>,  entries , <*>,  lookups, ",info,<org.apache.hadoop.hdfs.server.namenode.NameCache: void initialized()>
"Exception in creating socket address , <*>, ",warn,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>"
"Ending log segment , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void endCurrentLogSegment(boolean)>
timed poll(): timed out,debug,"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>"
"timed poll(): poll() returned null, sleeping for {} ms",debug,"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>"
"Using local interface , addr, ",debug,<org.apache.hadoop.hdfs.DFSClient: java.net.SocketAddress getRandomLocalInterfaceAddr()>
Using legacy short-circuit local reads.,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
No KeyProvider found.,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
"Found KeyProvider: , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
"dfs.client.test.drop.namenode.response.number is set to , <*>, , this hacked client will proactively drop responses, ",warn,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
"Using local interfaces , <*>,  with addresses , <*>, , ",debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
"got IOException while trying to validate header of , elf, .  Skipping., ",error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List getRemoteEditLogs(long,boolean)>"
"Renewed token for , <*>,  until: , <*>, ",debug,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
"Cancelled token for , <*>, ",debug,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
"Connecting to datanode , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
"<*>, : Transmitted , <*>,  (numBytes=, <*>, ) to , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
"<*>, : close-ack=, <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
"<*>, :Failed to transfer , <*>,  to , <*>,  got , ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
"<*>, : , block, \n  isClient  =, <*>, , clientname=, clientname, \n  isDatanode=, <*>, , srcDataNode=, srcDataNode, \n  inAddr=, inAddr, , myAddr=, myAddr, \n  cachingStrategy = , cachingStrategy, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"Could not get file descriptor for outputstream of class , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
IOException in BlockReceiver constructor. Cause is ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"DIR* FSDirectory.addBlock: , path,  with , block,  block is added to the in-memory , file system, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo addBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>"
Summary of operations loaded from edit log:\n  ,append,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: void dumpOpCounts(java.util.EnumMap)>
,debug,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: void dumpOpCounts(java.util.EnumMap)>
"Unable to fetch namespace information from active NN at , <*>, : , <*>, ",fatal,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
Full exception trace,debug,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
"Layout version on remote node (, <*>, ) does not match , this node\'s layout version (, <*>, ), ",fatal,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.,info,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
PendingReplicationMonitor thread is interrupted.,debug,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void run()>
"Starting CacheReplicationMonitor with interval , <*>,  milliseconds, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
Shutting down CacheReplicationMonitor,info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
Rescanning because of pending operations,info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
"Rescanning after , <*>,  milliseconds, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
"Scanned , <*>,  directive(s) and , <*>,  block(s) in , <*>,  , millisecond(s)., ",info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
Shutting down CacheReplicationMonitor.,info,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
Thread exiting,error,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
"Adding block pool , bpid, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>"
"Start checkpoint for , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
"renaming  , <*>,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean)>"
Directive {}: the directive expired at {} (now = {}),debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
Directive {}: got UnresolvedLinkException while resolving path {},debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
Directive {}: No inode found at {},debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
"Directive {}: ignoring non-directive, non-file inode {} ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
"url=, <*>, ",trace,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])>"
directive,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logModifyCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)>"
directive,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logModifyCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)>"
path,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logDisallowSnapshot(java.lang.String)>
"No data for block , blockId, ",debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>"
"Datanode responded with a block volume id we did not request, omitting.",debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>"
"Queued packet , <*>, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream: void queueCurrentPacket()>
"*BLOCK* NameNode.blockReport: from , nodeReg, , reports.length=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>"
,<init>,<org.apache.hadoop.hdfs.protocolPB.PBHelper: org.apache.hadoop.hdfs.server.protocol.RemoteEditLog convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteEditLogProto)>
"Invalid directory in: , <*>, : , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void makeBlockPoolDataDir(java.util.Collection,org.apache.hadoop.conf.Configuration)>"
"The given interval for marking stale datanode = , <*>, , which is less than , ,  heartbeat intervals. This may cause too frequent changes of , stale states of DataNodes since a heartbeat msg may be missing , due to temporary short-term failures. Reset stale interval to , <*>, ., ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
"The given interval for marking stale datanode = , staleInterval_, , which is larger than heartbeat expire interval , heartbeatExpireInterval, ., ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
.corrupt,renameSelf,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile: void moveAsideCorruptFile()>
"Cancelled image saving for , <*>, : , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>
"Unable to save image for , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>
"Aborting , this, ",warn,<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void abort()>
"Got: , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
"Decided to move , <*>,  bytes from , <*>,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: void matchSourceWithTargetToMove(org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)>"
"Opened streaming server at , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>
"Listening on UNIX domain socket: , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>
"this, : pulled the last slot , <*>,  out of , shm, ",trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>
"this, : pulled slot , <*>,  out of , shm, ",trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>
Logj is required to enable async auditlog,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enableAsyncAuditLog()>
"src, : masked=, <*>, ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.DFSOutputStream create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[])>"
genstamp,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logGenerationStampV2(long)>
"Not able to find datanode , hostname,  which has dependency with datanode , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup: int addDependentNodesToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)>"
"Not overwriting , $u,  with smaller file from , trash directory. This message can be safely ignored., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: int restoreBlockFilesFromTrash(java.io.File)>
"Purging logs older than , minTxIdToKeep, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void purgeLogsOlderThan(long)>
Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>
Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>
Unexpected exception while updating disk space.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitOrCompleteLastBlock(org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>"
"There are , <*>,  duplicate block , entries within the same volume., ",error,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void linkBlocks(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.io.File,int,org.apache.hadoop.fs.HardLink)>"
.trash,renameSelf,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile: void moveAsideTrashFile(long)>
"Unable to read transaction ids , firstTxIdInLogs, -, curTxIdOnOtherNode,  from the configured shared edits storage , <*>, . , Please copy these logs into the shared edits storage , or call saveNamespace on the active node.\n, Error: , <*>, ",fatal,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>"
"Unable to read transaction ids , firstTxIdInLogs, -, curTxIdOnOtherNode,  from the configured shared edits storage , <*>, . , Please copy these logs into the shared edits storage , or call saveNamespace on the active node.\n, Error: , <*>, ",fatal,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>"
Error registering FSDatasetState MBean,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>
Registered FSDatasetState MBean,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>
loggerFactory,<init>,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void <init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory)>"
"*DIR* NameNode.append: file , src,  for , clientName,  at , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock append(java.lang.String,java.lang.String)>"
"Finalizing edits file , <*>,  -> , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void finalizeLogSegment(long,long)>"
"current cluster id for sd=, <*>, ;lv=, <*>, ;cid=, <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
"this sd not available: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
couldn\'t find any VERSION file containing valid ClusterId,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
"Doing checkpoint. Last applied: , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
"Unable to roll forward using only logs. Downloading image with txid , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
"Loading image with txid , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
"Checkpoint completed in , <*>,  seconds.,  New Image Size: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
"Initializing shared journals for READ, already open for READ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initSharedJournalsForRead()>
"The dependency call returned null for host , <*>, ",error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"The dependency call returned null for host , <*>, ",<init>,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"Caught interrupted exception while waiting for thread , <*>,  to finish. Retrying join, ",error,<org.apache.hadoop.hdfs.server.namenode.FSImage: void waitForThreads(java.util.List)>
"updatePipeline(block=, oldBlock, , newGenerationStamp=, <*>, , newLength=, <*>, , newNodes=, <*>, , clientName=, clientName, ), ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"updatePipeline(, oldBlock, ) successfully to , newBlock, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"Recovery for replica , <*>,  on data-node , id,  is already in progress. Recovery id = , <*>,  is aborted., ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>
"Failed to obtain replica info for block (=, <*>, ) from datanode (=, id, ), ",warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>
"Convert , b,  from Temporary to RBW, visible length=, <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"this, : trying to create a remote block reader from a , TCP socket, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
"this, : got security exception while constructing , a remote block reader from , peer_, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
"Closed potentially stale remote peer , peer_, ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
I/O error constructing remote block reader.,warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
Interrupted. Stopping the WebImageViewer.,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServerAndWait(java.lang.String)>
"Scanning block pool , <*>,  on volume , <*>, ..., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
"Time taken to scan block pool , <*>,  on , <*>, : , timeTaken, ms, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
"Caught exception while scanning , <*>, . Will throw later., ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
"Created , <*>, ",info,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
"Cannot get delegation token from , renewer, ",info,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
"BLOCK* addToInvalidates: , b,  , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToInvalidates(org.apache.hadoop.hdfs.protocol.Block)>
"created new ShortCircuitRegistry with interruptCheck=, <*>, , shmPath=, <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
Disabling ShortCircuitRegistry,debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
"Abandoning , <*>, ",info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>
"Excluding datanode , <*>, ",info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>
"Bad checksum type: , <*>, . Using default , CRCC, ",warn,<org.apache.hadoop.hdfs.DFSClient$Conf: org.apache.hadoop.util.DataChecksum$Type getChecksumType(org.apache.hadoop.conf.Configuration)>
"this, : checked shared memory segment.  isStale=, <*>#__, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
"this,  is stale because it\'s , stale#,  ms old, and staleThresholdMs = , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
"this,  is not stale because it\'s only , stale#,  ms old, and staleThresholdMs = , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
"Skipping scan since bytesLeft=, <*>, , Start=, <*>, , period=, <*>, , now=, <*>,  , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean workRemainingInCurrentPeriod()>
"Image Transfer timeout configured to , <*>,  milliseconds, ",info,<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void setTimeout(java.net.HttpURLConnection)>
"Going to retain , <*>,  images with txid >= , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: long getImageTxIdToRetain(org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector)>
"Setting heartbeat recheck interval to , <*>,  since , dfs.namenode.stale.datanode.interval,  is less than , dfs.namenode.heartbeat.recheck-interval, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.conf.Configuration)>"
"leaseHolder, src, newHolder",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logReassignLease(java.lang.String,java.lang.String,java.lang.String)>"
,info,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void scan()>
"Restored , <*>,  block files from trash., ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doTransition(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Problem connecting to name-node: , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
"e_, . Shutting down., ",error,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
"Storage directory , dataDir,  has already been used., ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Failed to add storage for block pool: , <*>,  : , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
failed to create ShortCircuitShmManager,error,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void <init>(int,long,int,long,long,long,int)>"
"No version file in , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Unable to determine the max transaction ID seen by , sd, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Unable to inspect storage directory , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Checking file , f, ",debug,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Image file , f,  has improperly formatted , transaction ID, ",error,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"Found image file at , f,  but storage directory is , not configured to contain images., ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
error creating DomainSocket,warn,"<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: org.apache.hadoop.net.unix.DomainSocket createSocket(org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathInfo,int)>"
"Checkpoint done. New Image Size: , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>
Failed to write legacy OIV image: ,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>
"<*>, .removeLeaseWithPrefixPath: entry=, entry, ",debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLeaseWithPrefixPath(java.lang.String)>
"Renaming reserved path , <*>,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFullNameINodes(long,java.io.DataInput,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>"
"computePartialChunkCrc for , <*>, : sizePartialChunk=, sizePartialChunk, , block offset=, blkoff#, , metafile offset=, ckoff, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>"
"Read in partial CRC chunk from disk for , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>"
"For namenode , <*>,  using,  DELETEREPORT_INTERVAL of , <*>,  msec ,  BLOCKREPORT_INTERVAL of , <*>, msec,  CACHEREPORT_INTERVAL of , <*>, msec,  Initial delay: , <*>, msec, ; heartBeatInterval=, <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
"Took , <*>, ms to process , <*>,  commands from NN, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
"BPOfferService for , this,  interrupted, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
"this,  is shutting down, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
RemoteException in offerService,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
IOException in offerService,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
,setSinceTxId,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto)>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto)>
Block {}: removing from PENDING_UNCACHED for node {} because the DataNode uncached it.,trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: can\'t cache block because it is {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: removing from PENDING_CACHED for node {}because we already have {} cached replicas and we only need {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: removing from PENDING_UNCACHED for node {} because we only have {} cached replicas and we need {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
"Block {}: removing from cachedBlocks, since neededCached == , and pendingUncached and pendingCached are empty.",trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
"Invalidated , numOverReplicated_,  over-replicated blocks on , srcNode,  during recommissioning, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlocksOnReCommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Saving image file , newFile,  using , compression, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>"
"Image file , newFile,  of size , <*>,  bytes saved in , <*>,  seconds., ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>"
"BLOCK* addStoredBlock: , block,  on , <*>,  size , <*>,  but it does not belong to any file, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
block,logAddStoredBlock,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
"BLOCK* addStoredBlock: Redundant addStoredBlock request received for , storedBlock_,  on , <*>,  size , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
"Inconsistent number of corrupt replicas for , storedBlock_, blockMap has , <*>,  but corrupt replicas map has , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
"Number of active connections is: , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void <init>(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataXceiverServer)>"
"Removing node , <*>,  from the excluded nodes list, ",info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$2: void onRemoval(com.google.common.cache.RemovalNotification)>
"Purging old image , image, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeImage(org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile)>
"Error Recovery for , <*>,  waiting for responder to exit. , ",info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>
"Error recovering pipeline for writing , <*>, . Already retried  times for the same packet., ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>
"DataNode version: , <*>,  and NameNode layout version: , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Upgrading block pool storage directory , <*>, .\n   old LV = , <*>, ; old CTime = , <*>, .\n   new LV = , <*>, ; new CTime = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Upgrade of block pool , <*>,  at , <*>,  is complete, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Slow ReadProcessor read fields took , duration, ms (threshold=, <*>, ms); ack: , $u, , targets: , <*>, ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
"DFSClient , $u, ",debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
"A datanode is restarting: , <*>, ",info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
"DFSOutputStream ResponseProcessor exception  for block , <*>, ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
Using minimum value {} for {},info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager)>"
"Fenced by , fencerInfo,  with epoch , epoch, ",info,"<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.FenceResponse fence(org.apache.hadoop.hdfs.server.protocol.JournalInfo,long,java.lang.String)>"
"BLOCK* allocateBlock: , src, . , <*>,  , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo saveAllocatedBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>"
"Could not obtain block from any node:  , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
"Failed to connect to , <*>, :, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
Error reading block,error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
"Block , block,  cannot be repl from any node, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
"BLOCK* Removing , block,  from neededReplications as it has enough replicas, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
"BLOCK* Removing , <*>,  from neededReplications as it has enough replicas, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
"BLOCK* block , <*>,  is moved from neededReplications to pendingReplications, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
"BLOCK* ask , <*>,  to replicate , <*>,  to , $u, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
"BLOCK* neededReplications = , <*>,  pendingReplications = , <*>, ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
,info,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void processBlockInvalidation(org.apache.hadoop.hdfs.ExtendedBlockId)>
"Recover failed close , b, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String recoverClose(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
"can\'t get an mmap for , <*>,  of , <*>,  since SKIP_CHECKSUMS was not given, , we aren\'t skipping checksums, and the block is not mlocked., ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getClientMmap(java.util.EnumSet)>
"No file exists for block: , b, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
"Not able to delete the block file: , blockFile, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
"Not able to delete the meta block file: , metaFile, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
UNINITIALIZED,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLog$State: void <clinit>()>
BETWEEN_LOG_SEGMENTS,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLog$State: void <clinit>()>
IN_SEGMENT,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLog$State: void <clinit>()>
OPEN_FOR_READING,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLog$State: void <clinit>()>
CLOSED,<init>,<org.apache.hadoop.hdfs.server.namenode.FSEditLog$State: void <clinit>()>
"Address , targetAddr, <*>_, ",trace,<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>
"Address , targetAddr, <*>_, ",trace,<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>
"DIR* FSDirectory.unprotectedDelete: failed to remove , src,  because it does not exist, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>"
"DIR* FSDirectory.unprotectedDelete: failed to remove , src,  because the root is not allowed to be deleted, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>"
"BLOCK* findAndMarkBlockAsCorrupt: , blk,  not found, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String)>"
"Sending cacheReport from service actor: , this, ",debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
"CacheReport of , <*>,  block(s) took , <*>,  msec to generate and , <*>,  msecs for RPC and NN processing, ",debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder: org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto)>
,logAuditMessage,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger: void logAuditEvent(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager)>"
"Reading receipt verification byte for , slotId, ",trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
"Receipt verification is not enabled on the DataNode.  Not verifying , slotId, ",trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
"Unregistering , registeredSlotId_,  because the , requestShortCircuitFdsForRead operation failed., ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
"Unregistering , registeredSlotId_,  because the , requestShortCircuitFdsForRead operation failed., ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
"NameNode rolling its own edit log because number of edits in open segment exceeds threshold of , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
"Swallowing exception in , <*>, :, ",error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
"<*>,  was interrupted, exiting, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void linkAllBlocks(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.io.File)>"
"Need to save fs image? , <*>#__,  (staleImage=, <*>, , haEnabled=, <*>, , isRollingUpgrade=, <*>, ), ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>
"srcNode , srcNode,  is dead , when decommission is in progress. Continue to mark , it as decommission in progress. In that way, when it rejoins the , cluster it can continue the decommission process., ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean isReplicationInProgress(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"Formatting storage directory , sd, ",info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void recoverCreate(boolean)>
"Purging no-longer needed file , <*>, ",info,"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>"
"Unable to delete no-longer-needed data , f, ",warn,"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>"
"Failed to transfer block , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.hdfs.StorageType[][])>"
"Allowed RPC access from , <*>,  at , <*>, ",info,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>
"Disallowed RPC access from , <*>,  at , <*>, . Not listed in , dfs.cluster.administrators, ",warn,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>
blockId,logEdit,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logAllocateBlockId(long)>
Data node cannot fully support concurrent reading and writing without native code extensions on Windows.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void <clinit>()>
Received null remoteUser while authorizing access to GetJournalEditServlet,warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"Validating request made by , <*>,  / , <*>, . This user is: , <*>, ",debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
SecondaryNameNode principal could not be added,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"isValidRequestor is comparing to valid requestor: , msg, ",debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"isValidRequestor is allowing: , <*>, ",debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"isValidRequestor is allowing other JN principal: , <*>, ",debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"isValidRequestor is rejecting: , <*>, ",debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
"Planning to load image :\n, imageFile, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImageFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup,warn,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>"
Exception while checking heartbeat,error,<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor: void run()>
"<*>, : scheduling an incremental block report., ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>
"<*>, : scheduling a full block report., ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>
"NameNode low on available disk space. , Entering safe mode., ",warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
"NameNode low on available disk space. , Already in safe mode., ",warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
Exception in NameNodeResourceMonitor: ,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
"Finalizing upgrade for storage directory , <*>, .\n   cur LV = , <*>, ; cur CTime = , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
"storageTypes=, <*>, ",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
"Failed to place enough replicas, still in need of , <*>,  to reach , totalReplicasExpected,  (unavailableStorages=, unavailableStorages, , storagePolicy=, storagePolicy, , newBlock=, newBlock, ), ",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
"<*>,  , <*>, ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
"Fsck: deleted corrupt file , path, ",info,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>
"Fsck: error deleting corrupted file , path, ",error,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>
"Mapped HA service delegation token for logical URI , haUri,  to namenode , singleNNAddr, ",debug,"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>"
"No HA service delegation token found for logical URI , haUri, ",debug,"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>"
"Removing pending replication for , block, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks: void decrement(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Generating block token for , <*>, ",debug,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: byte[] createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier)>
"No class configured for , uriScheme, , , <*>,  is empty, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: java.lang.Class getJournalClass(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
"Cannot send OOB response , ackStatus, . Responder not running., ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
"Sending an out of band ack of type , ackStatus, ",info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exception in secureMain,fatal,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"Layout version rolled back to , <*>,  for storage , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Rolling back storage directory , <*>, .\n   target LV = , <*>, ; target CTime = , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Rollback of , <*>,  is complete, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"skip(n=, n, , block=, <*>, , filename=, <*>, ): discarded , discardedFromBuf_,  bytes from , dataBuf and advanced dataPos by , remaining_, ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: long skip(long)>
directive pc flags,logAddCacheDirectiveInfo,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: long addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)>"
"Added bpid=, blockPoolId,  to blockPoolScannerMap, new size=, <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlockPool(java.lang.String)>
Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
Interrupted waiting for namespace to freeze,warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
BackupNode namespace frozen.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
"Saved MD , digestString,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.util.MD5FileUtils: void saveMD5File(java.io.File,java.lang.String)>"
"addSymlink: failed to add , path, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
"addSymlink: , path,  is added, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
"Loading section , <*>,  length: , <*>, ",debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader load(java.lang.String)>
Getting new encryption token from NN,debug,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey newDataEncryptionKey()>
"initial capacity=, <*>, , max load factor= , maxLoadFactor, , min load factor= , minLoadFactor, ",debug,"<org.apache.hadoop.hdfs.util.LightWeightHashSet: void <init>(int,float,float)>"
"id, expiryTime",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logGetDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
"BLOCK* , <*>, : add , block,  to , datanode, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)>"
"Could not get block locations. Source file \, <*>, \ - Aborting..., ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
"Error Recovery for block , <*>,  in pipeline , $u, : bad datanode , <*>, ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
Failed to replace datanode. Continue with the remaining datanodes since dfs.client.block.write.replace-datanode-on-failure.best-effort is set to true.,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
"Datanode did not restart in time: , <*>, ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
"Failed to close the appender of , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void close()>
"Namenode for , nsId,  remains unresolved for ID , nnId, .  Check your hdfs-site.xml file to , ensure namenodes are configured properly., ",warn,"<org.apache.hadoop.hdfs.DFSUtil: java.util.Map getAddressesForNameserviceId(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String[])>"
"Failed to list directory , fullPath, . Ignore the directory and continue., ",warn,<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processPath(java.lang.String)>
"url=, <*>, ",trace,"<org.apache.hadoop.hdfs.web.HftpFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>"
"this, : trying to construct BlockReaderLocalLegacy, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
"this, : can\'t construct BlockReaderLocalLegacy because , the address , <*>,  is not local, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
"this, : can\'t construct , BlockReaderLocalLegacy because , disableLegacyBlockReaderLocal is set., ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
"this, : error creating legacy BlockReaderLocal.  , Disabling legacy local reads., ",warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
"Using hedged reads; pool threads=, num, ",debug,<org.apache.hadoop.hdfs.DFSClient: void initThreadsNumForHedgedReads(int)>
"Adjusting block totals from , <*>, /, <*>,  to , <*>, /, <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void adjustBlockTotals(int,int)>"
"this, : starting cache cleaner thread which will run , every , <*>,  ms, ",debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void startCacheCleanerThreadIfNeeded()>
"*DIR* NameNode.rename: , src,  to , dst, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>"
unregisterSlot: ShortCircuitRegistry is not enabled.,trace,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)>
"Block: , block, , Expected Replicas: , <*>, , live replicas: , <*>, , corrupt replicas: , <*>, , decommissioned replicas: , <*>, , excess replicas: , <*>, , Is Open File: , <*>, , Datanodes having this block: , <*>, , Current Datanode: , srcNode, , Is current datanode decommissioning: , <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void logBlockReplicationInfo(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)>"
Failed to start namenode.,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: void main(java.lang.String[])>
"DIR* FSDirectory.unprotectedRenameTo: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameDestination(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode)>"
"removeDirective of , id,  failed: , ",warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>"
"removeDirective of , id,  successful., ",info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>"
"Failed to delete restart meta file: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"<*>, \n, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"Shutting down for restart (, <*>, )., ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"Exception for , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"Failed to delete restart meta file: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"<*>, \n, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"Failed to delete restart meta file: , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"<*>, \n, <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
"STATE* Leaving safe mode after , <*>,  secs, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
STATE* Safe mode is OFF,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
"STATE* Network topology has , <*>,  racks and , <*>,  datanodes, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
"STATE* UnderReplicatedBlocks has , <*>,  blocks, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
"Error connecting to: , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void <init>(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.JournalInfo)>"
"Cancelled while waiting for datanode , <*>, : , <*>, ",info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
"Invalid access token when trying to retrieve information from datanode , <*>, ",warn,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
"Datanode , <*>,  does not support,  required #getHdfsBlocksMetadata() API, ",info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
"Failed to query block locations on datanode , <*>, : , <*>, ",info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Could not fetch information from datanode,debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Interrupted while fetching HdfsBlocksMetadata,info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
"msg_, . No responses yet., ",warn,"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>"
"msg_, . No responses yet., ",info,"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>"
Exporting access keys,debug,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys exportKeys()>
"Got a fatal error, exiting now",fatal,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void main(java.lang.String[])>
"Caught exception when adding , <*>, . Will throw later., ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>"
"Added volume - , <*>, , StorageType: , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>"
,mergeUnknownFields,<org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto$Builder: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto$Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto)>
"No block pool scanner found for block pool id: , poolId, ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
"Failover controller configured for NameNode , localTarget, ",info,"<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.tools.NNHAServiceTarget)>"
"Fast-forwarding stream \', <*>, \' to transaction ID , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
"got premature end-of-file at txid , <*>, ; expected file to go up to , <*>, ",<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
"Got error reading edit log input stream , <*>, ; failing over to edit log , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
"failing over to edit log , <*>, ",error,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
"this, : not trying to create a , remote block reader because the UNIX domain socket at , <*>,  is not usable., ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
"this, : trying to create a remote block reader from the , UNIX domain socket at , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
"this, : got security exception while constructing , a remote block reader from the unix domain socket at , <*>, ",trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
"Closed potentially stale domain peer , <*>, ",debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
"I/O error constructing remote block reader.  Disabling domain socket , <*>, ",warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
error closing DomainPeerServer: ,error,<org.apache.hadoop.hdfs.net.DomainPeerServer: void close()>
"Setting ADDRESS , address, ",info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void setServiceAddress(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"All volumes are within the configured free space balance threshold. Selecting , <*>,  for write of block size , replicaSize, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
"Volumes are imbalanced. Selecting , <*>,  from high available space volumes for write of block size , replicaSize, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
"Volumes are imbalanced. Selecting , <*>,  from low available space volumes for write of block size , replicaSize, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
SocketCache disabled.,info,"<org.apache.hadoop.hdfs.PeerCache: void <init>(int,long)>"
Exception ,debug,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void join()>
"FSCK started by , <*>,  from , <*>,  for path , <*>,  at , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>
"Fsck on path \', <*>, \' , FAILED, ",warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>
"*DIR* NameNode.complete: , src,  fileId=, fileId,  for , clientName, ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
"Slow flushOrSync took , <*>, ms (threshold=, <*>, ms), isSync:, isSync, , flushTotalNanos=, flushTotalNanos_, ns, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void flushOrSync(boolean)>
"<*>, : closing, ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void close()>
"Scheduling , <*>,  file , blockFile,  for deletion, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void deleteAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl,java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
"Quorum journal URI \', uri, \' has an even number , of Journal Nodes specified. This is not recommended!, ",warn,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: java.util.List getLoggerAddresses(java.net.URI)>
"*********** Upgrade is not supported from this  older version , oldVersion,  of storage to the current version.,  Please upgrade to , Hadoop-.,  or a later version and then upgrade to current,  version. Old layout version is , <*>_,  and latest layout version this software version can,  upgrade from is , $i, . ************, ",error,<org.apache.hadoop.hdfs.server.common.Storage: void checkVersionUpgradable(int)>
"msg,  \n, <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void reportStatus(java.lang.String,boolean)>"
"Token cancel failed: , <*>, ",debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void close()>
"Formatting block pool , <*>,  directory , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
"Skipping download of remote edit log , log,  since it already is stored locally at , f, ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
"Dest file: , f, ",debug,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
"Downloaded file , <*>,  size , <*>,  bytes., ",info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
"Renaming , <*>,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
"Unable to rename edits file from , <*>,  to , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
"<*>,  is interrupted, ",debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor: void run()>
"Not able to copy block , <*>,  , to , <*>,  because threads , quota is exceeded., ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"Copied , block,  to , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"opCopyBlock , block,  received exception , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"Found Checksum error for , <*>,  from , <*>,  at , <*>, ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>"
"Exception while reading from , <*>,  of , <*>,  from , <*>, ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>"
"open AuthenticatedURL connection, url, ",debug,"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>"
open URL connection,debug,"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>"
"DIR* FSDirectory.unprotectedAddFile: exception when add , path,  to the file system, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile unprotectedAddFile(long,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.util.List,java.util.List,short,long,long,long,boolean,java.lang.String,java.lang.String,byte)>"
"this, : closing, ",info,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void close()>
info,logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logModifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)>"
info,logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logModifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)>"
"Bumping up the client provided block\'s genstamp to latest , <*>,  for block , block, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"block=, block, , replica=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"Could not find metadata file for , block, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"<*>, :sendBlock() : , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"replica=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"block=, block, , bytesPerCRC=, <*>, , crcPerBlock=, crcPerBlock, , md=, <*>_, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"<*>, : starting, ",trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
"traceString_, : I/O error, ",trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
"traceString_, : returning , nRead_, ",trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
"DatanodeCommand action : DNA_REGISTER from , <*>,  with , <*>,  state, ",info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
bpid,<init>,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.fsdataset.RollingLogs createRollingLogs(java.lang.String,java.lang.String)>"
"Connecting to datanode , <*>,  addr=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>"
"this, : created mmap of size , <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
"this, : mmap error, ",warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
"this, : mmap error, ",warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
"DFSClient flush() : bytesCurBlock , <*>,  lastFlushOffset , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
"Unable to persist blocks in hflush for , <*>, ",warn,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
Error while syncing,warn,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
"Beginning recovery of unclosed segment starting at txid , segmentTxId, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
"Recovery prepare phase complete. Responses:\n, <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
"Using already-accepted recovery for segment starting at txid , segmentTxId, : , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
"Using longest log: , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
"One of the loggers had a response, but no best logger was found.",<init>,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
"None of the responders had a log to recover: , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
<*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
<*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
error reading hosts files: ,error,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
"dfs.block.invalidate.limit=, <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
"dfs.namenode.datanode.registration.ip-hostname-check=, <*>, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
Storage directory is in use.,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.datanode.DataStorage$VolumeBuilder prepareVolume(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.util.List)>"
"Space available on volume \', <*>, \' is , <*>, ",debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
"Space available on volume \', <*>, \' is , <*>, , which is below the configured reserved amount , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
starting recovery...,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY COMPLETE,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY FAILED: caught exception,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY FAILED: caught exception,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
"Encryption zone , <*>,  does not have a valid path., ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
"Could not find encryption XAttr for file , <*>,  in encryption zone , <*>, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
"Client is requesting a new log segment , txid,  though we are already writing , <*>, . , Aborting the current segment in order to begin the new one., ",warn,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>"
"Updating lastWriterEpoch from , <*>,  to , <*>,  for client , <*>, ",info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>"
"fetchBlockByteRange(). Got a checksum exception for , <*>,  at , <*>, :, <*>,  from , chosenNode, ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to , targetAddr,  : , <*>, ",info,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
"Connection failure: , <*>, ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
"Allocated new BlockPoolId: , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String)>"
"<*>, path, value, mtime, atime, <*>",logRpcIds,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSymlink(java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.server.namenode.INodeSymlink,boolean)>"
"<*>, path, value, mtime, atime, <*>",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSymlink(java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.server.namenode.INodeSymlink,boolean)>"
"getUGI is returning: , <*>, ",debug,"<org.apache.hadoop.hdfs.server.common.JspHelper: org.apache.hadoop.security.UserGroupInformation getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean)>"
"Failed to place enough replicas: expected size is , expectedSize,  but only , <*>,  storage types can be selected , (replication=, $i, , selected=, <*>, , unavailable=, unavailables, , removed=, $u, , policy=, this, ), ",warn,"<org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: java.util.List chooseStorageTypes(short,java.lang.Iterable,java.util.EnumSet,boolean)>"
"Discard the EditLog files, the given start txid is , startTxId, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>
"Trash the EditLog file , elf_, ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>
firstTxId lastTxId,waitForWriteQuorum,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void finalizeLogSegment(long,long)>"
"Start checkpoint at txid , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
"Name node , <*>,  has newer image layout version: LV = , <*>,  cTime = , <*>, . Current version: LV = , <*>,  cTime = , <*>, ",error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
"Restored trash for bpid , bpid, ",info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void restoreTrash(java.lang.String)>
"Total time to scan all replicas for block pool , bpid, : , arr$#, ms, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>"
"BLOCK* processReport: logged info for , <*>,  of , i$_,  reported., ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: java.util.Collection processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)>"
"Cancelling , <*>, ",info,<org.apache.hadoop.hdfs.DFSClient: void cancelDelegationToken(org.apache.hadoop.security.token.Token)>
"Decreasing replication from , $i,  to , $i,  for , src, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
"Increasing replication from , $i,  to , $i,  for , src, ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
"New BlockReaderLocalLegacy for file , <*>,  of size , <*>,  startOffset , startOffset,  length , length,  short circuit checksum , <*>_, ",debug,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>"
"BlockReaderLocalLegacy: Removing , blk,  from cache because local file , <*>,  could not be opened., ",warn,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>"
"Purging remote journals older than txid , minTxIdToKeep, ",info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void purgeLogsOlderThan(long)>
"BLOCK* Removing stale replica from location: , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void setGenerationStampAndVerifyReplicas(long)>
"Cached dfsUsed found for , <*>, : , <*>, ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: long loadDfsUsed()>
"Connecting to datanode , <*>, ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
Exception encountered:,debug,<org.apache.hadoop.hdfs.tools.DFSAdmin: int run(java.lang.String[])>
"Did not renew lease for client , c, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>
"Lease renewed for client , <*>, ",debug,<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>
"<*>, : , b,  (numBytes=, <*>, ), , stage=, stage, , clientname=, clientname, , targets=, <*>, , target storage types=, <*>_, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)>"
"Rolling forward previously half-completed synchronization: , <*>,  -> , <*>, ",info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void completeHalfDoneAcceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData)>
"src, nsQuota, dsQuota",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSetQuota(java.lang.String,long,long)>"
"In safemode, not computing replication work",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"DataNode , dn,  cannot be found with UUID , <*>, , removing block invalidation work., ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"BLOCK* , <*>, : ask , dn,  to delete , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"Edits URI , dir,  listed multiple times in , dfs.namenode.shared.edits.dir, . Ignoring duplicates., ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>"
"Edits URI , i$,  listed multiple times in , dfs.namenode.shared.edits.dir,  and , dfs.namenode.edits.dir, . Ignoring duplicates., ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>"
hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.,warn,<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>
dfs.https.enable is deprecated. Please use dfs.http.policy.,warn,<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>
"newInfo = , <*>, ",debug,<org.apache.hadoop.hdfs.DFSInputStream: long fetchLocatedBlocksAndGetLastBlockLength()>
"Failed to mkdirs , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
"Failed to move meta file from , file,  to , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
"Failed to move block file from , <*>,  to , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
"Failed to move , <*>,  to , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
Exception occured while compiling report: ,warn,"<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler: java.util.LinkedList compileReport(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,java.util.LinkedList)>"
"DFSClient readNextPacket got header , <*>, ",trace,<org.apache.hadoop.hdfs.RemoteBlockReader2: void readNextPacket()>
"Unable to stop HTTP server for , this, ",warn,<org.apache.hadoop.hdfs.qjournal.server.JournalNode: void stop(int)>
"Unresolved datanode registration: , <*>, ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"BLOCK* registerDatanode: from , nodeReg,  storage , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"BLOCK* registerDatanode: , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
BLOCK* registerDatanode: node restarted.,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"BLOCK* registerDatanode: , <*>,  is replaced by , nodeReg,  with the same storageID , <*>, ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
<*>,remove,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
<*>,add,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
"Sending heartbeat with , <*>,  storage reports from service actor: , this, ",debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse sendHeartBeat()>
"deleting  , <*>,  FAILED, ",warn,"<org.apache.hadoop.hdfs.util.MD5FileUtils: void renameMD5File(java.io.File,java.io.File)>"
,<init>,"<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long,boolean)>"
sinceTxId inProgressOk,<init>,"<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long,boolean)>"
"this,  starting to offer service, ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Initialization failed for , this,  , <*>, ",error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Initialization failed for , this, . Exiting. , ",fatal,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Ending block pool service for: , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Exception in BPOfferService for , this, ",error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Ending block pool service for: , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Unexpected exception in block pool , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Ending block pool service for: , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
"Ending block pool service for: , this, ",warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
"Slow manageWriterOsCache took , <*>, ms (threshold=, <*>, ms), ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>
"Error managing cache for writer of block , <*>, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>
Encountered exception setting Rollback Image,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo$Bean getRollingUpgradeStatus()>
"src, policyId",logEdit,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSetStoragePolicy(java.lang.String,byte)>"
"Found existing DT for , <*>, ",debug,<org.apache.hadoop.hdfs.web.TokenAspect: void initDelegationToken(org.apache.hadoop.security.UserGroupInformation)>
"Recovered , <*>,  replicas from , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>"
"Block pool storage directory , dataDir,  does not exist, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"Block pool storage directory , dataDir,  is not formatted for , <*>, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
"persistNewBlock: , path,  with new block , <*>, , current total block count is , <*>, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistNewBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
"set restore failed storage to , val, ",warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void setRestoreFailedStorage(boolean)>
Failed to set keys,error,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
InterruptedException in block key updater thread,debug,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
Exception in block key updater thread,error,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
"blocks = , <*>, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlocks createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo)>"
"<*>, : starting, ",info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
"traceString_, : I/O error, ",info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
"traceString_, : returning , nRead_, ",info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
Periodic block scanner is not running,warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Periodic block scanner is not running. Please check the datanode log if this is unexpected.,append,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"block=, <*>, , (length=, <*>, ), syncList=, syncList, ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>"
"Failed to updateBlock (newblock=, $u, , datanode=, <*>, ), ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>"
"Triggering log roll on remote NameNode , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>
Unable to trigger a roll of the active NN,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>
"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.",error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: boolean parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration)>"
"DIR* FSDirectory.unprotectedRenameTo: failed to rename , src,  to , dst_,  because destination exists, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
"DIR* FSDirectory.unprotectedRenameTo: failed to rename , src,  to , dst_,  because destination\'s parent does not exist, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
"DIR* FSDirectory.unprotectedRenameTo: failed to rename , src,  to , dst_,  because the source can not be removed, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
"DIR* FSDirectory.unprotectedRenameTo: , src,  is renamed to , dst_, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
"DIR* FSDirectory.unprotectedRenameTo: failed to rename , src,  to , dst_, ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
"Cannot parse line: , line, ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry parseEntry(java.lang.String)>
"Error reporting an error to NameNode , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void trySendErrorReport(int,java.lang.String)>"
"Periodic Block Verification Scanner initialized with interval , hours_,  hours for block pool , bpid, ",info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>"
Could not open verfication log. Verification times are not stored.,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>"
"DIR* NameSystem.renameTo: with options - , srcArg,  to , dstArg, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void renameTo(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>"
"<*>, : enqueue , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
"Unable to purge old storage , <*>, ",warn,<org.apache.hadoop.hdfs.server.namenode.FSImage: void purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>
"UnderReplicationBlocks.update , block,  curReplicas , curReplicas,  curExpectedReplicas , curExpectedReplicas,  oldReplicas , oldReplicas,  oldExpectedReplicas  , oldExpectedReplicas,  curPri  , <*>,  oldPri  , <*>, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>"
"BLOCK* NameSystem.UnderReplicationBlock.update:, block,  has only , curReplicas,  replicas and needs , curExpectedReplicas,  replicas so is added to neededReplications,  at priority level , <*>, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>"
"Renaming , <*>,  to , <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
"Renaming , <*>,  to , $u, , file length=, <*>, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
"Cannot move meta file , <*>, back to the finalized directory , <*>, ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
"Created new DT for , <*>, ",debug,<org.apache.hadoop.hdfs.web.TokenAspect: void ensureTokenInitialized()>
"resolveDuplicateReplicas decide to keep , replicaToKeep_, .  Will try to delete , <*>_, ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: org.apache.hadoop.hdfs.server.datanode.ReplicaInfo selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>"
"this, : , purgeReason#__, ",debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
"this, : unref replica , replica, : , purgeReason_,  refCount , <*>,  -> , <*>, <*>, ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
requestShortCircuitFdsForRead failed,debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.FileInputStream[] requestShortCircuitFdsForRead(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,int)>"
"WebImageViewer started. Listening on , <*>, . Press Ctrl+C to stop the viewer., ",info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServer(java.lang.String)>
"NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size = , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
"currently disabled dir , <*>, ; type=, <*>, ;canwrite=, <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
"restoring dir , <*>, ",info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
"read off , off,  len , len, ",trace,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: int read(byte[],int,int)>"
"Evicting block , <*>, ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void evictBlocks()>
"Storage directory , <*>,  is not formatted., ",info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>
Formatting ...,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>
"commitBlockSynchronization(lastblock=, lastblock, , newgenerationstamp=, newgenerationstamp, , newlength=, newlength, , newtargets=, <*>, , closeFile=, closeFile, , deleteBlock=, deleteblock, ), ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"Block (=, lastblock, ) not found, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"Unexpected block (=, lastblock, ) since the file (=, <*>, ) is not under construction, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"DatanodeDescriptor (=, <*>, ) not found, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"commitBlockSynchronization(newblock=, lastblock, , file=, src_, , newgenerationstamp=, newgenerationstamp, , newlength=, newlength, , newtargets=, <*>, ) successful, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"commitBlockSynchronization(, lastblock, ) successful, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"Using NN principal: , <*>, ",debug,<org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
"Retry cache on namenode is , <*>_, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>
"Retry cache will use , <*>,  of total heap and retry cache entry expiry time is , <*>,  millis, ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>
"BLOCK* NameSystem.getAdditionalBlock: , src,  inodeId , fileId,  for , clientName, ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalBlock(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.util.Set,java.util.List)>"
"Execution rejected, Executing in current thread",info,"<org.apache.hadoop.hdfs.DFSClient$2: void rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)>"
"Renamed root path .reserved to , renameString, ",info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: byte[] renameReservedRootComponentOnUpgrade(byte[],int)>"
"BLOCK* NameSystem.UnderReplicationBlock.add:, block,  has only , curReplicas,  replicas and need , expectedReplicas,  replicas so is added to neededReplications,  at priority level , <*>, ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean add(org.apache.hadoop.hdfs.protocol.Block,int,int,int)>"
"Fsck: ignoring open file , <*>, ",info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void check(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result)>"
"Ramping down all scheduled reduces:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void preemptReducesIfNeeded()>
"Going to preempt , <*>,  due to lack of space for maps, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void preemptReducesIfNeeded()>
Job init failed,warn,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: org.apache.hadoop.mapreduce.v2.app.job.JobStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>"
"OutputCommitter set in config , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1: org.apache.hadoop.mapreduce.OutputCommitter call(org.apache.hadoop.conf.Configuration)>
"OutputCommitter is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1: org.apache.hadoop.mapreduce.OutputCommitter call(org.apache.hadoop.conf.Configuration)>
Could not parse the old history file. Will not have old AMinfos ,warn,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: java.util.List readJustAMInfos()>
TaskHeartbeatHandler thread interrupted,info,<org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler$PingChecker: void run()>
"Task cleanup failed for attempt , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void handleTaskAbort(org.apache.hadoop.mapreduce.v2.app.commit.CommitterTaskAbortEvent)>
"Job failed as tasks failed. failedMaps:, <*>,  failedReduces:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition: org.apache.hadoop.mapreduce.v2.app.job.JobStateInternal checkJobAfterTaskCompletion(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
"Got allocated containers , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Assigning container , <*>,  with priority , <*>,  to NM , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Cannot assign container , allocated_,  for a map as either ,  container memory less than required , <*>,  or no pending map tasks - maps.isEmpty=, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Cannot assign container , allocated_,  for a reduce as either ,  container memory less than required , <*>,  or no pending reduce tasks - reduces.isEmpty=, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Container allocated at unwanted priority: , <*>, . Returning to RM..., ",warn,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Got allocated container on a blacklisted  host , <*>, . Releasing container , allocated_, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Placing a new container request for task attempt , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Could not map allocated container to a valid request. Releasing allocated container , allocated_, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Releasing unassigned and invalid container , allocated_, . RM may have assignment issues, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
"Progress of TaskAttempt , taskAttemptID,  is : , <*>, ",info,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: boolean statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus)>"
KnownNode Count at . Not computing ignoreBlacklisting,info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void computeIgnoreBlacklisting()>
"Ignore blacklisting set to true. Known: , <*>, , Blacklisted: , <*>, , , val, %, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void computeIgnoreBlacklisting()>
"Ignore blacklisting set to false. Known: , <*>, , Blacklisted: , <*>, , , val, %, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void computeIgnoreBlacklisting()>
Using mapred newApiCommitter.,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Attempt num: , <*>,  is last retry: , <*>,  because the staging dir doesn\'t exist., ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Attempt num: , <*>,  is last retry: , <*>,  because a commit was started., ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Not generating HistoryFinish event since start event not generated for task: , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillNewTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"No file for job-history with , jobId,  found in cache!, ",warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
"No file for jobconf with , jobId,  found in cache!, ",warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
"Unable to write out JobSummaryInfo to , qualifiedSummaryDoneFile_, , ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
"Error closing writer for JobID: , jobId, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
"Not generating HistoryFinish event since start event not generated for task: , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptKilledTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"Job end notification URL not set, skipping.",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void notify(org.apache.hadoop.mapreduce.v2.api.records.JobReport)>
"Job end notification couldn\'t parse , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void notify(org.apache.hadoop.mapreduce.v2.api.records.JobReport)>
"Job end notification attempts left , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void notify(org.apache.hadoop.mapreduce.v2.api.records.JobReport)>
"Job end notification failed to notify : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void notify(org.apache.hadoop.mapreduce.v2.api.records.JobReport)>
"Job end notification succeeded for , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void notify(org.apache.hadoop.mapreduce.v2.api.records.JobReport)>
Failed while getting the configured log directories,error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Failed while checking for/creating  history staging path: , <*>, , ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Creating intermediate history logDir: , <*>,  + based on conf. Should ideally be created by the JobHistoryServer: , yarn.app.mapreduce.am.create-intermediate-jh-base-dir, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Not creating intermediate history logDir: , <*>,  based on conf: , yarn.app.mapreduce.am.create-intermediate-jh-base-dir, . Either set to true or pre-create this directory with,  appropriate permissions, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Failed checking for the existance of history intermediate done directory: , e_, , ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Error creating user intermediate history done directory:  , <*>, , ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
Timeline service is enabled,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
Emitting job history data to the timeline server is enabled,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
Timeline service is not enabled,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
Emitting job history data to the timeline server is not enabled,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Invalid event , type,  on Task , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void internalError(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType)>
"Stopping JobHistoryEventHandler. Size of the outstanding queue size is , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Interrupting Event Handling thread,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Null event handling thread,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Waiting for Event Handling thread to complete,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Interrupted Exception while stopping,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
"Shutting down timer for , mi_, ",debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
"Exception while cancelling delayed flush timer. Likely caused by a failed flush , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
"In stop, writing event , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
"Found jobId , e,  to have not been closed. Will close, ",warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
"Exception while closing file , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Stopped JobHistoryEventHandler. super.stop(),info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
We are finishing cleanly so this is the last retry,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void shutDownJob()>
Calling stop for all the services,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void shutDownJob()>
"Job end notification started for jobID : , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void shutDownJob()>
"Job end notification interrupted for jobID : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void shutDownJob()>
Graceful stop failed ,warn,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void shutDownJob()>
"<*>,  given a go for committing the task output., ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"<*>,  already given a go for committing the task output, so killing , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"Recovered output from task attempt , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"Unable to recover task attempt , <*>, ",error,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"Task attempt , <*>,  will be recovered as KILLED, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"TaskAttempt, <*>,  had not completed, recovering as KILLED, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"TaskAttempt , <*>,  found in unexpected state , recoveredState_, , recovering as KILLED, ",warn,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"Task cleanup failed for attempt , <*>, ",warn,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"Log Directory is null, returning",error,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,java.lang.String)>"
Missing Log Directory for History,<init>,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,java.lang.String)>"
"Event Writer setup for JobId: , jobId, , File: , <*>, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,java.lang.String)>"
"Could not create log file: , <*>,  + for job , , <*>, , ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,java.lang.String)>"
Failed to write the job configuration file,info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,java.lang.String)>"
Assigned from earlierFailedMaps,info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignToFailedMap(org.apache.hadoop.yarn.api.records.Container)>
"Kill job , <*>,  received from , <*>,  at , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler: org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillJobResponse killJob(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillJobRequest)>
Can\'t make a speculation runtime estimator,error,"<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: org.apache.hadoop.mapreduce.v2.app.speculate.TaskRuntimeEstimator getEstimator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)>"
Can\'t make a speculation runtime estimator,error,"<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: org.apache.hadoop.mapreduce.v2.app.speculate.TaskRuntimeEstimator getEstimator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)>"
Can\'t make a speculation runtime estimator,error,"<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: org.apache.hadoop.mapreduce.v2.app.speculate.TaskRuntimeEstimator getEstimator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)>"
Can\'t make a speculation runtime estimator,error,"<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: org.apache.hadoop.mapreduce.v2.app.speculate.TaskRuntimeEstimator getEstimator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)>"
"Failed to render tasks page with task type : , <*>,  for job id : , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.webapp.AppController: void tasks()>
"The job-jar file on the remote FS is , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
Job jar is not present. Not adding any jar to the list of resources.,info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
"The job-conf file on the remote FS is , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
"Adding #, <*>,  tokens and #, <*>,  secret keys for NM use for launching container, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
"Size of containertokens_dob is , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
Putting shuffle token in serviceData,info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
Cannot locate shuffle secret in credentials. Using job token as shuffle secret.,warn,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
"Adding ShuffleProvider Service: , shuffleProvider,  to serviceData, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: org.apache.hadoop.yarn.api.records.ContainerLaunchContext createCommonContainerLaunchContext(java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.Token,org.apache.hadoop.mapred.JobID,org.apache.hadoop.security.Credentials)>"
"Diagnostics report from , <*>, : , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DiagnosticInformationUpdater: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>"
"Result of canCommit for , taskAttemptID, :, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: boolean canCommit(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)>
"Reduce preemption successful , tId, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests: boolean remove(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)>
"Processing the event , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator: void handle(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"Error while reading , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock: void render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)>
"ApplicationMaster is out of sync with ResourceManager, hence resync and send outstanding requests.",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
"Could not contact RM after , <*>,  milliseconds., ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
"headroom=, e, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
"Received new Container :, cont, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
"Received completed container , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
"Container complete event for unknown container id , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
Exception while unregistering ,error,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void unregister()>
"Job end notification using proxy type \, proxyType_, \ hostname \, <*>, \ and port \, <*>, \, ",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void setConf(org.apache.hadoop.conf.Configuration)>
"Job end notification couldn\'t parse configured proxy\'s port , <*>, . Not going to use a proxy, ",warn,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: void setConf(org.apache.hadoop.conf.Configuration)>
"Error closing writer for JobID: , jobId, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void closeEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
Could not abort job,warn,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void handleJobAbort(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobAbortEvent)>
"param,  is null, ",error,"<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void validateInputParam(java.lang.String,java.lang.String)>"
"Issuing kill to other attempt , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptSucceededTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"Assigned container (, allocated, ) ,  to task , <*>,  on node , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void containerAssigned(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest)>"
"JobHistoryEventHandler notified that forceJobCompletion is , forceJobCompletion, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void setForcejobCompletion(boolean)>
Job setup failed,warn,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void handleJobSetup(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobSetupEvent)>
"Not decrementing resource as , resourceName,  is not present in request table, ",debug,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>"
"BEFORE decResourceRequest: applicationId=, <*>,  priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>"
"AFTER decResourceRequest: applicationId=, <*>,  priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>"
"Error putting entity , <*>,  to Timeline, Server, ",error,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processEventForTimelineServer(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId,long)>"
"Error putting entity , <*>,  to Timeline, Server, ",error,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processEventForTimelineServer(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId,long)>"
"removed attempt , <*>,  from the futures to keep track of, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map)>"
"removed attempt , <*>,  from the futures to keep track of, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map)>"
"oopsie...  this can never happen: , <*>, ",fatal,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map)>"
"removed attempt , <*>,  from the futures to keep track of, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map)>"
"removed attempt , <*>,  from the futures to keep track of, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map)>"
Job Staging directory is null,warn,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void cleanupStagingDir()>
"Deleting staging directory , <*>,  , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void cleanupStagingDir()>
"Failed to cleanup staging dir , jobTempDir_, ",error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void cleanupStagingDir()>
"Returning, interrupted : , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$1: void run()>
"Error in handling event type , <*>,  to the ContainreAllocator, ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$1: void run()>
"Getting task report for , <*>,    , <*>, . Report-size will be , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler: org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsResponse getTaskReports(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsRequest)>
"Too many fetch-failures for output of task attempt: , task#,  ... raising fetch failure to map, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskAttemptFetchFailureTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>"
"Returning, interrupted : , <*>, ",error,<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void run()>
"Processing the event , <*>, ",info,<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void run()>
"canceling the task attempt , <*>, ",info,<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void run()>
"Ignoring unexpected event , <*>, ",warn,<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void run()>
HADOOP_ROOT_LOGGER,put,"<org.apache.hadoop.mapred.MapReduceChildJVM: void setVMEnv(java.util.Map,org.apache.hadoop.mapred.Task)>"
STDOUT_LOGFILE_ENV,put,"<org.apache.hadoop.mapred.MapReduceChildJVM: void setVMEnv(java.util.Map,org.apache.hadoop.mapred.Task)>"
STDERR_LOGFILE_ENV,put,"<org.apache.hadoop.mapred.MapReduceChildJVM: void setVMEnv(java.util.Map,org.apache.hadoop.mapred.Task)>"
"Launching , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container: void launch(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent)>
"Shuffle port returned by ContainerManager for , <*>,  : , port_, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container: void launch(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent)>
"msgPrefix, PendingReds:, <*>,  ScheduledMaps:, <*>,  ScheduledReds:, <*>,  AssignedMaps:, <*>,  AssignedReds:, <*>,  CompletedMaps:, <*>,  CompletedReds:, <*>,  ContAlloc:, <*>,  ContRel:, <*>,  HostLocal:, <*>,  RackLocal:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduleStats: void log(java.lang.String)>
"Scheduling a redundant attempt for task , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RedundantScheduleTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
InterruptedException while stopping,warn,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void serviceStop()>
"getResources() for , <*>, :,  ask=, <*>,  release= , <*>,  newContainers=, <*>,  finishedContainers=, <*>,  resourcelimit=, <*>,  knownNMs=, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse makeRemoteRequest()>
"Update the blacklist for , <*>, : blacklistAdditions=, <*>,  blacklistRemovals=, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse makeRemoteRequest()>
"Processing the event , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor: void run()>
"Not generating HistoryFinish event since start event not generated for taskAttempt: , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$TooManyFetchFailureTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>"
"MRAppMaster uberizing job , <*>,  in local container (\uber-AM\) on node , <*>, :, <*>, ., ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceStart()>
"MRAppMaster launching normal, non-uberized, multi-container job , <*>, ., ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void serviceStart()>
"Copying , <*>,  to , <*>, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
"Copied to done location: , toPath, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
copy failed,info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>"
Writing event,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void writeEvent(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent)>
"Upper limit on the thread pool size is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
"RMCommunicator notified that isSignalled is: , isSignalled, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void setSignalled(boolean)>
"Fail task attempt , <*>,  received from , <*>,  at , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler: org.apache.hadoop.mapreduce.v2.api.protocolrecords.FailTaskAttemptResponse failTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.FailTaskAttemptRequest)>
"We launched , <*>,  speculations.  Sleeping , <*>,  milliseconds., ",info,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1: void run()>
"Background thread returning, interrupted",error,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1: void run()>
"Returning, interrupted : , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$1: void run()>
"Setting ContainerLauncher pool size to , <*>,  as number-of-nodes to talk to is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$1: void run()>
"mapResourceRequest:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"MAP capability required is more than the supported max container capability in the cluster. Killing the Job. mapResourceRequest: , <*>,  maxContainerCapability:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"reduceResourceRequest:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"REDUCE capability required is more than the supported max container capability in the cluster. Killing the Job. reduceResourceRequest: , <*>,  maxContainerCapability:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"Processing the event , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"Could not deallocate container for task attemptId , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"Job end notification trying , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: boolean notifyURLOnce()>
"Job end notification to , <*>,  failed with code: , <*>,  and message \, <*>, \, ",warn,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: boolean notifyURLOnce()>
"Job end notification to , <*>,  succeeded, ",info,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: boolean notifyURLOnce()>
"Job end notification to , <*>,  failed, ",warn,<org.apache.hadoop.mapreduce.v2.app.JobEndNotifier: boolean notifyURLOnce()>
error trying to open previous history file. No history data will be copied over.,warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService: void parse()>
"Got an error parsing job-history file, ignoring incomplete events.",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService: void parse()>
Child starting,debug,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
Executing with tokens:,info,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"PID: , <*>, ",debug,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"Sleeping for , sleepTimeMilliSecs, ms before retrying again. Got null now., ",info,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
FSError from child,fatal,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"Exception running child : , <*>, ",warn,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"Exception cleaning up: , <*>, ",info,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"Error running child : , <*>, ",fatal,<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
"Unexpected event for REDUCE task , <*>, ",error,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveFailureTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"Task final state is not FAILED or KILLED: , <*>, ",error,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>"
In flush timer task,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$FlushTimerTask: void run()>
Before Scheduling: ,updateAndLogIfChanged,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void heartbeat()>
After Scheduling: ,updateAndLogIfChanged,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void heartbeat()>
"Input size for job , <*>,  = , inputLength, . Number of splits = , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void createMapTasks(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,long,org.apache.hadoop.mapreduce.split.JobSplit$TaskSplitMetaInfo[])>"
"nodeBlacklistingEnabled:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void serviceInit(org.apache.hadoop.conf.Configuration)>
"maxTaskFailuresPerNode is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void serviceInit(org.apache.hadoop.conf.Configuration)>
"blacklistDisablePercent is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Setting job diagnostics to , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void doUnregistration()>
"History url is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void doUnregistration()>
Waiting for application to be successfully unregistered.,info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void doUnregistration()>
"DefaultSpeculator.addSpeculativeAttempt -- we are speculating , taskID, ",info,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: void addSpeculativeAttempt(org.apache.hadoop.mapreduce.v2.api.records.TaskId)>
"Diagnostics report from , <*>, : , <*>, ",info,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: void reportDiagnosticInfo(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"Num completed Tasks: , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition: org.apache.hadoop.mapreduce.v2.app.job.JobStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>"
"Commit go/no-go request from , <*>, ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: boolean canCommit(org.apache.hadoop.mapred.TaskAttemptID)>
"Processing , <*>,  of type , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
"Can\'t handle this event at current state for , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
"<*>,  Task Transitioned from , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
Recovery is enabled. Will try to recover from previous life on best effort basis.,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void processRecovery()>
"Unable to parse prior job history, aborting recovery",warn,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void processRecovery()>
"Will not try to recover. recoveryEnabled: , <*>,  recoverySupportedByCommitter: , <*>,  numReduceTasks: , <*>,  shuffleKeyValidForRecovery: , shuffleKeyValidForRecovery,  ApplicationAttemptID: , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void processRecovery()>
"Processing , <*>,  of type , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
"Can\'t handle this event at current state for , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
"<*>,  TaskAttempt Transitioned from , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
"TaskAttempt: , <*>,  using containerId: , <*>,  on NM: , <*>, , ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void sendLaunchedEvents()>
"Created MRAppMaster for application , applicationAttemptId, ",info,"<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void <init>(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,int,int,org.apache.hadoop.yarn.util.Clock,long)>"
"Kill task attempt , <*>,  received from , <*>,  at , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler: org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskAttemptResponse killTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskAttemptRequest)>
"Task: , taskAttemptID,  - exited : , msg, ",fatal,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: void fatalError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"Processing , <*>,  of type , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>
Can\'t handle this event at current state,error,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>
"<*>, Job Transitioned from , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>
We got asked to run a debug speculation scan.,info,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: void scanForSpeculations()>
"Host , hostName,  is already blacklisted., ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void containerFailedOnHost(java.lang.String)>
"<*>,  failures on node , hostName, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void containerFailedOnHost(java.lang.String)>
"Blacklisted host , hostName, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void containerFailedOnHost(java.lang.String)>
"Error JobHistoryEventHandler in handleEvent: , event, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void handleEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent)>
"In HistoryEventHandler , <*>, ",debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void handleEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent)>
"Error writing History Event: , <*>, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void handleEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent)>
"Perms after creating , $i, , Expected: , $i, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)>"
"Explicitly setting permissions to : , $i, , , fsp, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)>"
"Directory: , path,  already exists., ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)>"
"Error closing writer for JobID: , id, ",error,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void closeWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
"Number of reduces for job , <*>,  = , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void createReduceTasks(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
"Task succeeded with attempt , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void sendTaskSucceededEvents()>
"Created attempt , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl addAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar)>
"Size of event-queue in RMContainerAllocator is , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handle(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"Very low remaining capacity in the event-queue of RMContainerAllocator: , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handle(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent)>
"MapCompletionEvents request from , <*>, . startIndex , startIndex,  maxEvents , maxEvents, ",info,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: org.apache.hadoop.mapred.MapTaskCompletionEventsUpdate getMapCompletionEvents(org.apache.hadoop.mapred.JobID,int,int,org.apache.hadoop.mapred.TaskAttemptID)>"
Can\'t make a speculator -- check yarn.app.mapreduce.am.job.speculator.class,error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$2: org.apache.hadoop.mapreduce.v2.app.speculate.Speculator call(org.apache.hadoop.conf.Configuration)>
Can\'t make a speculator -- check yarn.app.mapreduce.am.job.speculator.class,error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$2: org.apache.hadoop.mapreduce.v2.app.speculate.Speculator call(org.apache.hadoop.conf.Configuration)>
Can\'t make a speculator -- check yarn.app.mapreduce.am.job.speculator.class,error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$2: org.apache.hadoop.mapreduce.v2.app.speculate.Speculator call(org.apache.hadoop.conf.Configuration)>
Can\'t make a speculator -- check yarn.app.mapreduce.am.job.speculator.class,error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$2: org.apache.hadoop.mapreduce.v2.app.speculate.Speculator call(org.apache.hadoop.conf.Configuration)>
Timeout expired in FAIL_WAIT waiting for tasks to get killed. Going to fail job anyway,info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$JobFailWaitTimedOutTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>"
"Failed to resolve address: , src, . Continuing to use the same., ",warn,<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: java.lang.String resolveHost(java.lang.String)>
"Unable to delete unexpected local file/dir , <*>, : insufficient permissions?, ",warn,<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void relocalize()>
"RMCommunicator notified that shouldUnregistered is: , shouldUnregister, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void setShouldUnregister(boolean)>
Skipping cleaning up the staging dir. assuming AM will be retried.,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService: void serviceStop()>
Failed to cleanup staging dir: ,error,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService: void serviceStop()>
"Size of the JobHistory event queue is , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1: void run()>
EventQueue take interrupted. Returning,info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1: void run()>
Event handling interrupted,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1: void run()>
"ATTEMPT_START , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: void processSpeculatorEvent(org.apache.hadoop.mapreduce.v2.app.speculate.SpeculatorEvent)>
"JOB_CREATE , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: void processSpeculatorEvent(org.apache.hadoop.mapreduce.v2.app.speculate.SpeculatorEvent)>
"Host matched to the request list , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
"Assigned based on host match , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
"Assigned based on rack match , <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
Assigned based on * match,debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
"Local filesystem , <*>,  is unsupported?? (should never happen), ",error,"<org.apache.hadoop.mapred.LocalContainerLauncher: void <init>(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.mapred.TaskUmbilicalProtocol)>"
"Assigned container , <*>,  to , tId, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests: void add(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)>"
MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.,info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster$MRAppMasterShutdownHook: void run()>
Calling handler for JobFinishedEvent ,info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void logJobHistoryFinishedEvent()>
Executing with tokens:,info,"<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void initAndStartAppMaster(org.apache.hadoop.mapreduce.v2.app.MRAppMaster,org.apache.hadoop.mapred.JobConf,java.lang.String)>"
"Assigning container , allocated,  to fast fail map, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container)>
"Assigning container , allocated,  to reduce, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container)>
Cancelling commit,info,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: void cancelJobCommit()>
"JVM with ID : , jvmId,  asked for a task, ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: org.apache.hadoop.mapred.JvmTask getTask(org.apache.hadoop.mapred.JvmContext)>
"JVM with ID: , jvmId,  is invalid and will be killed., ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: org.apache.hadoop.mapred.JvmTask getTask(org.apache.hadoop.mapred.JvmContext)>
"JVM with ID: , jvmId,  asking for task before AM launch registered. Given null task, ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: org.apache.hadoop.mapred.JvmTask getTask(org.apache.hadoop.mapred.JvmContext)>
"JVM with ID: , jvmId,  given task: , <*>, ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: org.apache.hadoop.mapred.JvmTask getTask(org.apache.hadoop.mapred.JvmContext)>
"Finding containerReq for allocated container: , allocated, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container)>
"Replacing FAST_FAIL_MAP container , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container)>
"Found replacement: , toBeReplaced_, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container)>
"Replacing MAP container , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container)>
"Found replacement: , toBeReplaced_, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container)>
"Failed to render attempts page with task type : , <*>,  for job id : , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.webapp.AppController: void attempts()>
"Preempting , id, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests: void preemptReduce(int)>
"Done acknowledgement from , <*>, ",info,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: void done(org.apache.hadoop.mapred.TaskAttemptID)>
"mapreduce.cluster.local.dir for uber task: , <*>, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"CONTAINER_REMOTE_LAUNCH contains a map task (, attemptID, ), but should be finished with maps, ",error,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"CONTAINER_REMOTE_LAUNCH contains a reduce task (, attemptID, ), but not yet finished with maps, ",error,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
FSError from child,fatal,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"Exception running local (uberized) \'child\' : , <*>, ",warn,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"Exception cleaning up: , <*>, ",info,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"Error running local (uberized) \'child\' : , <*>, ",fatal,"<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map)>"
"Previous history file is at , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: org.apache.hadoop.fs.FSDataInputStream getPreviousJobHistoryStream(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>"
"Killing taskAttempt:, tid,  because it is running on unusable node:, <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void handleUpdatedNodes(org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse)>
"History file is at , <*>, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService: org.apache.hadoop.fs.FSDataInputStream getPreviousJobHistoryFileStream(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>"
"Moved tmp to done: , tmpPath,  to , <*>, ",info,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void moveTmpToDone(org.apache.hadoop.fs.Path)>
"Got an error parsing job-history file, ignoring incomplete events.",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void parsePreviousJobHistory()>
"Read from history task , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void parsePreviousJobHistory()>
"Read completed tasks from history , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void parsePreviousJobHistory()>
"KILLING , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container: void kill()>
"cleanup failed for container , <*>,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container: void kill()>
"Error communicating with RM: , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1: void run()>
ERROR IN CONTACTING RM. ,error,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1: void run()>
Allocated thread interrupted. Returning.,warn,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1: void run()>
"<*>, . AttemptId:, id, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void actOnUnusableNode(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.NodeState)>"
"Flushing , <*>, ",debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void flush()>
"Sending event , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition$TriggerScheduledFuture: void run()>
"Renaming map output file for task attempt , <*>,  from original location , <*>,  to destination , <*>, ",debug,"<org.apache.hadoop.mapred.LocalContainerLauncher: org.apache.hadoop.mapred.MapOutputFile renameMapOutputForReduce(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,org.apache.hadoop.mapred.MapOutputFile)>"
"Returning, interrupted : , <*>, ",error,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$2: void run()>
"Processing the event , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void run()>
Final Stats: ,log,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void serviceStop()>
Closing Writer,debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void closeWriter()>
"Ignoring killed event for successful reduce task attempt, <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$KilledAfterSuccessTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskAttemptStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>"
"Added priority=, priority, ",debug,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>"
"addResourceRequest: applicationId=, <*>,  priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>"
"Notify RMCommunicator isAMLastRetry: , isLastAMRetry, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void notifyIsLastAMRetry(boolean)>
"Notify JHEH isAMLastRetry: , isLastAMRetry, ",info,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void notifyIsLastAMRetry(boolean)>
"APPLICATION_ATTEMPT_ID: , <*>, ",debug,"<org.apache.hadoop.mapred.YarnChild: void configureTask(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task,org.apache.hadoop.security.Credentials,org.apache.hadoop.security.token.Token)>"
Shuffle secret missing from task credentials. Using job token secret as shuffle secret.,warn,"<org.apache.hadoop.mapred.YarnChild: void configureTask(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task,org.apache.hadoop.security.Credentials,org.apache.hadoop.security.token.Token)>"
Event from RM: shutting down Application Master,info,<org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator: void heartbeat()>
"ApplicationMaster is out of sync with ResourceManager, hence resync and send outstanding requests.",info,<org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator: void heartbeat()>
"Could not contact RM after , <*>,  milliseconds., ",error,<org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator: void heartbeat()>
"Uberizing job , <*>, : , <*>, m+, <*>, r tasks (, dataInputLength,  input bytes) will run sequentially on single node., ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
,info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void makeUberDecision(long)>
"Unexpected event for REDUCE task , <*>, ",error,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveKilledTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
Error starting MRAppMaster,fatal,<org.apache.hadoop.mapreduce.v2.app.MRAppMaster: void main(java.lang.String[])>
"Recovering task , <*>,  from prior app attempt, status was , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
"Missing successful attempt for task , <*>, , recovering as RUNNING, ",info,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)>"
Assigned to reduce,info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignToReduce(org.apache.hadoop.yarn.api.records.Container)>
"Kill task , <*>,  received from , <*>,  at , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler: org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskResponse killTask(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskRequest)>
"startJobs: parent=, <*>,  child=, <*>, ",debug,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
"Adding job token for , <*>,  to jobTokenSecretManager, ",info,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
Shuffle secret key missing from job credentials. Using job token secret as shuffle secret.,warn,<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
"Ping from , <*>, ",debug,<org.apache.hadoop.mapred.TaskAttemptListenerImpl: boolean ping(org.apache.hadoop.mapred.TaskAttemptID)>
"mapreduce.cluster.local.dir for child: , <*>, ",info,"<org.apache.hadoop.mapred.YarnChild: void configureLocalDirs(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JobConf)>"
could not create failure file.,error,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void handleJobCommit(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobCommitEvent)>
Could not commit job,error,<org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor: void handleJobCommit(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobCommitEvent)>
"Shutting down timer , <*>, ",debug,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void shutDownTimer()>
"Added , <*>,  to list of failed maps, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent)>
"Added attempt req to host , host_, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent)>
"Added attempt req to rack , host_, ",debug,<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent)>
"Not generating HistoryFinish event since start event not generated for task: , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptFailedTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>"
"Instantiated MRClientService at , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService: void serviceStart()>
Webapps failed to start. Ignoring for now:,error,<org.apache.hadoop.mapreduce.v2.app.client.MRClientService: void serviceStart()>
"Recalculating schedule, headroom=, headRoom_, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"Reduce slow start threshold not met. completedMapsForReduceSlowstart , completedMapsForReduceSlowstart, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
Reduce slow start threshold reached. Scheduling reduces.,info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"All maps assigned. Ramping up all remaining reduces:, numPendingReduces, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"completedMapPercent , completedMapsForReduceSlowstart#_,  totalResourceLimit:, <*>,  finalMapResourceLimit:, finalMapResourceLimit_,  finalReduceResourceLimit:, finalReduceResourceLimit_,  netScheduledMapResource:, <*>,  netScheduledReduceResource:, <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"Ramping up , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"Ramping down , <*>, ",info,"<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float)>"
"maxContainerCapability: , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void register()>
"queue: , <*>, ",info,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void register()>
Exception while registering,error,<org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator: void register()>
"Commit-pending state update from , <*>, ",info,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: void commitPending(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus)>"
"Task: , taskAttemptID,  - failed due to FSError: , message, ",fatal,"<org.apache.hadoop.mapred.TaskAttemptListenerImpl: void fsError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"mapreduce.cluster.local.dir for child : , <*>, ",debug,"<org.apache.hadoop.mapred.LocalJobRunner: void setupChildMapredLocalDirs(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JobConf)>"
Not creating job classloader since APP_CLASSPATH is not set.,warn,<org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.ClassLoader createJobClassLoader(org.apache.hadoop.conf.Configuration)>
Creating job classloader,info,<org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.ClassLoader createJobClassLoader(org.apache.hadoop.conf.Configuration)>
"APP_CLASSPATH=, <*>, ",debug,<org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.ClassLoader createJobClassLoader(org.apache.hadoop.conf.Configuration)>
"Task , taskid,  reportedNextRecordRange , range, ",info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void reportNextRecordRange(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.SortedRanges$Range)>"
"Setting classloader , <*>,  on the configuration and as the thread context classloader, ",info,"<org.apache.hadoop.mapreduce.v2.util.MRApps: void setClassLoader(java.lang.ClassLoader,org.apache.hadoop.conf.Configuration)>"
"Starting task: , $u, ",info,<org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable: void run()>
"Finishing task: , $u, ",info,<org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable: void run()>
"shuffleError: , message, from task: , taskId, ",fatal,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void shuffleError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"Connecting to MRHistoryServer at: , hsAddress, ",debug,"<org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)>"
"Fatal: , msg, from task: , taskId, ",fatal,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void fatalError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"Starting task: , $u, ",info,<org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable: void run()>
"Finishing task: , $u, ",info,<org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable: void run()>
Failed to createOutputCommitter,info,<org.apache.hadoop.mapred.LocalJobRunner$Job: void run()>
"Error cleaning up , <*>, : , <*>, ",warn,<org.apache.hadoop.mapred.LocalJobRunner$Job: void run()>
"Error cleaning up job:, <*>, ",info,<org.apache.hadoop.mapred.LocalJobRunner$Job: void run()>
"Error cleaning up , <*>, : , <*>, ",warn,<org.apache.hadoop.mapred.LocalJobRunner$Job: void run()>
"Error cleaning up , <*>, : , <*>, ",warn,<org.apache.hadoop.mapred.LocalJobRunner$Job: void run()>
"<*>, <*>,  conflicts with , <*>, <*>,  This will be an error in Hadoop ., ",warn,"<org.apache.hadoop.mapreduce.v2.util.MRApps: void parseDistributedCacheArtifacts(org.apache.hadoop.conf.Configuration,java.util.Map,org.apache.hadoop.yarn.api.records.LocalResourceType,java.net.URI[],long[],long[],boolean[])>"
"FSError: , message, from task: , taskId, ",fatal,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void fsError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)>"
"Default file system , <*>, , ",info,<org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: org.apache.hadoop.fs.FileContext getDefaultFileContext()>
"Unable to create default file context , <*>, , ",error,<org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: org.apache.hadoop.fs.FileContext getDefaultFileContext()>
Default file system is set solely by core-default.xml therefore -  ignoring,info,<org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: org.apache.hadoop.fs.FileContext getDefaultFileContext()>
"Unable to parse submit time from job history file , jhFileName,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Unable to parse finish time from job history file , jhFileName,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Unable to parse num maps from job history file , jhFileName,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Unable to parse num reduces from job history file , jhFileName,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Unable to parse start time from job history file , jhFileName,  : , <*>, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Parsing job history file with partial data encoded into name: , jhFileName, ",warn,<org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils: org.apache.hadoop.mapreduce.v2.jobhistory.JobIndexInfo getIndexInfo(java.lang.String)>
"Failed to delete symlink created by the local job runner: , symlink, ",warn,<org.apache.hadoop.mapred.LocalDistributedCacheManager: void close()>
"OutputCommitter set in config , <*>, ",info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: org.apache.hadoop.mapreduce.OutputCommitter createOutputCommitter(boolean,org.apache.hadoop.mapred.JobID,org.apache.hadoop.conf.Configuration)>"
"OutputCommitter is , <*>, ",info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: org.apache.hadoop.mapreduce.OutputCommitter createOutputCommitter(boolean,org.apache.hadoop.mapred.JobID,org.apache.hadoop.conf.Configuration)>"
Starting mapper thread pool executor.,debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createMapExecutor()>
"Max local threads: , <*>, ",debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createMapExecutor()>
"Map tasks to process: , <*>, ",debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createMapExecutor()>
"Looking for a token with service , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>"
"Token kind is , <*>,  and the token\'s service name is , <*>, ",debug,"<org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>"
"Waiting for , taskType,  tasks, ",info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void runTasks(java.util.List,java.util.concurrent.ExecutorService,java.lang.String)>"
"taskType,  task executor complete., ",info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: void runTasks(java.util.List,java.util.concurrent.ExecutorService,java.lang.String)>"
,info,"<org.apache.hadoop.mapred.LocalJobRunner$Job: boolean statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus)>"
Starting reduce thread pool executor.,debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createReduceExecutor()>
"Max local threads: , <*>, ",debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createReduceExecutor()>
"Reduce tasks to process: , <*>, ",debug,<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createReduceExecutor()>
"Task status: \, status, \ truncated to max limit (, <*>,  characters), ",warn,"<org.apache.hadoop.mapred.Task: java.lang.String normalizeStatus(java.lang.String,org.apache.hadoop.conf.Configuration)>"
"Task \', <*>, \' done., ",info,<org.apache.hadoop.mapred.Task: void sendDone(org.apache.hadoop.mapred.TaskUmbilicalProtocol)>
"Failure signalling completion: , <*>, ",warn,<org.apache.hadoop.mapred.Task: void sendDone(org.apache.hadoop.mapred.TaskUmbilicalProtocol)>
"Merging data from , from,  to , to, ",debug,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void mergePaths(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)>"
mapreduce.client.completion.pollinterval has been set to an invalid value; replacing with ,warn,<org.apache.hadoop.mapreduce.Job: int getCompletionPollInterval(org.apache.hadoop.conf.Configuration)>
,setConf,<org.apache.hadoop.mapreduce.tools.CLI: int run(java.lang.String[])>
"sending reportNextRecordRange , <*>, ",debug,"<org.apache.hadoop.mapred.Task: void reportNextRecordRange(org.apache.hadoop.mapred.TaskUmbilicalProtocol,long)>"
"mapreduce.task.io.sort.mb: , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void init(org.apache.hadoop.mapred.MapOutputCollector$Context)>
"soft limit at , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void init(org.apache.hadoop.mapred.MapOutputCollector$Context)>
"bufstart = , <*>, ; bufvoid = , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void init(org.apache.hadoop.mapred.MapOutputCollector$Context)>
"kvstart = , <*>, ; length = , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void init(org.apache.hadoop.mapred.MapOutputCollector$Context)>
"Sending AUTHENTICATION_REQ, digest=, digest, , challenge=, challenge, ",debug,"<org.apache.hadoop.mapred.pipes.BinaryProtocol: void authenticate(java.lang.String,java.lang.String)>"
"adding the following namenodes\' delegation tokens:, <*>, ",debug,"<org.apache.hadoop.mapreduce.JobSubmitter: void populateTokenCache(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>"
"Ignoring exception during close for , c, ",info,<org.apache.hadoop.mapred.MapTask: void closeQuietly(org.apache.hadoop.mapreduce.RecordReader)>
"Created a new BackupStore with a memory of , maxSize_, ",info,"<org.apache.hadoop.mapred.BackupStore: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)>"
"Notification error , <*>, , ",error,"<org.apache.hadoop.mapred.JobEndNotifier: void localRunnerNotification(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobStatus)>"
"Notification error , <*>, , ",error,"<org.apache.hadoop.mapred.JobEndNotifier: void localRunnerNotification(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobStatus)>"
"Notification retry error , <*>, , ",error,"<org.apache.hadoop.mapred.JobEndNotifier: void localRunnerNotification(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobStatus)>"
"Max block location exceeded for split: , split,  splitsize: , <*>,  maxsize: , <*>, ",warn,"<org.apache.hadoop.mapreduce.split.JobSplitWriter: org.apache.hadoop.mapreduce.split.JobSplit$SplitMetaInfo[] writeOldSplits(org.apache.hadoop.mapred.InputSplit[],org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.conf.Configuration)>"
"Failure cleaning up: , <*>, ",warn,<org.apache.hadoop.mapred.Task: void discardOutput(org.apache.hadoop.mapred.TaskAttemptContext)>
"host,  freed by , <*>,  in , <*>, ms, ",info,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void freeHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
"(RESET) equator , e,  kv , <*>, (, <*>, ),  kvi , <*>, (, <*>, ), ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void resetSpill()>
Waiting for authentication response,debug,<org.apache.hadoop.mapred.pipes.Application: void waitForAuthentication()>
Output Path is null in commitJob(),warn,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
"Created a new mem block of , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore$MemoryCache: void reinitialize(boolean)>
"IV written to Stream , <*>, , ",debug,"<org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataOutputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream)>"
"counterName,  is not a recognized counter., ",warn,<org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup: org.apache.hadoop.mapreduce.Counter findCounter(java.lang.String)>
"Aborting because of , <*>, ",info,<org.apache.hadoop.mapred.pipes.Application: void abort(java.lang.Throwable)>
"Map ID, mapId,  not found in queue!!, ",warn,<org.apache.hadoop.mapred.IndexCache: void removeMap(java.lang.String)>
"Map ID , mapId,  not found in cache, ",info,<org.apache.hadoop.mapred.IndexCache: void removeMap(java.lang.String)>
"Read , <*>,  bytes from map-output for , <*>, ",info,"<org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput: void shuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.InputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter)>"
"MergerManager: memoryLimit=, <*>, , , maxSingleShuffleLimit=, <*>, , , mergeThreshold=, <*>, , , ioSortFactor=, <*>, , , memToMemMergeOutputsThreshold=, <*>, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: void <init>(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.LocalDirAllocator,org.apache.hadoop.mapred.Reporter,org.apache.hadoop.io.compress.CompressionCodec,java.lang.Class,org.apache.hadoop.mapred.Task$CombineOutputCollector,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapreduce.task.reduce.ExceptionReporter,org.apache.hadoop.util.Progress,org.apache.hadoop.mapred.MapOutputFile)>"
"task-diagnostic-info for task , <*>,  : , info, ",info,<org.apache.hadoop.mapred.TaskStatus: void setDiagnosticInfo(java.lang.String)>
"task-diagnostic-info for task , <*>,  : , <*>, ",info,<org.apache.hadoop.mapred.TaskStatus: void setDiagnosticInfo(java.lang.String)>
"Reserve(int, InputStream) not supported by BackupRamManager",warn,"<org.apache.hadoop.mapred.BackupStore$BackupRamManager: boolean reserve(int,java.io.InputStream)>"
STDOUT,<init>,<org.apache.hadoop.mapred.TaskLog$LogName: void <clinit>()>
STDERR,<init>,<org.apache.hadoop.mapred.TaskLog$LogName: void <clinit>()>
SYSLOG,<init>,<org.apache.hadoop.mapred.TaskLog$LogName: void <clinit>()>
PROFILE,<init>,<org.apache.hadoop.mapred.TaskLog$LogName: void <clinit>()>
DEBUGOUT,<init>,<org.apache.hadoop.mapred.TaskLog$LogName: void <clinit>()>
"Reporting fetch failure for , mapId,  to jobtracker., ",info,"<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void checkAndInformJobTracker(int,org.apache.hadoop.mapreduce.TaskAttemptID,boolean,boolean,boolean)>"
"Configuring job , <*>,  with , <*>,  as the submit dir, ",debug,"<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>"
"Creating splits at , <*>, ",debug,"<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>"
"number of splits:, <*>, ",info,"<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>"
"Cleaning up the staging area , <*>, ",info,"<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>"
"Cleaning up the staging area , <*>, ",info,"<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>"
closing connection,debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol: void close()>
Failed to contact the tasktracker,fatal,"<org.apache.hadoop.mapred.Task: void reportFatalError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.Throwable,java.lang.String)>"
"ID: , <*>,  WRITE TO MEM, ",debug,"<org.apache.hadoop.mapred.BackupStore$MemoryCache: void write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer)>"
Unable to refresh queues because queue-hierarchy changed. Retaining existing configuration. ,warn,"<org.apache.hadoop.mapred.QueueManager: void refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher)>"
,error,"<org.apache.hadoop.mapred.QueueManager: void refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher)>"
Queue configuration is refreshed successfully.,info,"<org.apache.hadoop.mapred.QueueManager: void refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher)>"
"Added Memory Segment to List. List Size is , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore$MemoryCache: void createInMemorySegment()>
Counter name MAP_INPUT_BYTES is deprecated. Use FileInputFormatCounters as group name and  BYTES_READ as counter name instead,warn,"<org.apache.hadoop.mapred.Counters: org.apache.hadoop.mapred.Counters$Counter findCounter(java.lang.String,java.lang.String)>"
"Could not find method setSessionTimeZone in , <*>, ",error,"<org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader: void setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection)>"
"Time zone has been set to , <*>, ",info,"<org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader: void setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection)>"
"Time zone , <*>,  could not be set on Oracle database., ",warn,"<org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader: void setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection)>"
Setting default time zone: GMT,warn,"<org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader: void setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection)>"
Could not set time zone for oracle connection,error,"<org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader: void setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection)>"
"MergeQ: adding: , file, ",debug,"<org.apache.hadoop.mapred.Merger$MergeQueue: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.RawComparator,org.apache.hadoop.util.Progressable,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapreduce.TaskType)>"
"Skipped line of size , newSize_,  at pos , <*>, ",info,"<org.apache.hadoop.mapred.LineRecordReader: boolean next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text)>"
Invalid map id ,warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"header: , <*>, , len: , compressedLength_, , decomp len: , decompressedLength_, ",debug,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"fetcher#, <*>,  - MergeManager returned status WAIT ..., ",info,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"fetcher#, <*>,  about to shuffle output of map , <*>,  decomp: , decompressedLength_,  len: , compressedLength_,  to , <*>, ",info,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"Failed to shuffle for fetcher#, <*>, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"fetcher#, <*>,  failed to read map header, mapId_,  decomp: , decompressedLength_, , , compressedLength_, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
"Failed to shuffle output of , mapId_,  from , <*>, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>"
" current name , <*>,  not equal to , <*>, ",info,<org.apache.hadoop.mapred.Queue: boolean isHierarchySameAs(org.apache.hadoop.mapred.Queue)>
"newState,  has added children in refresh , ",info,<org.apache.hadoop.mapred.Queue: boolean isHierarchySameAs(org.apache.hadoop.mapred.Queue)>
"In the current state, queue , <*>,  has , <*>,  but the new state has none!, ",fatal,<org.apache.hadoop.mapred.Queue: boolean isHierarchySameAs(org.apache.hadoop.mapred.Queue)>
"Number of children for queue , <*>,  in newState is , <*>,  which is not equal to , <*>,  in the current state., ",fatal,<org.apache.hadoop.mapred.Queue: boolean isHierarchySameAs(org.apache.hadoop.mapred.Queue)>
" Queue , <*>,  not equal to , <*>, ",info,<org.apache.hadoop.mapred.Queue: boolean isHierarchySameAs(org.apache.hadoop.mapred.Queue)>
"Got dt for , <*>, ; , token, ",info,"<org.apache.hadoop.mapreduce.security.TokenCache: void obtainTokensForNamenodesInternal(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)>"
"ID: , <*>,  WRITE TO DISK, ",debug,"<org.apache.hadoop.mapred.BackupStore$FileCache: void write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer)>"
"IV read from Stream , <*>, , ",debug,"<org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataInputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataInputStream)>"
Dropping a segment,debug,<org.apache.hadoop.mapred.BackupStore: void mark()>
"Setting the FirsSegmentOffset to , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore: void mark()>
"map , mapId,  done , <*>, ",debug,"<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void copySucceeded(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MapHost,long,long,long,org.apache.hadoop.mapreduce.task.reduce.MapOutput)>"
Found UTF- BOM and skipped it,info,<org.apache.hadoop.mapred.LineRecordReader: int skipUtfByteOrderMark(org.apache.hadoop.io.Text)>
"Running job: , <*>, ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
"Job , <*>,  running in uber mode : , <*>, ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
" map , <*>,  reduce , <*>, ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
"Job , <*>,  completed successfully, ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
"Job , <*>,  failed with state , <*>,  due to: , <*>, ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
", ",info,<org.apache.hadoop.mapreduce.Job: boolean monitorAndPrintJob()>
"Permissions on staging directory , <*>,  are , incorrect: , <*>, . Fixing permissions , to correct value , <*>, ",info,"<org.apache.hadoop.mapreduce.JobSubmissionFiles: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.mapreduce.Cluster,org.apache.hadoop.conf.Configuration)>"
"assigned , includedMaps_,  of , <*>,  to , host,  to , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: java.util.List getMapsForHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
"Saved output of task \', <*>, \' to , <*>, ",info,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>"
"No Output found for , <*>, ",warn,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>"
Output Path is null in commitTask(),warn,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>"
"<*>,  Thread started: , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
"<*>, : , Got , <*>,  new map-outputs, ",info,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
GetMapEventsThread about to sleep for ,debug,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
EventFetcher is interrupted.. Returning,info,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
Exception in getting events,info,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
"closeInMemoryMergedFile -> size: , <*>, , inMemoryMergedMapOutputs.size() -> , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: void closeInMemoryMergedFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput)>
No ondisk files to merge...,info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger: void merge(java.util.List)>
"OnDiskMerger: We have  , <*>,  map outputs on disk. Triggering merge..., ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger: void merge(java.util.List)>
"<*>,  Finished merging , <*>,  map output files on disk of total-size , approxOutputSize_, .,  Local output file is , <*>,  of size , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger: void merge(java.util.List)>
"<*>, : Starting merge with , <*>,  segments, while ignoring , <*>,  segments, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeThread: void startMerge(java.util.Set)>
Could not find output size ,warn,<org.apache.hadoop.mapred.Task: long calculateOutputSize()>
"Read , <*>,  bytes from map-output for , <*>, ",info,"<org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput: void shuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.InputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter)>"
mkdirs failed. Ignoring.,debug,<org.apache.hadoop.mapred.TaskLog: java.io.File getUserLogDir()>
"Task:, <*>,  is done.,  And is in the process of committing, ",info,"<org.apache.hadoop.mapred.Task: void done(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
"Failure sending commit pending: , <*>, ",warn,"<org.apache.hadoop.mapred.Task: void done(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
USER,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
OPERATION,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
TARGET,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
RESULT,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
IP,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
PERMISSIONS,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
DESCRIPTION,<init>,<org.apache.hadoop.mapred.AuditLogger$Keys: void <clinit>()>
"numReduceTasks: , <*>, ",info,"<org.apache.hadoop.mapred.MapTask: void runOldMapper(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.split.JobSplit$TaskSplitIndex,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
"The url to track the job: , <*>, ",info,<org.apache.hadoop.mapreduce.Job: void submit()>
"Trying ClientProtocolProvider : , <*>, ",debug,"<org.apache.hadoop.mapreduce.Cluster: void initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>"
"Picked , <*>,  as the ClientProtocolProvider, ",debug,"<org.apache.hadoop.mapreduce.Cluster: void initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>"
"Cannot pick , <*>,  as the ClientProtocolProvider - returned null protocol, ",debug,"<org.apache.hadoop.mapreduce.Cluster: void initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>"
"Failed to use , <*>,  due to error: , <*>, ",info,"<org.apache.hadoop.mapreduce.Cluster: void initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>"
"Reset - First segment offset is , <*>,  Segment List Size is , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore: void reset()>
"seed: , <*>, ",debug,"<org.apache.hadoop.mapred.lib.InputSampler$RandomSampler: java.lang.Object[] getSample(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.mapred.JobConf)>"
"Time taken to get FileStatuses: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
"Total input paths to process : , <*>, ",info,<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
"closeInMemoryFile -> map-output of size: , <*>, , inMemoryMapOutputs.size() -> , <*>, , commitMemory -> , <*>, , usedMemory ->, <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: void closeInMemoryFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput)>
"Starting inMemoryMerger\'s merge since commitMemory=, <*>,  > mergeThreshold=, <*>, . Current usedMemory=, <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: void closeInMemoryFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput)>
Spilling map output,info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void startSpill()>
"bufstart = , <*>, ; bufend = , <*>, ; bufvoid = , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void startSpill()>
"kvstart = , <*>, (, <*>, ); kvend = , <*>, (, <*>, ); length = , <*>, /, <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void startSpill()>
"Initiating in-memory merge with , <*>,  segments..., ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger: void merge(java.util.List)>
"<*>,  Merge of the , <*>,  files in-memory complete.,  Local file is , <*>,  of size , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger: void merge(java.util.List)>
"SQLException closing resultset: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"SQLException closing statement: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"SQLException committing split transaction: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"SQLException closing resultset: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"SQLException closing statement: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"SQLException committing split transaction: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"IndexCache HIT: MapId , mapId,  found, ",debug,"<org.apache.hadoop.mapred.IndexCache: org.apache.hadoop.mapred.IndexRecord getIndexInformation(java.lang.String,int,org.apache.hadoop.fs.Path,java.lang.String)>"
"Trying map output collector class: , <*>, ",debug,"<org.apache.hadoop.mapred.MapTask: org.apache.hadoop.mapred.MapOutputCollector createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter)>"
"Map output collector class = , <*>, ",info,"<org.apache.hadoop.mapred.MapTask: org.apache.hadoop.mapred.MapOutputCollector createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter)>"
"<*>,  (, remainingCollectors_,  more collector(s) to try), ",warn,"<org.apache.hadoop.mapred.MapTask: org.apache.hadoop.mapred.MapOutputCollector createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter)>"
"Trying to delete , <*>, ",debug,<org.apache.hadoop.mapred.CleanupQueue: boolean deletePath(org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext)>
"Finished spill , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void sortAndSpill()>
"DEBUG: Terminated node allocation with : CompletedNodes: , <*>, , size left: , totalLength_, ",info,"<org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat: void createSplits(java.util.Map,java.util.Map,java.util.Map,long,long,long,long,java.util.List)>"
"Got interrupted while joining , <*>, ",warn,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void shutDown()>
Further groups got skipped.,warn,<org.apache.hadoop.mapred.ReduceTask$SkippingReduceValuesIterator: void mayBeSkip()>
"previousRange , previousRange, ",debug,<org.apache.hadoop.mapred.SortedRanges: void add(org.apache.hadoop.mapred.SortedRanges$Range)>
"nextRange , nextRange,    startIndex:, startIndex_,   endIndex:, endIndex_, ",debug,<org.apache.hadoop.mapred.SortedRanges: void add(org.apache.hadoop.mapred.SortedRanges$Range)>
Starting flush of map output,info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void flush()>
Spilling map output,info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void flush()>
"bufstart = , <*>, ; bufend = , <*>, ; bufvoid = , <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void flush()>
"kvstart = , <*>, (, <*>, ); kvend = , <*>, (, <*>, ); length = , <*>, /, <*>, ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void flush()>
,info,"<org.apache.hadoop.mapreduce.Job: void printTaskEvents(org.apache.hadoop.mapreduce.TaskCompletionEvent[],org.apache.hadoop.mapreduce.Job$TaskStatusFilter,boolean,org.apache.hadoop.conf.Configuration$IntegerRanges,org.apache.hadoop.conf.Configuration$IntegerRanges)>"
,info,"<org.apache.hadoop.mapreduce.Job: void printTaskEvents(org.apache.hadoop.mapreduce.TaskCompletionEvent[],org.apache.hadoop.mapreduce.Job$TaskStatusFilter,boolean,org.apache.hadoop.conf.Configuration$IntegerRanges,org.apache.hadoop.conf.Configuration$IntegerRanges)>"
,info,"<org.apache.hadoop.mapreduce.Job: void printTaskEvents(org.apache.hadoop.mapreduce.TaskCompletionEvent[],org.apache.hadoop.mapreduce.Job$TaskStatusFilter,boolean,org.apache.hadoop.conf.Configuration$IntegerRanges,org.apache.hadoop.conf.Configuration$IntegerRanges)>"
,info,"<org.apache.hadoop.mapreduce.Job: void printTaskEvents(org.apache.hadoop.mapreduce.TaskCompletionEvent[],org.apache.hadoop.mapreduce.Job$TaskStatusFilter,boolean,org.apache.hadoop.conf.Configuration$IntegerRanges,org.apache.hadoop.conf.Configuration$IntegerRanges)>"
Unable to determine FileDescriptor,info,<org.apache.hadoop.mapred.IFileInputStream: java.io.FileDescriptor getFileDescriptorIfAvail(java.io.InputStream)>
"<*>,  Instead use , mapreduce.map.memory.mb,  and , mapreduce.reduce.memory.mb, ",warn,<org.apache.hadoop.mapred.JobConf: void checkAndWarnDeprecation()>
"mapId, : Shuffling to disk since , requestedSize,  is greater than maxSingleShuffleLimit (, <*>, ), ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapreduce.task.reduce.MapOutput reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int)>"
"mapId, : Stalling shuffle since usedMemory (, <*>, ) is greater than memoryLimit (, <*>, ).,  CommitMemory is (, <*>, ), ",debug,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapreduce.task.reduce.MapOutput reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int)>"
"mapId, : Proceeding with shuffle since usedMemory (, <*>, ) is lesser than memoryLimit (, <*>, )., CommitMemory is (, <*>, ), ",debug,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapreduce.task.reduce.MapOutput reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int)>"
"Initiating Memory-to-Memory merge with , <*>,  segments of total-size: , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$IntermediateMemoryToMemoryMerger: void merge(java.util.List)>
"<*>,  Memory-to-Memory merge of the , <*>,  files in-memory complete., ",info,<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$IntermediateMemoryToMemoryMerger: void merge(java.util.List)>
"Handling uplink command , <*>, ",debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread: void run()>
"Message , <*>,  received before authentication is , complete. Ignoring, ",warn,<org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread: void run()>
Pipe child done,debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread: void run()>
"MapId=, <*>,  Reducer=, i__, Spill =, i__, (, <*>, ,, <*>, , , <*>, ), ",debug,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void mergeParts()>
"Mkdirs failed to create , <*>, ",error,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void setupJob(org.apache.hadoop.mapreduce.JobContext)>
Output Path is null in setupJob(),warn,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void setupJob(org.apache.hadoop.mapreduce.JobContext)>
"cleanUpPartialOutputForTask: removing everything belonging to , <*>,  in: , <*>, ",info,<org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter: void cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
"Record too large for in-memory buffer: , <*>, ",info,"<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void collect(java.lang.Object,java.lang.Object,int)>"
#NAME?,warn,<org.apache.hadoop.mapred.pipes.Submitter: int run(java.lang.String[])>
"Error : , <*>, ",info,<org.apache.hadoop.mapred.pipes.Submitter: int run(java.lang.String[])>
"Skipped line of size , newSize_,  at pos , <*>, ",info,<org.apache.hadoop.mapreduce.lib.input.LineRecordReader: boolean nextKeyValue()>
Generating splits for a floating-point index column. Due to the,warn,"<org.apache.hadoop.mapreduce.lib.db.FloatSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
"imprecise representation of floating-point values in Java, this",warn,"<org.apache.hadoop.mapreduce.lib.db.FloatSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
may result in an incomplete import.,warn,"<org.apache.hadoop.mapreduce.lib.db.FloatSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
You are strongly encouraged to choose an integral split column.,warn,"<org.apache.hadoop.mapreduce.lib.db.FloatSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
"Error executing shell command , <*>, <*>, ",warn,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isProcessGroupAlive(java.lang.String)>
Sent close command,debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol: void endOfInput()>
Generating splits for a textual index column.,warn,"<org.apache.hadoop.mapreduce.lib.db.TextSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
"If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.",warn,"<org.apache.hadoop.mapreduce.lib.db.TextSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
You are strongly encouraged to choose an integral split column.,warn,"<org.apache.hadoop.mapreduce.lib.db.TextSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
Encountered a NULL date in the split column. Splits may be poorly balanced.,warn,"<org.apache.hadoop.mapreduce.lib.db.DateSplitter: long resultSetColToLong(java.sql.ResultSet,int,int)>"
"Trying to set illegal startTime for task : , <*>, .Stack trace is : , <*>, ",error,<org.apache.hadoop.mapred.TaskStatus: void setStartTime(long)>
The API getMaxPhysicalMemoryForTask() is deprecated. Refer to the APIs getMemoryForMapTask() and getMemoryForReduceTask() for details.,warn,<org.apache.hadoop.mapred.JobConf: long getMaxPhysicalMemoryForTask()>
"Got interrupt while joining , <*>, ",warn,<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void shutDown()>
"Max block location exceeded for split: , split,  splitsize: , <*>,  maxsize: , <*>, ",warn,"<org.apache.hadoop.mapreduce.split.JobSplitWriter: org.apache.hadoop.mapreduce.split.JobSplit$SplitMetaInfo[] writeNewSplits(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.InputSplit[],org.apache.hadoop.fs.FSDataOutputStream)>"
"Skipping index , <*>, -, <*>, ",warn,<org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator: void skipIfInRange()>
\tFinishTime\tHostName\tError\tTaskLogs,append,<org.apache.hadoop.mapreduce.jobhistory.HistoryViewer: void printAllTaskAttempts(org.apache.hadoop.mapreduce.TaskType)>
"Counters: , <*>, ",info,<org.apache.hadoop.mapred.Counters: void log(org.apache.commons.logging.Log)>
"  , <*>, ",info,<org.apache.hadoop.mapred.Counters: void log(org.apache.commons.logging.Log)>
"    , <*>, =, <*>, ",info,<org.apache.hadoop.mapred.Counters: void log(org.apache.commons.logging.Log)>
"Assigning , host_,  with , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: org.apache.hadoop.mapreduce.task.reduce.MapHost getHost()>
"Trying to recover task from , <*>,  into , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
"Saved output of , <*>,  to , <*>, ",info,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
"<*>,  had no output to recover., ",warn,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
Output Path is null in recoverTask(),warn,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
Output Path is null in cleanupJob(),warn,<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void cleanupJob(org.apache.hadoop.mapreduce.JobContext)>
"Parent died.  Exiting , <*>, ",warn,<org.apache.hadoop.mapred.Task: void statusUpdate(org.apache.hadoop.mapred.TaskUmbilicalProtocol)>
"Failure sending status update: , <*>, ",warn,<org.apache.hadoop.mapred.Task: void statusUpdate(org.apache.hadoop.mapred.TaskUmbilicalProtocol)>
"Queue , queueName,  is not present, ",info,"<org.apache.hadoop.mapred.QueueManager: boolean hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)>"
"Cannot submit job to parent queue , <*>, ",info,"<org.apache.hadoop.mapred.QueueManager: boolean hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)>"
"Checking access for the acl , <*>,  for user , <*>, ",debug,"<org.apache.hadoop.mapred.QueueManager: boolean hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)>"
Could not obtain compressor from CodecPool,warn,"<org.apache.hadoop.mapred.IFile$Writer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.mapred.Counters$Counter,boolean)>"
"fetcher#, <*>,  - MergeManager returned Status.WAIT ..., ",info,<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: boolean copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)>
"localfetcher#, <*>,  about to shuffle output of map , <*>,  decomp: , decompressedLength,  len: , compressedLength,  to , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: boolean copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)>
"IOException closing inputstream from map output: , <*>, ",warn,<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: boolean copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)>
"IOException closing inputstream from map output: , <*>, ",warn,<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: boolean copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)>
Thread sleep is interrupted.,warn,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sigKillInCurrentThread(java.lang.String,boolean,long)>"
"TaskInfo is null for TaskAttemptUnsuccessfulCompletionEvent taskId:  , <*>, ",warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser: void handleTaskAttemptFailedEvent(org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent)>
"AttemptInfo is null for TaskAttemptUnsuccessfulCompletionEvent taskAttemptId:  , <*>, ",warn,<org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser: void handleTaskAttemptFailedEvent(org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent)>
"Could not find $CONDITIONS token in query: , query, ; splits may not partition data., ",warn,"<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: void setBoundingQuery(org.apache.hadoop.conf.Configuration,java.lang.String)>"
"Exception in closing , c, ",info,"<org.apache.hadoop.mapred.ReduceTask: void closeQuietly(org.apache.hadoop.mapred.RecordWriter,org.apache.hadoop.mapred.Reporter)>"
"<*>,  invalid lengths in map output header: id: , mapId,  len: , compressedLength, , decomp len: , decompressedLength, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: boolean verifySanity(long,long,int,java.util.Set,org.apache.hadoop.mapreduce.TaskAttemptID)>"
"<*>,  data for the wrong reduce map: , mapId,  len: , compressedLength,  decomp len: , decompressedLength,  for reduce , forReduce, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: boolean verifySanity(long,long,int,java.util.Set,org.apache.hadoop.mapreduce.TaskAttemptID)>"
"Invalid map-output! Received output for , mapId, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: boolean verifySanity(long,long,int,java.util.Set,org.apache.hadoop.mapreduce.TaskAttemptID)>"
"Created file: , <*>, ",info,<org.apache.hadoop.mapred.BackupStore$FileCache: org.apache.hadoop.mapred.IFile$Writer createSpillFile()>
"Error executing shell command , <*>, <*>, ",warn,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isAlive(java.lang.String)>
"Exception, <*>, ",info,<org.apache.hadoop.mapred.jobcontrol.Job: void setJobConf(org.apache.hadoop.mapred.JobConf)>
"previousRange , previousRange, ",debug,<org.apache.hadoop.mapred.SortedRanges: void remove(org.apache.hadoop.mapred.SortedRanges$Range)>
"removed previousRange , previousRange, ",debug,<org.apache.hadoop.mapred.SortedRanges: void remove(org.apache.hadoop.mapred.SortedRanges$Range)>
"nextRange , nextRange,    startIndex:, <*>,   endIndex:, <*>, ",debug,<org.apache.hadoop.mapred.SortedRanges: void remove(org.apache.hadoop.mapred.SortedRanges$Range)>
"IV read from , <*>, , ",debug,"<org.apache.hadoop.mapreduce.CryptoUtils: java.io.InputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,java.io.InputStream,long)>"
"Trying to set finish time for task , <*>,  when no start time is set, stackTrace is : , <*>, ",error,<org.apache.hadoop.mapred.TaskStatus: void setFinishTime(long)>
Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.,warn,"<org.apache.hadoop.mapreduce.JobResourceUploader: void uploadFiles(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>"
"default FileSystem: , <*>, ",debug,"<org.apache.hadoop.mapreduce.JobResourceUploader: void uploadFiles(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>"
No job jar file set.  User classes may not be found. See Job or Job#setJar(String).,warn,"<org.apache.hadoop.mapreduce.JobResourceUploader: void uploadFiles(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>"
The API setMaxPhysicalMemoryForTask() is deprecated. The value set is ignored. Refer to  setMemoryForMapTask() and setMemoryForReduceTask() for details.,warn,<org.apache.hadoop.mapred.JobConf: void setMaxPhysicalMemoryForTask(long)>
"Submitting tokens for job: , jobId, ",info,"<org.apache.hadoop.mapreduce.JobSubmitter: void printTokens(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.security.Credentials)>"
"Failure asking whether task can commit: , <*>, ",warn,"<org.apache.hadoop.mapred.Task: void commit(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapreduce.OutputCommitter)>"
"Task , <*>,  is allowed to commit now, ",info,"<org.apache.hadoop.mapred.Task: void commit(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapreduce.OutputCommitter)>"
"Failure committing: , <*>, ",warn,"<org.apache.hadoop.mapred.Task: void commit(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapreduce.OutputCommitter)>"
"Connection retry failed with , attempts_,  attempts in , retryTimeInSeconds,  seconds, ",error,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void connect(java.net.URLConnection,int)>"
Sleep in connection retry get interrupted.,warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void connect(java.net.URLConnection,int)>"
using new api for output committer,debug,"<org.apache.hadoop.mapred.Task: void initialize(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobID,org.apache.hadoop.mapred.Reporter,boolean)>"
" Using ResourceCalculatorProcessTree : , <*>, ",info,"<org.apache.hadoop.mapred.Task: void initialize(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobID,org.apache.hadoop.mapred.Reporter,boolean)>"
Authentication succeeded,debug,"<org.apache.hadoop.mapred.pipes.Application: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter,java.lang.Class,java.lang.Class)>"
"Got , <*>,  map completion events from , <*>, ",debug,<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: int getMapCompletionEvents()>
"Shuffle failed : local error on this node: , <*>, ",error,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void reportLocalError(java.io.IOException)>
Shuffle failed : local error on this node,error,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void reportLocalError(java.io.IOException)>
"AllQueues : , <*>, ; LeafQueues : , <*>, ",info,<org.apache.hadoop.mapred.QueueManager: void initialize(org.apache.hadoop.mapred.QueueConfigurationParser)>
"counterName,  is not a recognized counter., ",warn,"<org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup: org.apache.hadoop.mapreduce.Counter findCounter(java.lang.String,boolean)>"
"added , $u, ",debug,"<org.apache.hadoop.mapred.SortedRanges: void add(long,long)>"
"Configuring jobConf , <*>,  to use , <*>,  threads, ",debug,<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void configure(org.apache.hadoop.mapred.JobConf)>
Set BigDecimal splitSize to MIN_INCREMENT,warn,"<org.apache.hadoop.mapreduce.lib.db.BigDecimalSplitter: java.util.List split(java.math.BigDecimal,java.math.BigDecimal,java.math.BigDecimal)>"
"Shuffle output from , <*>,  failed, retry it., ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void checkTimeoutOrRetry(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.IOException)>"
"Timeout for copying MapOutput with retry on host , host, after , <*>,  milliseconds., ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void checkTimeoutOrRetry(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.IOException)>"
Sent abort command,debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol: void abort()>
"Ignoring exception during close for , c, ",info,"<org.apache.hadoop.mapred.MapTask: void closeQuietly(org.apache.hadoop.mapreduce.RecordWriter,org.apache.hadoop.mapreduce.Mapper$Context)>"
Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead,warn,<org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner: void setConf(org.apache.hadoop.conf.Configuration)>
mapreduce.client.progressmonitor.pollinterval has been set to an invalid value;  replacing with ,warn,<org.apache.hadoop.mapreduce.Job: int getProgressPollInterval(org.apache.hadoop.conf.Configuration)>
"finalMerge called with , <*>,  in-memory map-outputs and , <*>,  on-disk map-outputs, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"Merged , <*>,  segments, , <*>,  bytes to disk to satisfy , reduce memory limit, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"Keeping , <*>,  segments, , <*>,  bytes in memory for , intermediate, on-disk merge, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"Disk file: , file,  Length is , <*>, ",debug,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"Merging , <*>,  files, , numMemDiskSegments_,  bytes from disk, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"Merging , <*>,  segments, , <*>,  bytes from memory into reduce, ",info,"<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>"
"<*>,  started., ",debug,<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
"CleanupThread:Unable to delete path , <*>, ",warn,<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
"DELETED , <*>, ",debug,<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
"Interrupted deletion of , <*>, ",warn,<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
"Error deleting path , <*>, : , <*>, ",warn,<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
Runnning cleanup for the task,info,<org.apache.hadoop.mapred.Task: void taskCleanup(org.apache.hadoop.mapred.TaskUmbilicalProtocol)>
getMaxVirtualMemoryForTask() is deprecated. Instead use getMemoryForMapTask() and getMemoryForReduceTask(),warn,<org.apache.hadoop.mapred.JobConf: long getMaxVirtualMemoryForTask()>
"Task: Loaded jobTokenFile from: , <*>, ; num of sec keys  = , <*>,  Number of tokens , <*>, ",debug,"<org.apache.hadoop.mapreduce.security.TokenCache: org.apache.hadoop.security.Credentials loadTokens(java.lang.String,org.apache.hadoop.mapred.JobConf)>"
"LocalFetcher , <*>,  going to fetch: , map, ",debug,<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: void doCopy(java.util.Set)>
"Error while tyring to clean up , <*>, ",error,<org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl: void failAllJobs(java.lang.Throwable)>
"Error while tyring to clean up , <*>, ",error,<org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl: void failAllJobs(java.lang.Throwable)>
"Group , groupName,  is deprecated. Use , newGroupName_,  instead, ",warn,<org.apache.hadoop.mapreduce.counters.AbstractCounters: org.apache.hadoop.mapreduce.counters.CounterGroupBase getGroup(java.lang.String)>
"Failed to connect to host: , url, after , <*>,  milliseconds., ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void openConnectionWithRetry(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Set,java.net.URL)>"
"Sending signal to all members of process group , pid, : , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Signaling process , pid,  with , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Error executing shell command , <*>, ",warn,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Sending signal to all members of process group , pid, : , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Signaling process , pid,  with , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Sending signal to all members of process group , pid, : , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"Signaling process , pid,  with , signalName, . Exit code , <*>, ",info,"<org.apache.hadoop.mapreduce.util.ProcessTree: void sendSignal(java.lang.String,int,java.lang.String)>"
"IndexCache created with max memory = , <*>, ",info,<org.apache.hadoop.mapred.IndexCache: void <init>(org.apache.hadoop.mapred.JobConf)>
"Ignoring exception during close for , c, ",info,<org.apache.hadoop.mapred.MapTask: void closeQuietly(org.apache.hadoop.mapred.MapOutputCollector)>
"Merging , <*>,  sorted segments, ",info,"<org.apache.hadoop.mapred.Merger$MergeQueue: org.apache.hadoop.mapred.RawKeyValueIterator merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress)>"
"Down to the last merge-pass, with , numSegments_,  segments left of total size: , <*>,  bytes, ",info,"<org.apache.hadoop.mapred.Merger$MergeQueue: org.apache.hadoop.mapred.RawKeyValueIterator merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress)>"
"Merging , <*>,  intermediate segments out of a total of , <*>, ",info,"<org.apache.hadoop.mapred.Merger$MergeQueue: org.apache.hadoop.mapred.RawKeyValueIterator merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress)>"
"Creating db record reader for db product: , <*>, ",debug,"<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: org.apache.hadoop.mapreduce.RecordReader createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit,org.apache.hadoop.conf.Configuration)>"
setMaxVirtualMemoryForTask() is deprecated.Instead use setMemoryForMapTask() and setMemoryForReduceTask(),warn,<org.apache.hadoop.mapred.JobConf: void setMaxVirtualMemoryForTask(long)>
"<*>, is not a known counter., ",warn,<org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup: void addCounter(org.apache.hadoop.mapreduce.Counter)>
"Using ShuffleConsumerPlugin: , <*>, ",info,"<org.apache.hadoop.mapred.ReduceTask: void run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol)>"
Cleaning up job,info,"<org.apache.hadoop.mapred.Task: void runJobCleanupTask(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
"Aborting job with runstate : , <*>, ",info,"<org.apache.hadoop.mapred.Task: void runJobCleanupTask(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
Committing job,info,"<org.apache.hadoop.mapred.Task: void runJobCleanupTask(org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
,setTotalLogFileSize,<org.apache.hadoop.mapred.TaskLogAppender: void setOptionsFromSystemProperties()>
"<*>, \nignoreInputKey:, <*>, ",info,<org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
"Not able to initialize queue , name, ",warn,<org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser: java.util.List createQueues(org.apache.hadoop.conf.Configuration)>
"seed: , <*>, ",debug,"<org.apache.hadoop.mapreduce.lib.partition.InputSampler$RandomSampler: java.lang.Object[] getSample(org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Job)>"
starting downlink,debug,<org.apache.hadoop.mapred.pipes.BinaryProtocol: void start()>
starting application,info,"<org.apache.hadoop.mapred.pipes.PipesReducer: void startApplication(org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>"
"Ignoring obsolete output of , <*>,  map-task: \', <*>, \', ",info,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void resolve(org.apache.hadoop.mapred.TaskCompletionEvent)>
"Ignoring output of failed map TIP: \', <*>, \', ",info,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void resolve(org.apache.hadoop.mapred.TaskCompletionEvent)>
"Processing split: , inputSplit, ",info,"<org.apache.hadoop.mapred.MapTask: void updateJobWithSplit(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.InputSplit)>"
"Configuring multithread runner to use , <*>,  threads, ",debug,<org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper: void run(org.apache.hadoop.mapreduce.Mapper$Context)>
"currentIndex , <*>,    , <*>, ",debug,<org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator: void doNext()>
"checkAccess job acls, jobOwner: , jobOwner,  jobacl: , <*>,  user: , <*>, ",debug,"<org.apache.hadoop.mapred.JobACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList)>"
"Reserving: , <*>,  Requested: , requestedSize, ",debug,<org.apache.hadoop.mapred.BackupStore$BackupRamManager: int reserve(int)>
Exception on close,debug,<org.apache.hadoop.mapreduce.lib.db.DBInputFormat: void closeConnection()>
Further records got skipped.,warn,"<org.apache.hadoop.mapred.MapTask$SkippingRecordReader: boolean next(java.lang.Object,java.lang.Object)>"
Could not obtain decompressor from CodecPool,warn,"<org.apache.hadoop.mapred.IFile$Reader: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.mapred.Counters$Counter)>"
Cannot find a range for NUMERIC or DECIMAL fields with one end NULL.,error,"<org.apache.hadoop.mapreduce.lib.db.BigDecimalSplitter: java.util.List split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)>"
"Caught exception parsing history file after , eventCtr_,  events, ",info,"<org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser: void parse(org.apache.hadoop.mapreduce.jobhistory.EventReader,org.apache.hadoop.mapreduce.jobhistory.HistoryEventHandler)>"
"Checking state of job , j, ",debug,<org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl: void run()>
Error while trying to run jobs.,error,<org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl: void run()>
"Total # of splits generated by getSplits: , <*>, , TimeTaken: , <*>, ",debug,"<org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>"
Bad conf file: top-level element not <queues>,info,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue parseResource(org.w3c.dom.Element)>
"Configuring aclsEnabled flag in mapred-queues.xml is not valid. This tag is ignored. Configure mapreduce.cluster.acls.enabled in mapred-site.xml. See the  documentation of mapreduce.cluster.acls.enabled, which is used for enabling job level authorization and  queue level authorization.",warn,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue parseResource(org.w3c.dom.Element)>
 Bad configuration no queues defined ,info,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue parseResource(org.w3c.dom.Element)>
At root level only \ queue \ tags are allowed ,info,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue parseResource(org.w3c.dom.Element)>
"Error parsing conf file: , <*>, ",info,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue parseResource(org.w3c.dom.Element)>
Shuffle failed with too many fetch failures and insufficient progress!,fatal,<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void checkReducerHealth()>
"getTaskLogFileDetail threw an exception , <*>, ",error,"<org.apache.hadoop.mapred.TaskLog: java.io.File getRealTaskLogFileLocation(org.apache.hadoop.mapred.TaskAttemptID,boolean,org.apache.hadoop.mapred.TaskLog$LogName)>"
"created jobQInfo , <*>, ",debug,<org.apache.hadoop.mapred.Queue: org.apache.hadoop.mapred.JobQueueInfo getJobQueueInfo()>
"(EQUATOR) , pos,  kvi , <*>, (, <*>, ), ",info,<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void setEquator(int)>
"Failed to connect to , host,  with , <*>,  map outputs, ",warn,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: java.io.DataInputStream openShuffleUrl(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Set,java.net.URL)>"
"default FileSystem: , <*>, ",debug,"<org.apache.hadoop.mapreduce.JobResourceUploader: void copyLog4jPropertyFile(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,short)>"
"No space available. Available: , <*>,  MinSize: , minSize, ",debug,"<org.apache.hadoop.mapred.BackupStore$BackupRamManager: int reserve(int,int)>"
"MapOutput URL for , host,  -> , <*>, ",debug,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: java.net.URL getMapOutputURL(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Collection)>"
Configuring \mapred.queue.names\ in mapred-site.xml or hadoop-site.xml is deprecated and will overshadow mapred-queues.xml. Remove this property and configure queue hierarchy in mapred-queues.xml,warn,<org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser: boolean deprecatedConf(org.apache.hadoop.conf.Configuration)>
Configuring queue ACLs in mapred-site.xml or hadoop-site.xml is deprecated. Configure queue ACLs in mapred-queues.xml,warn,<org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser: boolean deprecatedConf(org.apache.hadoop.conf.Configuration)>
"<*>,  got an error while submitting , ",info,<org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob: void submit()>
"Processing split: , split, ",info,"<org.apache.hadoop.mapred.MapTask: void runNewMapper(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.split.JobSplit$TaskSplitIndex,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter)>"
"Could not delete , taskAttemptPath_, ",warn,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>"
Output Path is null in abortTask(),warn,"<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>"
Found UTF- BOM and skipped it,info,<org.apache.hadoop.mapreduce.lib.input.LineRecordReader: int skipUtfByteOrderMark()>
"Unreserving: , requestedSize, . Available: , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore$BackupRamManager: void unreserve(int)>
"Fetcher , <*>,  going to fetch from , host,  for: , <*>, ",debug,<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
"copyMapOutput failed for tasks , <*>, ",warn,<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
"Using , <*>,  samples, ",info,"<org.apache.hadoop.mapreduce.lib.partition.InputSampler: void writePartitionFile(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.lib.partition.InputSampler$Sampler)>"
"Failed to set setXIncludeAware(true) for parser , <*>, :, <*>, ",info,<org.apache.hadoop.mapred.QueueConfigurationParser: org.apache.hadoop.mapred.Queue loadResource(java.io.InputStream)>
"Ignoring exception during close for , c, ",info,<org.apache.hadoop.mapred.MapTask: void closeQuietly(org.apache.hadoop.mapred.RecordReader)>
,setFile,<org.apache.hadoop.mapred.TaskLogAppender: void activateOptions()>
"Parent died.  Exiting , <*>, ",warn,<org.apache.hadoop.mapred.Task$TaskReporter: void run()>
"Communication exception: , <*>, ",info,<org.apache.hadoop.mapred.Task$TaskReporter: void run()>
"Last retry, killing , <*>, ",warn,<org.apache.hadoop.mapred.Task$TaskReporter: void run()>
"setsid exited with exit code , <*>, ",info,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isSetsidSupported()>
setsid is not available on this machine. So not using it.,warn,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isSetsidSupported()>
"setsid exited with exit code , <*>, ",info,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isSetsidSupported()>
"setsid exited with exit code , <*>, ",info,<org.apache.hadoop.mapreduce.util.ProcessTree: boolean isSetsidSupported()>
"Total # of splits generated by getSplits: , <*>, , TimeTaken: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
"Finished dispatching all Mappper.map calls, job , <*>, ",debug,"<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>"
"Awaiting all running Mappper.map calls to finish, job , <*>, ",debug,"<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>"
"state-string for task , <*>,  : , stateString, ",info,<org.apache.hadoop.mapred.TaskStatus: void setStateString(java.lang.String)>
"url=, msgToEncode, ;encHash=, encHash, ;replyHash=, <*>, ",debug,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void verifyConnection(java.net.URL,java.lang.String,java.lang.String)>"
"for url=, msgToEncode,  sent hash and received reply, ",info,"<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void verifyConnection(java.net.URL,java.lang.String,java.lang.String)>"
"IndexCache HIT: MapId , mapId,  found, ",debug,"<org.apache.hadoop.mapred.IndexCache: org.apache.hadoop.mapred.IndexCache$IndexInformation readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>"
"IndexCache MISS: MapId , mapId,  not found, ",debug,"<org.apache.hadoop.mapred.IndexCache: org.apache.hadoop.mapred.IndexCache$IndexInformation readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>"
"name, is not a known counter., ",warn,"<org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup: org.apache.hadoop.mapreduce.Counter addCounter(java.lang.String,java.lang.String,long)>"
"loading user\'s secret keys from , <*>, ",info,"<org.apache.hadoop.mapreduce.JobSubmitter: void readTokensFromFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>"
couldn\'t parse Token Cache JSON file with user secret keys,warn,"<org.apache.hadoop.mapreduce.JobSubmitter: void readTokensFromFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>"
"Time taken to get FileStatuses: , <*>, ",debug,<org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.mapred.JobConf)>
"Total input paths to process : , <*>, ",info,<org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.mapred.JobConf)>
waiting for finish,info,<org.apache.hadoop.mapred.pipes.PipesReducer: void close()>
got done,info,<org.apache.hadoop.mapred.pipes.PipesReducer: void close()>
"failure to clean up , <*>, ",info,<org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput: void abort()>
Compressed input; cannot compute number of records in the split,info,"<org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader: void initialize(org.apache.hadoop.conf.Configuration,long,long,org.apache.hadoop.fs.Path)>"
"Expecting , <*>,  records each with a length of , <*>,  bytes in the split with an effective size of , cIn,  bytes, ",info,"<org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader: void initialize(org.apache.hadoop.conf.Configuration,long,long,org.apache.hadoop.fs.Path)>"
"Could not find the clause substitution token $CONDITIONS in the query: , <*>, . Parallel splits may not work correctly., ",error,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader: java.lang.String getSelectQuery()>
"Using query: , <*>, ",debug,<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader: java.lang.String getSelectQuery()>
"Disk Segment added to List. Size is , <*>, ",debug,<org.apache.hadoop.mapred.BackupStore$FileCache: void createInDiskSegment()>
"Connecting to ApplicationMaster at: , serviceAddr, ",trace,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateAMProxy(java.net.InetSocketAddress)>
"Connected to ApplicationMaster at: , serviceAddr, ",trace,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateAMProxy(java.net.InetSocketAddress)>
Job History Server is not configured.,warn,"<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol checkAndGetHSProxy(org.apache.hadoop.yarn.api.records.ApplicationReport,org.apache.hadoop.mapreduce.v2.api.records.JobState)>"
Could not connect to History server.,warn,<org.apache.hadoop.mapred.ClientCache: org.apache.hadoop.mapred.ClientServiceDelegate getClient(org.apache.hadoop.mapreduce.JobID)>
"Usage of -Djava.library.path in , javaConf,  can cause , programs to no longer function if hadoop native libraries , are used. These values should be set as part of the , LD_LIBRARY_PATH in the , component,  JVM env using , envConf,  config settings., ",warn,"<org.apache.hadoop.mapred.YARNRunner: void warnForJavaLibPath(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>"
"getStagingAreaDir: dir=, <*>, ",debug,<org.apache.hadoop.mapred.ResourceMgrDelegate: java.lang.String getStagingAreaDir()>
Error when checking for application status,debug,<org.apache.hadoop.mapred.YARNRunner: void killJob(org.apache.hadoop.mapreduce.JobID)>
"Connecting to HistoryServer at: , <*>, ",debug,<org.apache.hadoop.mapred.ClientCache: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy()>
"Connected to HistoryServer at: , <*>, ",debug,<org.apache.hadoop.mapred.ClientCache: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy()>
"Failed to contact AM/History for job , <*>,  retrying.., ",debug,"<org.apache.hadoop.mapred.ClientServiceDelegate: java.lang.Object invoke(java.lang.String,java.lang.Class,java.lang.Object)>"
ClientServiceDelegate invoke call interrupted,warn,"<org.apache.hadoop.mapred.ClientServiceDelegate: java.lang.Object invoke(java.lang.String,java.lang.Class,java.lang.Object)>"
"Failed to contact AM/History for job , <*>,   Will retry.., ",debug,"<org.apache.hadoop.mapred.ClientServiceDelegate: java.lang.Object invoke(java.lang.String,java.lang.Class,java.lang.Object)>"
ClientServiceDelegate invoke call interrupted,warn,"<org.apache.hadoop.mapred.ClientServiceDelegate: java.lang.Object invoke(java.lang.String,java.lang.Class,java.lang.Object)>"
"AppMaster capability = , capability, ",debug,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
"Creating setup context, jobSubmitDir url is , <*>, ",debug,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
Job jar is not present. Not adding any jar to the list of resources.,info,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
yarn.app.mapreduce.am.log.level INFO,addLog4jSystemProperties,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
><LOG_DIR>/stdout,add,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
><LOG_DIR>/stderr,add,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
"Command to launch container for ApplicationMaster is : , $u, ",debug,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
"Invalid reservationId: , <*>,  specified for the app: , <*>, ",warn,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
"SUBMITTING ApplicationSubmissionContext app:, <*>,  to queue:, <*>,  with reservationId:, <*>, ",info,"<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials)>"
,<init>,"<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.LogParams getLogFilePath(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskAttemptID)>"
,<init>,"<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.LogParams getLogFilePath(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskAttemptID)>"
Cannot get log path for a in-progress job,<init>,"<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.LogParams getLogFilePath(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskAttemptID)>"
"Could not get Job info from RM for job , <*>, . Redirecting to job history server., ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
AM not assigned to Job. Waiting to get the AM ...,debug,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Application state is , <*>, ",debug,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Job , <*>,  is running, but the host is unknown.,  Verify user has VIEW_JOB access., ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Connecting to , <*>, ",debug,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Network ACL closed to AM for job , <*>, . Not going to try to reach the AM., ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Could not connect to , e_, . Waiting for getting the latest AM address..., ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
getProxy() call interruped,warn,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Could not get Job info from RM for job , <*>, . Redirecting to job history server., ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
getProxy() call interruped,warn,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
"Application state is completed. FinalApplicationStatus=, <*>, . Redirecting to job history server, ",info,<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
getBlacklistedTrackers - Not implemented yet,warn,<org.apache.hadoop.mapred.ResourceMgrDelegate: org.apache.hadoop.mapreduce.TaskTrackerInfo[] getBlacklistedTrackers()>
"Exception in creating socket address , <*>, ",warn,<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getRMHAId(org.apache.hadoop.conf.Configuration)>
"getConfValueForRMInstance: prefix = , prefix, ; confKey being looked up = , <*>, ; value being set to = , <*>, ",trace,"<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getConfValueForRMInstance(java.lang.String,org.apache.hadoop.conf.Configuration)>"
Check the condition for main loop.,debug,"<org.apache.hadoop.yarn.client.api.async.AMRMClientAsync: void waitFor(com.google.common.base.Supplier,int,int)>"
Exits the main loop.,info,"<org.apache.hadoop.yarn.client.api.async.AMRMClientAsync: void waitFor(com.google.common.base.Supplier,int,int)>"
Waiting in main loop.,info,"<org.apache.hadoop.yarn.client.api.async.AMRMClientAsync: void waitFor(com.google.common.base.Supplier,int,int)>"
"Unchecked exception is thrown from onContainerStarted for Container , <*>, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StartContainerTransition: org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerState transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)>"
"Refreshing proxy as NMToken got updated for node : , containerManagerBindAddr, ",info,"<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData getProxy(java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId)>"
"Replacing token for : , <*>, ",info,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void populateNMTokens(java.util.List)>
"Received new token for : , <*>, ",info,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void populateNMTokens(java.util.List)>
"Exception when scheduling the event of stopping Container , containerId, ",warn,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl: void stopContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)>"
"Failed to stop Container , <*>, when stopping NMClientImpl, ",error,<org.apache.hadoop.yarn.client.api.impl.NMClientImpl: void cleanupRunningContainers()>
"Failed to stop Container , <*>, when stopping NMClientImpl, ",error,<org.apache.hadoop.yarn.client.api.impl.NMClientImpl: void cleanupRunningContainers()>
"Failed to resolve rack for node , node, ., ",warn,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: java.util.Set resolveRacks(java.util.List)>
"ContainerRequest has duplicate racks: , <*>, ",warn,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addContainerRequest(org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>
"ContainerRequest has duplicate nodes: , <*>, ",warn,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addContainerRequest(org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>
"Unchecked exception is thrown from onGetContainerStatusError for Container , containerId, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor: void onExceptionRaised(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable)>"
Check the condition for main loop.,debug,"<org.apache.hadoop.yarn.client.api.AMRMClient: void waitFor(com.google.common.base.Supplier,int,int)>"
Exits the main loop.,info,"<org.apache.hadoop.yarn.client.api.AMRMClient: void waitFor(com.google.common.base.Supplier,int,int)>"
Waiting in main loop.,info,"<org.apache.hadoop.yarn.client.api.AMRMClient: void waitFor(com.google.common.base.Supplier,int,int)>"
"Unchecked exception is thrown from onContainerStopped for Container , <*>, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition: org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerState transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)>"
Logs are not avaiable right now.,println,<org.apache.hadoop.yarn.client.cli.LogsCLI: int run(java.lang.String[])>
Unable to get ApplicationState. Attempting to fetch logs directly from the filesystem.,println,<org.apache.hadoop.yarn.client.cli.LogsCLI: int run(java.lang.String[])>
,setConf,<org.apache.hadoop.yarn.client.cli.LogsCLI: int run(java.lang.String[])>
"Unchecked exception is thrown from onStopContainerError for Container , <*>, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition: org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerState onExceptionRaised(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent,java.lang.Throwable)>"
"yarn.client.max-cached-nodemanagers-proxies : , <*>, ",info,"<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.api.NMTokenCache)>"
"Submitted application , <*>, ",info,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: org.apache.hadoop.yarn.api.records.ApplicationId submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)>
"Application submission is not finished, submitted application , <*>,  is still in , <*>, ",info,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: org.apache.hadoop.yarn.api.records.ApplicationId submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)>
"Interrupted while waiting for application , <*>,  to be successfully submitted., ",error,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: org.apache.hadoop.yarn.api.records.ApplicationId submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)>
"Re-submit application , <*>, with the , same ApplicationSubmissionContext, ",info,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: org.apache.hadoop.yarn.api.records.ApplicationId submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)>
"Unchecked exception is thrown from onStartContainerError for Container , <*>, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StartContainerTransition: org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerState onExceptionRaised(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent,java.lang.Throwable)>"
"Cleaning up the proxy cache, size=, <*>,  max=, <*>, ",debug,"<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void addProxyToCache(java.lang.String,org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>"
Can\'t handle this event at current state,error,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer: void handle(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)>
Error joining with heartbeat thread,error,<org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl: void serviceStop()>
"Returning, thread interrupted",error,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$1: void run()>
"Set NMClientAsync thread pool size to , <*>,  as the number of nodes to talk to is , <*>, ",info,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$1: void run()>
"Opening proxy : , containerManagerBindAddr, ",info,"<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData: org.apache.hadoop.yarn.api.ContainerManagementProtocol newProxy(org.apache.hadoop.yarn.ipc.YarnRPC,java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Token)>"
"Killed application , applicationId, ",info,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void killApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
"Waiting for application , applicationId,  to be killed., ",info,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void killApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
"Interrupted while waiting for application , applicationId,  to be killed., ",error,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void killApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
\tLOG-URL : ,print,<org.apache.hadoop.yarn.client.cli.ApplicationCLI: void printContainerReport(java.lang.String)>
Waiting for application to be successfully unregistered.,info,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String)>"
Interrupted while waiting for application to be removed from RMStateStore,info,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String)>"
"ApplicationMaster is out of sync with ResourceManager, hence resyncing.",warn,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String)>"
Error closing connection,error,<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void stopAllProxies()>
"Exception when scheduling the event of querying the status of Container , containerId, ",warn,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl: void getContainerStatusAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)>"
Stopping callback due to: ,error,<org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread: void run()>
Interrupted while waiting for queue,info,<org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread: void run()>
"Closing proxy : , <*>, ",info,<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: boolean tryCloseProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>
Retrieve logs for completed YARN applications.,println,<org.apache.hadoop.yarn.client.cli.LogsCLI: void printHelpMessage(org.apache.commons.cli.Options)>
yarn logs -applicationId <application ID> OPTIONS,printHelp,<org.apache.hadoop.yarn.client.cli.LogsCLI: void printHelpMessage(org.apache.commons.cli.Options)>
"Processing Event , <*>,  for Container , <*>, ",info,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor: void run()>
"Unchecked exception is thrown from onContainerStatusReceived for Container , <*>, ",info,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor: void run()>
"Container , <*>,  is already stopped or failed, ",info,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor: void run()>
"Added priority=, priority, ",debug,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String)>"
"addResourceRequest: applicationId= priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",debug,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String)>"
"Add timline delegation token into credentials: , <*>, ",debug,<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void addTimelineDelegationToken(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>
"ApplicationMaster is out of sync with ResourceManager, hence resyncing.",warn,<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(float)>
"The thread of , <*>,  didn\'t finish normally., ",error,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl: void serviceStop()>
"Not decrementing resource as priority , priority,  is not present in request table, ",debug,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>"
"Not decrementing resource as , resourceName,  is not present in request table, ",debug,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>"
"BEFORE decResourceRequest: applicationId= priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",debug,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>"
"AFTER decResourceRequest: applicationId= priority=, <*>,  resourceName=, resourceName,  numContainers=, <*>,  #asks=, <*>, ",info,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>"
"Unchecked exception is thrown from onStartContainerError for Container , <*>, ",info,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$OutOfOrderTransition: void transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)>"
"Upper bound of the thread pool size is , <*>, ",info,<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
"Exception when scheduling the event of starting Container , <*>, ",warn,"<org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl: void startContainerAsync(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>"
The same resources appear in both blacklistAdditions and blacklistRemovals in updateBlacklist.,warn,"<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void updateBlacklist(java.util.List,java.util.List)>"