Log,LoggingLevel,Method
Token cancel failed:  <*> ,debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void close()>
Using UGI token:  <*> ,debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>
Fetched new token:  <*> ,debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>
url= <*> ,trace,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])>"
url= <*> ,trace,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>"
Replaced expired token:  <*> ,debug,<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: boolean replaceExpiredDelegationToken()>
"Retrying connect to namenode:  <*> . Already tried  retry  time(s); retry policy is  <*> , delay  <*> ms. ",info,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>"
Original exception is ,warn,"<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner: void shouldRetry(java.io.IOException,int)>"
Cannot load customized ssl related configuration. Fallback to system-generic settings.,debug,<org.apache.hadoop.hdfs.web.URLConnectionFactory: org.apache.hadoop.hdfs.web.URLConnectionFactory newDefaultURLConnectionFactory(org.apache.hadoop.conf.Configuration)>
open AuthenticatedURL connection url ,debug,"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>"
open URL connection,debug,"<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>"
Found existing DT for  <*> ,debug,<org.apache.hadoop.hdfs.web.TokenAspect: void initDelegationToken(org.apache.hadoop.security.UserGroupInformation)>
Created new DT for  <*> ,debug,<org.apache.hadoop.hdfs.web.TokenAspect: void ensureTokenInitialized()>
GOT EXCEPITION,trace,<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
INTERNAL_SERVER_ERROR,warn,<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
url= <*> ,trace,"<org.apache.hadoop.hdfs.web.HftpFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>"
Exception getting delegation token,debug,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
Got dt for  <*> ;t.service= <*> ,debug,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
"Couldn\'t connect to  <*> , assuming security is disabled ",warn,<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
Saved MD  digestString  to  <*> ,debug,"<org.apache.hadoop.hdfs.util.MD5FileUtils: void saveMD5File(java.io.File,java.lang.String)>"
deleting   <*>  FAILED ,warn,"<org.apache.hadoop.hdfs.util.MD5FileUtils: void renameMD5File(java.io.File,java.io.File)>"
"initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor ",debug,"<org.apache.hadoop.hdfs.util.LightWeightHashSet: void <init>(int,float,float)>"
Unable to delete tmp file  <*> ,warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void close()>
Unable to abort file  <*> ,warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>
Unable to delete tmp file during abort  <*> ,warn,<org.apache.hadoop.hdfs.util.AtomicFileOutputStream: void abort()>
Interrupted. Stopping the WebImageViewer.,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServerAndWait(java.lang.String)>
WebImageViewer started. Listening on  <*> . Press Ctrl+C to stop the viewer. ,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer: void initServer(java.lang.String)>
image loading failed at offset  <*> ,error,<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer: void go()>
Loading section  <*>  length:  <*> ,debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader load(java.lang.String)>
Loading inode directory section,info,"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>"
Loaded  counter  directories ,info,"<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.util.Map loadINodeDirectorySection(java.io.InputStream,java.util.List)>"
Loading  <*>  strings ,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: java.lang.String[] loadStringTable(java.io.InputStream)>
Loading inode references,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>
Loaded  counter  inode references ,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: com.google.common.collect.ImmutableList loadINodeReferenceSection(java.io.InputStream)>
Sorting inodes,debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
Finished sorting inodes,debug,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
Loading  <*>  inodes. ,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
<*>  method= <*>  op= <*>  target= path ,info,<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler: org.jboss.netty.channel.ChannelFuture handleOperation(org.jboss.netty.channel.MessageEvent)>
Got IOException at position  <*> ,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Got IOException while reading stream!  Resyncing.,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Got RuntimeException at position  <*> ,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Got RuntimeException while reading stream!  Resyncing.,error,<org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader: void loadEdits()>
Using NN principal:  <*> ,debug,<org.apache.hadoop.hdfs.tools.GetGroups: void setConf(org.apache.hadoop.conf.Configuration)>
"Got a fatal error, exiting now",fatal,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void main(java.lang.String[])>
Allowed RPC access from  <*>  at  <*> ,info,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>
Disallowed RPC access from  <*>  at  <*> . Not listed in  dfs.cluster.administrators ,warn,<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void checkRpcAdminAccess()>
Failover controller configured for NameNode  localTarget ,info,"<org.apache.hadoop.hdfs.tools.DFSZKFailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.tools.NNHAServiceTarget)>"
Using NN principal:  <*> ,debug,<org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
Exception encountered:,debug,<org.apache.hadoop.hdfs.tools.DFSAdmin: int run(java.lang.String[])>
Retrieving token from:  <*> ,debug,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: org.apache.hadoop.security.Credentials getDTfromRemote(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,java.lang.String,java.lang.String)>"
error in renew over HTTP,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>"
rethrowing exception from HTTP request:  <*> ,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: long renewDelegationToken(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,org.apache.hadoop.security.token.Token)>"
Error when dealing remote token:,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>"
rethrowing exception from HTTP request:  <*> ,info,"<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.net.HttpURLConnection run(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URL)>"
Error response from HTTP request= <*> ;ec= ie ;em= exceptionMsg ,info,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
Exception from HTTP response= <*> ,info,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
failed to create object of this class,warn,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher: java.io.IOException getExceptionFromResponse(java.net.HttpURLConnection)>
Renewed token for  <*>  until:  <*> ,debug,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
Cancelled token for  <*> ,debug,<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
this : unregisterSlot  slotIdx ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void unregisterSlot(int)>
this : freed ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>
this : failed to munmap ,warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>
"creating  <*> (shmId= shmId , mmappedLength= <*> , baseAddress= <*> , slots.length= <*> ) ",trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void <init>(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$ShmId,java.io.FileInputStream)>"
failed to load misc.Unsafe,error,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: sun.misc.Unsafe safetyDance()>
this : registerSlot  slotIdx : allocatedSlots= <*> <*> ,trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)>"
this : allocAndRegisterSlot  <*> : allocatedSlots= <*> <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)>
closed  this suffix ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: void close()>
this : created mmap of size  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
this : mmap error ,warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
this : mmap error ,warn,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
this : checked shared memory segment.  isStale= <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
"this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
"this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> ",trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
this : added no-checksum anchor to slot  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>
this : could not add no-checksum anchor to slot  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>
this :  purgeReason#_ ,debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
this : trimEvictionMaps is purging  replica <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void trimEvictionMaps()>
this : starting cache cleaner thread which will run  every  <*>  ms ,debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void startCacheCleanerThreadIfNeeded()>
this :  <*>  no longer contains  replica .  refCount  <*>  ->  <*> <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
this : replica  refCount  <*>  ->  <*> <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
this : closing ,info,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void close()>
failed to create ShortCircuitShmManager,error,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void <init>(int,long,int,long,long,long,int)>"
this : retrying  <*> ,debug,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>"
this : can\'t fetchOrCreate  key  because the cache is closed. ,trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>"
this : interrupted while waiting for  key ,info,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
this : got stale replica  <*> .  Removing  this replica from the replicaInfoMap and retrying. ,info,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
this : found waitable for  key ,trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
this : could not get  key  due to InvalidToken  exception. ,warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
this : failed to get  key ,warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>"
this : loading  key ,trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
this : successfully loaded  <*> ,trace,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
this : failed to load  key ,warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
this : could not load  key  due to InvalidToken  exception. ,warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
this : failed to load  key ,warn,"<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>"
demoteOldEvictable: demoting  <*> :  <*> :  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: int demoteOldEvictableMmaped(long)>
<*> : failed to release  short-circuit shared memory slot  <*>  by sending  ReleaseShortCircuitAccessRequestProto to  <*> .  Closing shared memory segment. ,error,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
<*> : about to release  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
<*> : released  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
this : cache cleaner running at  <*> ,debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
this : finishing cache cleaner run started at  <*> .  Demoted  <*>  mmapped replicas;  purged  numPurged  replicas. ,debug,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
CacheCleaner: purging  replica :  <*> ,trace,<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
Both short-circuit local reads and UNIX domain socket are disabled.,debug,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
feature  is enabled. ,debug,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
feature#_  cannot be used because  <*> ,warn,<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.DFSClient$Conf)>
error creating DomainSocket,warn,"<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: org.apache.hadoop.net.unix.DomainSocket createSocket(org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathInfo,int)>"
this : error shutting down shm: got IOException calling  shutdown(SHUT_RDWR) ,warn,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void shutdown(org.apache.hadoop.hdfs.shortcircuit.DfsClientShm)>
this : freeing empty stale  shm ,trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
this : shutting down UNIX domain socket for  empty  shm ,trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
this : pulled the last slot  <*>  out of  shm ,trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>
this : pulled slot  <*>  out of  shm ,trace,<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>
this : the UNIX domain socket associated with  this short-circuit memory closed before we could make  use of the shm. ,debug,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
this : the DfsClientShmManager has been closed. ,trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
this : shared memory segment access is disabled. ,trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
this : waiting for loading to finish... ,trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>"
this : datanode does not support short-circuit  shared memory access:  <*> ,info,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
this : createNewShm: created  <*> ,trace,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
this : error requesting short-circuit shared memory  access:  <*> ,warn,"<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>"
"HTTP  <*> :  op ,  path , ugi= ugi ,  username ,  doAsUser <*> ",trace,"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>"
redirectURI= <*> ,trace,"<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: java.net.URI redirectURI(org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>"
Image upload with txid  txid  conflicted with a previous image upload to the  same NameNode. Continuing... ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>"
Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)>"
Image Transfer timeout configured to  <*>  milliseconds ,info,<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void setTimeout(java.net.HttpURLConnection)>
Dest file:  f ,debug,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
Renaming  <*>  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
Skipping download of remote edit log  log  since it already is stored locally at  f ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
Downloaded file  <*>  size  <*>  bytes. ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
Unable to rename edits file from  <*>  to  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>"
Connection closed by client,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>"
SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler)>"
Overwriting existing file  f  with file downloaded from  url ,warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Unable to download file  f ,warn,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Downloaded file  <*>  size  <*>  bytes. ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash handleUploadImageRequest(javax.servlet.http.HttpServletRequest,long,org.apache.hadoop.hdfs.server.common.Storage,java.io.InputStream,long,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Opening connection to  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash getFileClient(java.net.URL,java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean)>"
Downloaded file  <*>  size  <*>  bytes. ,info,"<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: org.apache.hadoop.io.MD5Hash downloadImageToStorage(java.net.URL,long,org.apache.hadoop.hdfs.server.common.Storage,boolean)>"
response.isCommitted()= <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.StreamFile: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
BUG: removeSnapshot increases namespace usage.,error,"<org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature: org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot removeSnapshot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
Interrupted waiting to join on checkpointer thread,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Exception shutting down SecondaryNameNode,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Exception while closing CheckpointStorage,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void shutdown()>
Failed to parse options,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>
Failed to start secondary namenode,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void main(java.lang.String[])>
Exception ,debug,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void join()>
Web server init done,info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
Checkpoint Period   : <*>  secs  ( <*>  min) ,info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
Log Size Trigger    : <*>  txns ,info,"<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>"
Exception in doCheckpoint,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
Merging failed  <*>  times. ,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
Throwable Exception in doCheckpoint,fatal,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void doWork()>
Will connect to NameNode at  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.net.URL getInfoServer()>
<*> :  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
<*> :  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
<*> :  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: int processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)>
Checkpoint done. New Image Size:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>
Failed to write legacy OIV image: ,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: boolean doCheckpoint()>
Formatting storage directory  sd ,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void recoverCreate(boolean)>
Failed to delete temporary edits file:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: void deleteTempEdits()>
Image has not changed. Will not download image.,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>
Image has changed. Downloading updated image from NN.,info,<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2: java.lang.Boolean run()>
Request for token received with no authentication from  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Exception while renewing token. Re-throwing. s= <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.RenewDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
got premature end-of-file at txid  <*> ; expected file to go up to  <*> ,<init>,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
Got error reading edit log input stream  <*> ; failing over to edit log  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
failing over to edit log  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
Fast-forwarding stream \' <*> \' to transaction ID  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOp()>
Unable to rename temp to previous for  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>"
Performing upgrade of storage directory  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage)>"
Rollback of  <*>  is complete. ,info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Starting upgrade of storage directory  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doPreUpgrade(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>"
Directory  <*>  does not exist. ,info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Finalize upgrade for  <*>  is not required. ,info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Finalizing upgrade of storage directory  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Finalize upgrade for  <*>  is complete. ,info,<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Storage directory  <*>  does not contain previous fs state. ,info,"<org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: boolean canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int)>"
Deleting  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
Invalid file name. Skipping  fName ,warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
Failed to delete image file:  $u ,warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: void purgeOldLegacyOIVImages(java.lang.String,long)>"
Going to retain  <*>  images with txid >=  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: long getImageTxIdToRetain(org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector)>
Purging old edit log  log ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeLog(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)>
Purging old image  image ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void purgeImage(org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile)>
Could not delete  file ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: void deleteOrWarn(java.io.File)>
writeTransactionIdToStorage failed on  sd ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void writeTransactionIdFileToStorage(long)>
set restore failed storage to  val ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void setRestoreFailedStorage(boolean)>
current list of storage dirs: <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
at the end current list of storage dirs: <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Error reported on storage directory  sd ,error,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
About to remove corresponding storage:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Unable to unlock bad storage directory:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Using clusterid:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>"
"Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> ",warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: void processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int)>"
Storage directory  <*>  has been successfully formatted. ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
currently disabled dir  <*> ; type= <*> ;canwrite= <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
restoring dir  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: void attemptRestoreRemovedStorage()>
Storage directory  sd  contains no VERSION file. Skipping... ,warn,"<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector readAndInspectDirs(java.util.EnumSet,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Error converting file to URI,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory getStorageDirectory(java.net.URI)>
Could not find ip address of \default\ inteface.,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String newBlockPoolID()>
current cluster id for sd= <*> ;lv= <*> ;cid= <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
this sd not available:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
couldn\'t find any VERSION file containing valid ClusterId,warn,<org.apache.hadoop.hdfs.server.namenode.NNStorage: java.lang.String determineClusterId()>
ACLs enabled?  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
XAttrs enabled?  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
Maximum size of an xattr:  <*> <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NNConf: void <init>(org.apache.hadoop.conf.Configuration)>
<*> . Note: This is normal during a rolling upgrade. ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
<*>  DN:  dnReg ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
Registration IDs mismatched: the  <*>  ID is  <*>  but the expected ID is  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void verifyRequest(org.apache.hadoop.hdfs.server.protocol.NodeRegistration)>
*DIR* NameNode.rename:  src  to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>"
Refreshing all user-to-groups mappings. Requested by user:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshUserToGroupsMappings()>
Refreshing SuperUser proxy group mapping list ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshSuperUserGroupsConfiguration()>
Refreshing call queue.,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void refreshCallQueue()>
Error report from  registration :  msg ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)>"
Error report from  <*> :  msg ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
Error report from  <*> :  msg ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
Disk error on  <*> :  msg ,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
Fatal disk error on  <*> :  msg ,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)>"
*BLOCK* NameNode.blockReceivedAndDeleted: from  nodeReg   <*>  blocks. ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])>"
*BLOCK* NameNode.abandonBlock:  b  of file  src ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
Service RPC server is binding to  bindHost : <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>"
RPC server is binding to  serviceHandlerCount : <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NameNode)>"
*BLOCK* NameNode.cacheReport: from  nodeReg   <*>  blocks ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)>"
"*BLOCK* NameNode.blockReport: from  nodeReg , reports.length= <*> ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>"
Tried to read from deleted or moved edit log segment,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
Tried to read from deleted edit log segment,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
rollingUpgrade  action ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)>
"getAdditionalDatanode: src= src , fileId= fileId , blk= blk , existings= <*> , excludes= <*> , numAdditionalNodes= numAdditionalNodes , clientName= clientName ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)>"
*DIR* NameNode.append: file  src  for  clientName  at  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock append(java.lang.String,java.lang.String)>"
*BLOCK* NameNode.addBlock: file  src  fileId= fileId  for  clientName ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[])>"
*DIR* NameNode.create: file  src  for  clientName  at  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>"
NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.inotify.EventBatchList getEditsFromTxid(long)>
Getting groups for user  user ,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: java.lang.String[] getGroupsForUser(java.lang.String)>
*DIR* NameNode.rename:  src  to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean rename(java.lang.String,java.lang.String)>"
*DIR* NameNode.mkdirs:  src ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>"
"*DIR* Namenode.delete: src= src , recursive= recursive ",debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean delete(java.lang.String,boolean)>"
*DIR* NameNode.complete:  src  fileId= fileId  for  clientName ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
Going to check the following volumes disk space:  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: java.util.Collection getVolumesLowOnSpace()>
Space available on volume \' <*> \' is  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
"Space available on volume \' <*> \' is  <*> , which is below the configured reserved amount  <*> ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
Added filter \' <*> \' (class= <*> ) ,info,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: void initWebHdfs(org.apache.hadoop.conf.Configuration)>
"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.",error,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>
"WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.keytab\' is not set.",error,<org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: java.util.Map getAuthFilterParams(org.apache.hadoop.conf.Configuration)>
Cannot use /lost+found : a regular file with this name exists.,warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>
Cannot initialize /lost+found .,warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void lostFoundInit(org.apache.hadoop.hdfs.DFSClient)>
FSCK started by  <*>  from  <*>  for path  <*>  at  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>
Fsck on path \' <*> \'  FAILED ,warn,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void fsck()>
Fsck: error deleting corrupted file  path ,error,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>
Fsck: deleted corrupt file  path ,info,<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void deleteCorruptedFile(java.lang.String)>
Fsck: could not copy block  <*>  to  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
copyBlocksToLostFound: error processing  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
Fsck: copied the remains of the corrupted file  <*>  to /lost+found ,info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
"Fsck: can\'t copy the remains of  <*>  to  lost+found, because  <*>  already exists. ",warn,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
Fsck: there were errors copying the remains of the corrupted file  <*>  to /lost+found ,warn,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks)>"
Error reading block,error,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
Could not obtain block from any node:   <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
Failed to connect to  <*> : <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream)>"
Fsck: ignoring open file  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: void check(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result)>"
Exception while stopping httpserver,error,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopHttpServer()>
ServicePlugin  p  could not be stopped ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stopCommonServices()>
Encountered exception while exiting state ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void stop()>
<*>  RPC up at:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
<*>  service RPC up at:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
ServicePlugin  p  could not be started ,warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void startCommonServices(org.apache.hadoop.conf.Configuration)>
Setting ADDRESS  address ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void setServiceAddress(org.apache.hadoop.conf.Configuration,java.lang.String)>"
fs.defaultFS is  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>
Clients are to use  <*>  to access  this namenode/service. ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void setClientNamenodeAddress(org.apache.hadoop.conf.Configuration)>
Failed to start namenode.,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: void main(java.lang.String[])>
Caught interrupted exception ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void join()>
Setting fs.defaultFS to  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>"
Clients are to use  <*>  to access  this namenode/service. ,info,<org.apache.hadoop.hdfs.server.namenode.NameNode: void initialize(org.apache.hadoop.conf.Configuration)>
starting recovery...,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY COMPLETE,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY FAILED: caught exception,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
RECOVERY FAILED: caught exception,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration)>"
Error encountered requiring NN shutdown. Shutting down immediately.,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: void doImmediateShutdown(java.lang.Throwable)>
<*> L,closeAllStreams,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
Beginning to copy stream  stream  to shared edits ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
ending log segment because of END_LOG_SEGMENT op in  stream ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
ending log segment because of end of stream in  stream ,debug,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
copying op:  <*> ,trace,"<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>"
"Allowing manual HA control from  <*>  even though automatic HA is enabled, because the user  specified the force flag ",warn,<org.apache.hadoop.hdfs.server.namenode.NameNode: void checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)>
createNameNode  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.namenode.NameNode createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration)>"
Must specify a valid cluster ID after the  <*>  flag ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Must specify a valid cluster ID after the  <*>  flag ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Must specify a valid cluster ID after the  <*>  flag ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Unknown upgrade flag  clusterId ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Must specify a rolling upgrade startup option  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Invalid argument:  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.NameNode: org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption parseArguments(java.lang.String[])>
Could not initialize shared edits dir,error,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
No shared edits directory configured for namespace  <*>  namenode  <*> ,fatal,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not close sharedEditsImage,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Could not unlock storage directories,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
Encountered exception during format: ,warn,"<org.apache.hadoop.hdfs.server.namenode.NameNode: boolean format(org.apache.hadoop.conf.Configuration,boolean,boolean)>"
initialized with  <*>  entries  <*>  lookups ,info,<org.apache.hadoop.hdfs.server.namenode.NameCache: void initialized()>
Exiting on user request.,error,<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void quit()>
Continuing,info,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: void editLogLoaderPrompt(java.lang.String,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,java.lang.String)>"
"I\'m sorry, I cannot understand your response.\n",error,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>"
automatically choosing  firstChoice ,info,"<org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext: java.lang.String ask(java.lang.String,java.lang.String,java.lang.String[])>"
ListPathServlet encountered InterruptedException,warn,"<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
ListPathsServlet - Path  p  does not exist ,warn,<org.apache.hadoop.hdfs.server.namenode.ListPathsServlet$2: java.lang.Void run()>
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void stopMonitor()>
<*> .removeLeaseWithPrefixPath: entry= entry ,debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLeaseWithPrefixPath(java.lang.String)>
src  not found in lease.paths (= <*> ) ,debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>"
lease  not found in sortedLeases ,error,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String)>"
Removing non-existent lease! holder= holder  src= src ,warn,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(java.lang.String,java.lang.String)>"
"<*> .changelease:   src= src , dest= dst ",debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>"
changeLease: replacing  oldpath  with  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void changeLease(java.lang.String,java.lang.String)>"
Number of blocks under construction:  numUCBlocks ,info,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>
The file  <*>  is not under construction but has lease. ,warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: long getNumUnderConstructionBlocks()>
Ignore the lease of file  p  for checkpoint since the file is not under construction ,warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map getINodesUnderConstruction()>
<*> .findLease: prefix= prefix ,debug,"<org.apache.hadoop.hdfs.server.namenode.LeaseManager: java.util.Map findLeaseWithPrefixPath(java.lang.String,java.util.SortedMap)>"
Lease recovery for  p  is complete. File closed. ,debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
Started block recovery  p  lease  leaseToCheck ,debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
Cannot release the path  p  in the lease  leaseToCheck ,error,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
leaseToCheck  has expired hard limit ,info,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
Unable to release hard-limit expired lease:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
<*>  is interrupted ,debug,<org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor: void run()>
Error in setting outputbuffer capacity,error,<org.apache.hadoop.hdfs.server.namenode.JournalSet: void setOutputBufferCapacity(int)>
Skipping jas  jas  since it\'s disabled ,info,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>"
Unable to determine input streams from  <*> . Skipping. ,warn,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void selectInputStreams(java.util.Collection,long,boolean)>"
Error:  status  failed for (journal  jas ) ,error,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
Error:  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
Error:  status  failed for required journal ( jas ) ,fatal,"<org.apache.hadoop.hdfs.server.namenode.JournalSet: void mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String)>"
Disabling journal  j ,error,<org.apache.hadoop.hdfs.server.namenode.JournalSet: void disableAndReportErrorOnJournals(java.util.List)>
Found gap in logs at  j# :  not returning previous logs in manifest. ,debug,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
Generated manifest for logs since  fromTxId : $u ,debug,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
Cannot list edit logs in  fjm ,warn,<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
Unable to abort stream  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream: void abort()>
UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.INodesInPath: org.apache.hadoop.hdfs.server.namenode.INodesInPath resolve(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,byte[][],int,boolean)>"
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
should not exceed quota while snapshot deletion,error,"<org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference: void destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List)>"
Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>"
Received an invalid request file transfer request from a secondary with storage info  theirStorageInfoString ,warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: void validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String)>"
SecondaryNameNode principal could not be added,debug,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
ImageServlet allowing checkpointer:  remoteUser ,info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
ImageServlet allowing administrator:  remoteUser ,info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
ImageServlet rejecting:  remoteUser ,info,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
Received null remoteUser while authorizing access to getImage servlet,warn,"<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>"
Edit log tailer thread exited with an exception,warn,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void stop()>
Starting standby checkpoint thread...\nCheckpointing active NN at  <*> \n Serving checkpoints at  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void start()>
"Standby Checkpointer should only attempt a checkpoint when NN is in standby mode, but the edit logs are in an unexpected state",<init>,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>
A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid  <*> . Skipping... ,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: void doCheckpoint()>
Exception in doCheckpoint,error,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Triggering a rollback fsimage for rolling upgrade.,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Triggering checkpoint because there have been  <*>  txns since the last checkpoint, which  exceeds the configured threshold  <*> ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
"Triggering checkpoint because it has been  <*>  seconds since the last checkpoint, which  exceeds the configured interval  <*> ",info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
But skipping this checkpoint since we are about to failover!,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Checkpoint was cancelled:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Interrupted during checkpointing,info,<org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread: void doWork()>
Triggering log roll on remote NameNode  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>
Unable to trigger a roll of the active NN,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void triggerActiveLogRoll()>
Edit log tailer thread exited with an exception,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void stop()>
lastTxnId:  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
edit streams to load from:  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
Edits tailer failed to find any streams. Will try again later.,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
logRollPeriodMs= <*>  sleepTime= <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
Will roll logs on active node at  <*>  every  <*>  seconds. ,info,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
Not going to trigger log rolls on active node because dfs.ha.log-roll.period is negative.,info,"<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
Unknown error encountered while tailing edits. Shutting down standby NN.,fatal,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
Error while reading edits from disk. Will try again.,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
Edit log tailer interrupted,warn,<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread: void doWork()>
Failed to create RPC proxy to NameNode,error,<org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider: org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo getProxy()>
Full exception trace,debug,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
Unable to fetch namespace information from active NN at  <*> :  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
Layout version on remote node ( <*> ) does not match  this node\'s layout version ( <*> ) ,fatal,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.,info,<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
Failed to move aside pre-upgrade storage in image directory  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
The storage directory is in an inconsistent state,warn,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Unable to read transaction ids  firstTxIdInLogs - curTxIdOnOtherNode  from the configured shared edits storage  <*> .  Please copy these logs into the shared edits storage  or call saveNamespace on the active node.\n Error:  <*> ,fatal,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>"
Unable to read transaction ids  firstTxIdInLogs - curTxIdOnOtherNode  from the configured shared edits storage  <*> .  Please copy these logs into the shared edits storage  or call saveNamespace on the active node.\n Error:  <*> ,fatal,"<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: boolean checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)>"
Request for token received with no authentication from  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"Sending token: { <*> , <*> } ",info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Exception while sending token. Re-throwing ,info,"<org.apache.hadoop.hdfs.server.namenode.GetDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
"ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkPermission(java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSDirectory,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean,boolean)>"
Update  oldBlock  (len =  <*> ) to an older state:  newBlock  (len =  <*> ) ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean)>"
"updatePipeline(block= oldBlock , newGenerationStamp= <*> , newLength= <*> , newNodes= <*> , clientName= clientName ) ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
updatePipeline( oldBlock ) successfully to  newBlock ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
Stopping services started for standby state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopStandbyServices()>
Stopping services started for active state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void stopActiveServices()>
Starting services required for standby state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startStandbyServices(org.apache.hadoop.conf.Configuration)>
Successfully saved namespace for preparing rolling upgrade.,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startRollingUpgradeInternalForNonHA(long)>
NameNode metadata after re-processing replication and invalidation queues during failover:\n <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Starting services required for active state,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Catching up to latest edits from old active before taking over writer role in edits logs,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Reprocessing replication and invalidation queues,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Will take over writing edit logs at txnid  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
New namespace image has been created,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void saveNamespace()>
*DIR* reportBadBlocks,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])>
DIR* NameSystem.renameTo: with options -  srcArg  to  dstArg ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void renameTo(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>"
"Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete ",debug,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo)>
Registered FSNamesystemState MBean,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void registerMBean()>
"recoverLease:  <*> , src= src  from client  <*> ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"startFile: recover  <*> , src= src  client  <*> ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.INodeFile,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"persistNewBlock:  path  with new block  <*> , current total block count is  <*> ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistNewBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
persistBlocks:  path  with  <*>  blocks is persisted to  the file system ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void persistBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
"Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>
STATE* Safe mode is already OFF,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void leaveSafeMode()>
initializing replication queues,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void initializeReplQueues()>
BLOCK* fsync:  src  for  clientName ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void fsync(java.lang.String,long,java.lang.String,long)>"
<*> ,logFinalizeRollingUpgrade,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void finalizeRollingUpgrade()>
STATE* Safe mode is ON <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enterSafeMode(boolean)>
End checkpoint for  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>"
Logj is required to enable async auditlog,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void enableAsyncAuditLog()>
DIR* NameSystem.createSymlink: target= target  link= linkArg ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void createSymlinkInt(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
DIR* NameSystem.concat:  <*>  to  target ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concatInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String[],boolean)>"
concat  <*>  to  target ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void concat(java.lang.String,java.lang.String[])>"
Unexpected exception while updating disk space.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitOrCompleteLastBlock(org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>"
Block (= lastblock ) not found ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
Unexpected block (= lastblock ) since the file (= <*> ) is not under construction ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
DatanodeDescriptor (= <*> ) not found ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"commitBlockSynchronization(lastblock= lastblock , newgenerationstamp= newgenerationstamp , newlength= newlength , newtargets= <*> , closeFile= closeFile , deleteBlock= deleteblock ) ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
"commitBlockSynchronization(newblock= lastblock , file= src , newgenerationstamp= newgenerationstamp , newlength= newlength , newtargets= <*> ) successful ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
commitBlockSynchronization( lastblock ) successful ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>"
closeFile:  path  with  <*>  blocks is persisted to the file system ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void closeFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>
Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void checkConfiguration(org.apache.hadoop.conf.Configuration)>
<*>  initialization failed. ,error,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
<*>  initialization failed. ,error,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
No KeyProvider found.,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Found KeyProvider:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Enabling async auditlog,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
fsLock is fair: <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
fsOwner             =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
supergroup          =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
isPermissionEnabled =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Determined nameservice ID:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
HA Enabled:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Append Enabled:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
Configured NNs:\n <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FSImage,boolean)>"
trying to get DT with no secret manager running,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
Retry cache on namenode is  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>
Retry cache will use  <*>  of total heap and retry cache entry expiry time is  <*>  millis ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.ipc.RetryCache initRetryCache(org.apache.hadoop.conf.Configuration)>
Start checkpoint for  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
addSymlink:  path  is added ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
addSymlink: failed to add  path ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>"
DIR* NameSystem.startFile: added  src  inode  <*>   holder ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>"
DIR* NameSystem.startFile:  src   <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo startFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,boolean,boolean,boolean,short,long,boolean,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion,boolean)>"
BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= src  lastBlock= <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
BLOCK* allocateBlock: caught retry for allocation of a new block in  src . Returning previously allocated block  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem$FileState analyzeFileState(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
Finished loading FSImage in  ioe  msecs ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>
Encountered exception loading fsimage,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.namenode.FSNamesystem loadFromDisk(org.apache.hadoop.conf.Configuration)>
BLOCK* allocateBlock:  src .  <*>   <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo saveAllocatedBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>"
Encountered exception setting Rollback Image,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo$Bean getRollingUpgradeStatus()>
BLOCK* NameSystem.getAdditionalBlock:  src  inodeId  fileId  for  clientName ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalBlock(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.util.Set,java.util.List)>"
DIR* NameSystem.append:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInternal(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,java.lang.String,boolean)>"
"DIR* NameSystem.appendFile: src= srcArg , holder= holder , clientMachine= clientMachine ",debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>"
DIR* NameSystem.appendFile: file  <*>  for  holder  at  clientMachine  block  <*>  block size  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock appendFileInt(java.lang.String,java.lang.String,java.lang.String,boolean)>"
Ignoring unknown CryptoProtocolVersion provided by client:  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[])>"
Edits URI  dir  listed multiple times in  dfs.namenode.shared.edits.dir . Ignoring duplicates. ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>"
Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.List getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean)>"
there are no corrupt file blocks.,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>"
list corrupt file blocks returned:  count ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>"
!!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection getStorageDirs(org.apache.hadoop.conf.Configuration,java.lang.String)>"
Get corrupt file blocks returned error:  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.lang.String getCorruptFiles()>
Unexpected safe mode action,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction)>
DIR* NameSystem.renameTo:  srcArg  to  dstArg ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean renameToInt(java.lang.String,java.lang.String,boolean)>"
mkdirs: created directory  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsRecursively(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,long)>"
DIR* NameSystem.mkdirs:  srcArg ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean mkdirsInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)>"
Error while resolving the link :  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean isInSnapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>
"Recovering  lease , src= src ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
DIR* NameSystem.internalReleaseLease: attempt to release a create lock on  src  but file is already closed. ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
DIR* NameSystem.internalReleaseLease: Failed to release lease for file  src . Committed blocks are waiting to be minimally replicated.  Try again later. ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
BLOCK* internalReleaseLease: Removed empty last block and closed file.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
DIR* NameSystem.internalReleaseLease: File  src  has not been closed.  Lease recovery is in progress.  RecoveryId =  <*>  for block  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,java.lang.String)>"
DIR* Namesystem.delete:  <*>  is removed ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInternal(java.lang.String,boolean,boolean,boolean)>"
DIR* NameSystem.delete:  src ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean deleteInt(java.lang.String,boolean,boolean)>"
"DIR* completeFile: request from  holder  to complete inode  fileId ( src ) which is already closed. But, it appears to be  an RPC retry. Returning success ",info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFileInternal(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long)>"
DIR* NameSystem.completeFile:  srcArg  for  holder ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
DIR* completeFile:  srcArg  is closed by  holder ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>"
BLOCK* checkFileProgress:  block  has not reached minimal replication  $i ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
BLOCK* checkFileProgress:  <*>  has not reached minimal replication  $i ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean checkFileProgress(org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>"
BLOCK* NameSystem.abandonBlock:  b of file  src ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
BLOCK* NameSystem.abandonBlock:  b  is removed from pendingCreates ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>"
"NameNode is being shutdown, exit SafeModeMonitor thread",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeMonitor: void run()>
msg  \n <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void reportStatus(java.lang.String,boolean)>"
STATE* Leaving safe mode after  <*>  secs ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
STATE* Safe mode is OFF,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
STATE* Network topology has  <*>  racks and  <*>  datanodes ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
STATE* UnderReplicatedBlocks has  <*>  blocks ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void leave()>
Adjusting block totals from  <*> / <*>  to  <*> / <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void adjustBlockTotals(int,int)>"
dfs.namenode.safemode.threshold-pct =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
dfs.namenode.safemode.min.datanodes =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
dfs.namenode.safemode.extension     =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
"The threshold value should\'t be greater than , threshold:  <*> ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
Exception in NameNodeResourceMonitor: ,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
NameNode low on available disk space.  Entering safe mode. ,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
NameNode low on available disk space.  Already in safe mode. ,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: void run()>
Swallowing exception in  <*> : ,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
NameNode rolling its own edit log because number of edits in open segment exceeds threshold of  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
"<*>  was interrupted, exiting ",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller: void run()>
Ignoring exception in LazyPersistFileScrubber:,error,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
"LazyPersistFileScrubber was interrupted, exiting",info,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
Removing lazyPersist file  <*>  with no replicas. ,warn,<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void clearCorruptLazyPersistFiles()>
Checking file  f ,debug,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Image file  f  has improperly formatted  transaction ID ,error,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
No version file in  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Unable to determine the max transaction ID seen by  sd ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Unable to inspect storage directory  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Found image file at  f  but storage directory is  not configured to contain images. ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
This is a rare failure scenario!!!,error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
Image checkpoint time  <*>  > edits checkpoint time  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,error,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestImages()>
"Name checkpoint time is newer than edits, not loading edits.",debug,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestEditsFiles()>
Performing recovery in  <*>  and  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>
Unable to delete dir  <*>  before rename ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>
Unrecognized section  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void loadInternal(java.io.RandomAccessFile,java.io.FileInputStream)>"
Loaded FSImage in  <*>  seconds. ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: void load(java.io.File)>
Loading  <*>  INodes. ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader: void loadINodeSection(java.io.InputStream)>
Will rename reserved path  <*>  to  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: void setRenameReservedMapInternal(java.lang.String)>
Upgrade process renamed reserved path  oldPath  to  path ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: java.lang.String renameReservedPathsOnUpgrade(java.lang.String,int)>"
Renamed root path .reserved to  renameString ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat: byte[] renameReservedRootComponentOnUpgrade(byte[],int)>"
Saving image file  newFile  using  compression ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>"
Image file  newFile  of size  <*>  bytes saved in  <*>  seconds. ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: void save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression)>"
Renaming reserved path  <*>  to  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFullNameINodes(long,java.io.DataInput,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>"
Number of files under construction =  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void loadFilesUnderConstruction(java.io.DataInput,boolean,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter)>"
load last allocated InodeId from fsimage: <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Old layout version doesn\'t have inode id. Will assign new id for each inode.,debug,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Upgrading to sequential block IDs. Generation stamp for new blocks set to  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Loading image file  curFile  using  stampAtIdSwitch ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Number of files =  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Image file  curFile  of size  <*>  bytes loaded in  <*>  seconds. ,info,<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Caught interrupted exception while waiting for thread  <*>  to finish. Retrying join ,error,<org.apache.hadoop.hdfs.server.namenode.FSImage: void waitForThreads(java.util.List)>
BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# ,error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>"
BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void updateCountForQuotaRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.Quota$Counts)>"
editLog must be initialized,<init>,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>"
Save namespace ...,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)>"
renaming   <*>  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean)>"
Unable to rename checkpoint in  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>"
Unable to rename checkpoint in  sd ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameCheckpoint(long,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,boolean)>"
Reloading namespace from  file ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void reloadFromImageFile(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
Unable to purge old storage  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImage: void purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)>
Planning to load image :\n imageFile ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImageFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Loaded image for txid  <*>  from  curFile ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void loadFSImage(java.io.File,org.apache.hadoop.io.MD5Hash,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean)>"
Allocated new BlockPoolId:  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: void format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String)>"
Finalizing upgrade for local dirs.  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void finalizeUpgrade(boolean)>
End checkpoint at txid  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void endCheckpoint(org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)>
Edits log must not be open.,<init>,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Failed to move aside pre-upgrade storage in image directory  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Starting upgrade of local storage directories.\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Can perform rollback for  sd ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Can perform rollback for shared edit log.,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Rolling back storage directory  <*> .\n   new LV =  <*> ; new CTime =  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage: void doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
Unable to delete cancelled checkpoint in  sd ,warn,<org.apache.hadoop.hdfs.server.namenode.FSImage: void deleteCancelledCheckpoint(long)>
Name node  <*>  has newer image layout version: LV =  <*>  cTime =  <*> . Current version: LV =  <*>  cTime =  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
Start checkpoint at txid  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: org.apache.hadoop.hdfs.server.protocol.NamenodeCommand startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
About to load edits:\n   <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Reading  editIn  expecting start txid # <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Storage directory  <*>  is not formatted. ,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Data dir states:\n   <*> ,trace,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
<*> toAtLeastTxId recovery ,closeAllStreams,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Planning to load edit log stream:  elis ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Failed to load image from  i$ ,error,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
No edit log streams selected.,info,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
dfs.namenode.max.op.size ,setMaxOpSize,"<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Unable to save image for  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>
Cancelled image saving for  <*> :  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver: void run()>
Summary of operations loaded from edit log:\n  ,append,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: void dumpOpCounts(java.util.EnumMap)>
Caught exception after reading  numValid  ops from  in  while determining its valid length. Position was  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"After resync, position is  <*> ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation validateEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
Caught exception after reading  numValid  ops from  in  while determining its valid length. Position was  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
"After resync, position is  <*> ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
We failed to read txId  expectedTxId ,editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"There appears to be a gap in the edit log.  We expected txid  expectedTxId , but got txid  <*> . ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"There appears to be an out-of-order edit in the edit log.  We expected txid  expectedTxId , but got txid  <*> . ",editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Failed to apply edit log operation  <*> : error  <*> ,editLogLoaderPrompt,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Encountered exception on operation  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
replaying edit log:  deltaTxId / <*>  transactions completed. ( <*> %) ,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Stopped at OP_START_ROLLING_UPGRADE for rollback.,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Acquiring write lock to replay edit log,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
"op= <*> , startOpt= startOpt , numEdits= numEdits , totalEdits= <*> ",trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
replaying edit log finished,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
replaying edit log finished,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
Stopped reading edit log at  <*> / <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>"
<*> :  <*>  numblocks :  <*>  clientHolder  <*>  clientMachine  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
Reopening an already-closed file for append,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
<*> :  <*>  numblocks :  <*>  clientHolder  <*>  clientMachine  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
<*> :  <*>  numblocks :  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
<*> :  <*>  new block id :  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
replaying edit log:  op ,trace,"<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>"
Starting log segment at  segmentTxId ,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void startLogSegment(long,boolean)>"
Removing backup journal  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void releaseBackupStream(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>
Backup node  bnReg  re-registers ,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
Registering new backup node:  bnReg ,info,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)>"
Could not sync enough journals to persistent storage due to  <*> .  Unsynced transactions:  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>
Could not sync enough journals to persistent storage. Unsynced transactions:  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void logSync()>
"Initializing shared journals for READ, already open for READ",warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initSharedJournalsForRead()>
No edits directories configured!,error,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void initJournals(java.util.List)>
Ending log segment  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void endCurrentLogSegment(boolean)>
Closing log when already closed,debug,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
Error closing journalSet,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
Error closing journalSet,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
All journals failed to abort,warn,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void abortCurrentLogSegment()>
Rolling edit logs,info,<org.apache.hadoop.hdfs.server.namenode.FSEditLog: long rollEditLog()>
"No class configured for  uriScheme ,  <*>  is empty ",warn,"<org.apache.hadoop.hdfs.server.namenode.FSEditLog: java.lang.Class getJournalClass(org.apache.hadoop.conf.Configuration,java.lang.String)>"
FSDirectory.verifyMaxDirItems:  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxDirItems(org.apache.hadoop.hdfs.server.namenode.INode[],int)>"
ERROR in FSDirectory.verifyINodeName,error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void verifyMaxComponentLength(byte[],java.lang.Object,int)>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
DIR* FSDirectory.unprotectedRenameTo:  rename source cannot be the root ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameSource(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void validateRenameDestination(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode)>"
BUG: unexpected exception ,error,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void updateCountNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,int,long,long)>"
DIR* FSNamesystem.concat to  target ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void unprotectedConcat(java.lang.String,java.lang.String[],long)>"
DIR* FSDirectory.renameTo:  src  to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void renameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
Error parsing protocol buffer of EZ XAttr  <*>  dir: <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void addEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields,org.apache.hadoop.hdfs.server.namenode.XAttrFeature)>"
Caching file names occuring more than  <*>  times ,info,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>"
DIR* FSDirectory.unprotectedAddFile: exception when add  path  to the file system ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile unprotectedAddFile(long,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.util.List,java.util.List,short,long,long,long,boolean,java.lang.String,java.lang.String,byte)>"
DIR* addFile:  path  is added ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>"
DIR* addFile: failed to add  path ,info,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INodeFile addFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>"
Could not get full path. Corresponding file might have deleted already.,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.hdfs.server.namenode.INode[] getRelativePathINodes(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)>"
Encryption zone  <*>  does not have a valid path. ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
Could not find encryption XAttr for file  <*>  in encryption zone  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>"
DIR* FSDirectory.unprotectedDelete:  <*>  is removed ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>"
DIR* FSDirectory.delete:  src ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: long delete(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,long)>"
Resolved path is  <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: java.lang.String constructRemainingPath(java.lang.String,byte[][],int)>"
DIR* FSDirectory.unprotectedRenameTo:  src  is renamed to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo:  rename destination cannot be the root ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo:  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>"
DIR* FSDirectory.unprotectedRenameTo:  src  is renamed to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because destination exists ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because destination\'s parent does not exist ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst  because the source can not be removed ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.unprotectedRenameTo: failed to rename  src  to  dst ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRenameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.removeBlock:  path  with  block  block is removed from the file system ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean unprotectedRemoveBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>"
DIR* FSDirectory.renameTo:  src  to  dst ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean renameTo(java.lang.String,java.lang.String,long)>"
DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist ,debug,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>"
DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted ,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)>"
FSDirectory.addChildNoQuotaCheck - unexpected,warn,"<org.apache.hadoop.hdfs.server.namenode.FSDirectory: boolean addLastINodeNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode)>"
this : selecting input streams starting at  fromTxId <*> from among  <*>  candidate file(s) ,debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>"
Recovering unfinalized segments in  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
Deleting zero-length edit log file  elf ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
Moving aside edit log file that seems to have zero transactions  elf ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void recoverUnfinalizedSegments()>
Purging logs older than  minTxIdToKeep ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void purgeLogsOlderThan(long)>
Finalizing edits file  <*>  ->  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void finalizeLogSegment(long,long)>"
Failed to move aside pre-upgrade storage in image directory  <*> ,error,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>
Starting upgrade of edits directory  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void doPreUpgrade()>
"Discard the EditLog files, the given start txid is  startTxId ",info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>
Trash the EditLog file  elf ,info,<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void discardEditLogSegments(long)>
passing over  elf  because it is in progress  and we are ignoring in-progress logs. ,debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
"passing over  elf  because it ends at  <*> , but we only care about transactions  as new as  fromTxId ",debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
selecting edit log stream  elf ,debug,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
got IOException while trying to validate header of  elf .  Skipping. ,error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,boolean)>"
Unable to start log segment  txid  at  <*> :  <*> ,warn,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>"
Edits file  f  has improperly formatted  transaction ID ,error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
In-progress edits file  f  has improperly  formatted transaction ID ,error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
In-progress stale edits file  f  has improperly  formatted transaction ID ,error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List matchEditLogs(java.io.File[],boolean)>"
got IOException while trying to validate header of  elf .  Skipping. ,error,"<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: java.util.List getRemoteEditLogs(long,boolean)>"
Preallocated  total  bytes at the end of  the edit log (offset  <*> ) ,debug,<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void preallocate()>
Nothing to flush,info,<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void flushAndSync(boolean)>
No header found in log,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>
EOF while reading layout flags from log,<init>,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: void init(boolean)>
nextValidOp: got exception while reading  this ,error,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextValidOp()>
skipping  skipAmt  bytes at the end  of edit log  \' <*> \': reached txid  <*>  out of  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
caught exception initializing  this ,error,<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
Nothing to flush,info,<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void flushAndSync(boolean)>
Error connecting to:  <*> ,error,"<org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream: void <init>(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.JournalInfo)>"
BUG: Inconsistent diskspace for directory  <*> . Cached =  <*>  != Computed =  computed ,error,"<org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature: void checkDiskspace(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,long)>"
Cluster console encounters a not handled situtation.,warn,<org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: void getDecommissionNodeClusterState(java.util.Map)>
Exception in doCheckpoint: ,error,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>
Throwable Exception in doCheckpoint: ,error,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()>
 <*> <*>,<init>,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
Checkpointer about to load edits from  <*>  stream(s). ,info,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>"
Checkpoint Period :  <*>  secs  ( <*>  min) ,info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>
"Transactions count is  :  <*> , to trigger checkpoint ",info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void initialize(org.apache.hadoop.conf.Configuration)>
Doing checkpoint. Last applied:  <*> ,debug,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
Unable to roll forward using only logs. Downloading image with txid  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
Loading image with txid  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
Checkpoint completed in  <*>  seconds.  New Image Size:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
Checkpointer got exception,warn,"<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.BackupNode)>"
Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns ,warn,<org.apache.hadoop.hdfs.server.namenode.CheckpointConf: void warnForDeprecatedConfigs(org.apache.hadoop.conf.Configuration)>
Request for token received with no authentication from  <*> ,info,"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Exception while cancelling token. Re-throwing. ,info,"<org.apache.hadoop.hdfs.server.namenode.CancelDelegationTokenServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
removeDirective of  id  successful. ,info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>"
removeDirective of  id  failed:  ,warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)>"
removeCachePool of  poolName  failed:  ,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>
removeCachePool of  poolName  successful. ,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void removeCachePool(java.lang.String)>
Cache report from datanode {} has block {},trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Added block {}  to cachedBlocks,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Added block {} to CACHED list.,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
Removed block {} from PENDING_CACHED list.,trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>"
"Processed cache report from {}, blocks: {}, processing time: {} msecs",debug,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List)>"
modifyDirective of {} successfully applied {}.,info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
modifyDirective of  idString  failed:  ,warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
modifyCachePool of  info  failed:  ,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
modifyCachePool of {} successful; {},info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
Using minimum value {} for {},info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager)>"
addCachePool of  info  failed:  ,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
addCachePool of {} successful.,info,<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CachePoolInfo addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)>
addDirective of {} successful.,info,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
addDirective of  info  failed:  ,warn,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet)>"
Validating directive {} pool maxRelativeExpiryTime {},trace,"<org.apache.hadoop.hdfs.server.namenode.CacheManager: long validateExpiryTime(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,long)>"
Failed to report to name-node.,error,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void stop()>
e . Shutting down. ,error,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Problem connecting to name-node:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode: void registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Incompatible build versions: active name-node BV =  <*> ; backup node BV =  <*> ,fatal,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol)>
Problem connecting to server:  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>
Encountered exception ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo handshake(org.apache.hadoop.conf.Configuration)>
Invalid namespaceID in journal request - expected  <*>  actual  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>
Invalid clusterId in journal request - expected  <*>  actual  <*> ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: void verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo)>
Fenced by  fencerInfo  with epoch  epoch ,info,"<org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.FenceResponse fence(org.apache.hadoop.hdfs.server.protocol.JournalInfo,long,java.lang.String)>"
Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
BackupNode namespace frozen.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
Interrupted waiting for namespace to freeze,warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void waitUntilNamespaceFrozen()>
State transition  <*>  ->  newState ,debug,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState)>
Storage directory  <*>  is not formatted. ,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>
Formatting ...,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void recoverCreateRead()>
NameNode started a new log segment at txid  txid ,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
Stopped applying edits to prepare for checkpoint.,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
"NN started new log segment at txid  txid , but BN had only written up to txid  <*> in the log segment starting at  <*> . Aborting this  log segment. ",warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: void namenodeStartedLogSegment(long)>
"Got journal, state =  <*> ; firstTxId =  firstTxId ; numTxns =  numTxns ",trace,"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void journal(long,int,byte[])>"
data: <*> ,debug,"<org.apache.hadoop.hdfs.server.namenode.BackupImage: void applyEdits(long,int,byte[])>"
 <*> <*>,closeAllStreams,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
 <*> <*>,closeAllStreams,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Logs rolled while catching up to current segment,debug,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Loading edits into backupnode to try to catch up from txid  <*>  to  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream ,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Successfully synced BackupNode with NameNode at txnid  <*> ,info,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Unable to find stream starting with  <*> . This indicates that there is an error in synchronization in BackupImage ,warn,<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
Exiting  <*>  due to an exception ,error,<org.apache.hadoop.hdfs.server.mover.Mover: void main(java.lang.String[])>
namenodes =  namenodes ,info,"<org.apache.hadoop.hdfs.server.mover.Mover: int run(java.util.Map,org.apache.hadoop.conf.Configuration)>"
Failed to get snapshottable directories. Ignore and continue.,warn,<org.apache.hadoop.hdfs.server.mover.Mover$Processor: void getSnapshottableDirs()>
Failed to check the status of  parent . Ignore it and continue. ,warn,"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processRecursively(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus)>"
Failed to list directory  fullPath . Ignore the directory and continue. ,warn,<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processPath(java.lang.String)>
Failed to get the storage policy of file  fullPath ,warn,"<org.apache.hadoop.hdfs.server.mover.Mover$Processor: boolean processFile(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus)>"
"HTTP  <*> :  op ,  path , ugi= ugi <*> ",trace,"<org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>"
unregisterSlot: ShortCircuitRegistry is not enabled.,trace,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)>
removing shm  shm ,debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm)>
this  can\'t register a slot because the  ShortCircuitRegistry is not enabled. ,trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>"
this : registered  blockId  with slot  slotId  (isCached= isCached ) ,trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>"
"created new ShortCircuitRegistry with interruptCheck= <*> , shmPath= <*> ",debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
Disabling ShortCircuitRegistry,debug,<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
createNewMemorySegment: ShortCircuitRegistry is not enabled.,trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>"
createNewMemorySegment: created  <*> ,trace,"<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>"
<*> \n <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: void stopWriter(long)>
writeTo blockfile is  <*>  of size  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>"
writeTo metafile is  <*>  of size  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>"
detachFile failed to delete temporary file  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: void unlinkFile(java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
CopyOnWrite for block  this ,info,<org.apache.hadoop.hdfs.server.datanode.ReplicaInfo: boolean unlinkBlock(int)>
Failed to read next line.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RollingLogsImpl$Reader: java.lang.String next()>
Failed to delete block file  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>
Failed to delete meta file  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: void deleteSavedFiles()>
LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
LazyWriter failed to create  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
Shutting down all async lazy persist service threads,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
All async lazy persist service threads have been shut down,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
AsyncLazyPersistService has already shut down.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void shutdown()>
LazyWriter failed to async persist RamDisk block pool id:  <*> block Id:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask: void run()>
"The volume list has been changed concurrently, retry to remove volume:  target ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Volume  target  does not exist or is removed by others. ,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Removed volume:  target ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Total time to add all replicas to map:  arr$# ms ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>"
"The volume list has been changed concurrently, retry to remove volume:  newVolume ",debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Added new volume:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Total time to scan all replicas for block pool  bpid :  arr$# ms ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>"
Scanning block pool  <*>  on volume  <*> ... ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
Time taken to scan block pool  <*>  on  <*> :  timeTaken ms ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
Caught exception while scanning  <*> . Will throw later. ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2: void run()>
Adding replicas to map for block pool  <*>  on volume  <*> ... ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
Time to add replicas to map for block pool  <*>  on volume  <*> :  timeTaken ms ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
Caught exception while adding replicas from  <*> . Will throw later. ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: void run()>
Block  blockFile  does not have a metafile! ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: long getGenerationStampFromFile(java.io.File[],java.io.File)>"
Block  b  unfinalized and removed.  ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void unfinalizeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"truncateBlock: blockFile= blockFile , metaFile= metaFile , oldlen= oldlen , newlen= newlen ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void truncateBlock(java.io.File,java.io.File,long,long)>"
Removing block pool  bpid ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdownBlockPool(java.lang.String)>
FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void shutdown()>
Removing  <*>  from FsDataset. ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void removeVolumes(java.util.Collection)>
Registered FSDatasetState MBean,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>
Error registering FSDatasetState MBean,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void registerMBean(java.lang.String)>
Failed to save replica  <*> . re-enqueueing it. ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onFailLazyPersist(java.lang.String,long)>"
LazyWriter: Finish persisting RamDisk block:  block pool Id:  bpId  block id:  blockId  to block file  <*>  and meta file  <*>  on target volume  targetVolume ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>"
Failed to delete replica  <*> : ReplicaInfo not found. ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
"bpid  has some block files, cannot delete unless forced ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void deleteBlockPool(java.lang.String,boolean)>"
Removing replica  bpid : <*>  on failed volume  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>
Removed  removedBlocks  out of  totalBlocks (took  mlsec  millisecs) ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkDataDir()>
Deleted a metadata file without a block  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Removed block  blockId  from memory with missing block file on the disk ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Deleted a metadata file for the deleted block  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Added missing block to memory  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Failed to delete  diskFile . Will retry on next scan ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Block file in volumeMap  <*>  does not exist. Updating it to the file found during scan  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Updating generation stamp for block  blockId  from  <*>  to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Metadata file in memory  <*>  does not match file found by scan  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Updating generation stamp for block  blockId  from  <*>  to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Updating size of block  blockId  from  <*>  to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Reporting the block  corruptBlock  as corrupt due to length mismatch ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
Failed to repot bad block  corruptBlock ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void checkAndUpdate(java.lang.String,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>"
"Failed to cache block with id  blockId , pool  bpid : ReplicaInfo not found. ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Failed to cache block with id  blockId , pool  bpid : replica is not finalized; it is in state  <*> ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
"Failed to cache block with id  blockId , pool  bpid : volume not found. ",warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
Failed to cache block with id  blockId : volume was not an instance of FsVolumeImpl. ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
Caching not supported on block with id  blockId  since the volume is backed by RAM. ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void cacheBlock(java.lang.String,long)>"
Renaming  <*>  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void bumpReplicaGS(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,long)>"
Changing meta file offset of block  b  from  <*>  to  newPos ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)>"
"Added volume -  <*> , StorageType:  <*> ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>"
Caught exception when adding  <*> . Will throw later. ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List)>"
"Added volume -  <*> , StorageType:  <*> ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addVolume(java.util.Collection,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>"
Adding block pool  bpid ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)>"
Data node cannot fully support concurrent reading and writing without native code extensions on Windows.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void <clinit>()>
"initReplicaRecovery:  block , recoveryId= recoveryId , replica= <*> ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
initReplicaRecovery: update recovery id for  block  from  <*>  to  recoveryId ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
initReplicaRecovery: changing replica state for  block  from  <*>  to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo initReplicaRecovery(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long,long)>"
Recover RBW replica  b ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>"
Recovering  rbw ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long)>"
Recover failed append to  b ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline recoverAppend(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
Unable to stop existing writer for block  b  after  writerStopMs  miniseconds. ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline createTemporary(org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>"
"Convert  b  from Temporary to RBW, visible length= <*> ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Appending to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline append(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
Renaming  <*>  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
"Renaming  <*>  to  $u , file length= <*> ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
Cannot move meta file  <*> back to the finalized directory  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>"
"updateReplica:  oldBlock , recoveryId= recoveryId , length= newlength , replica= <*> ",info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String updateReplicaUnderRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
Recover failed close  b ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.lang.String recoverClose(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long)>"
Copied  srcMeta  to  <*>  and calculated checksum ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>"
Copied  srcFile  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(long,long,java.io.File,java.io.File,java.io.File)>"
"blockId= blockId , f= <*> ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File validateBlockFile(java.lang.String,long)>"
addFinalizedBlock: Moved  <*>  to  <*>  and  srcfile  to  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File moveBlockFiles(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
No file exists for block:  b ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
Not able to delete the block file:  blockFile ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
Not able to delete the meta block file:  metaFile ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: boolean delBlockFromDisk(java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.Block)>"
"LazyWriter was interrupted, exiting",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>
Ignoring exception in LazyWriter:,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void run()>
Evicting block  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void evictBlocks()>
LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
Exception saving replica  block ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
Failed to save replica  block . re-enqueueing it. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
Failed to save replica  block . re-enqueueing it. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
"Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"Cancelling caching for block with id {}, pool {}.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"{} is anchored, and can\'t be uncached now.  Scheduling it for uncaching in {} ",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
{} has been scheduled for immediate uncaching.,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"Block with id {}, pool {} does not need to be uncached, because it is in state {}.",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>"
"Block with id {}, pool {} already exists in the FsDatasetCache with state {}",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>"
"Initiating caching for Block with id {}, pool {}",debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>"
Uncaching of {} completed. usedBytes = {},debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>
Deferred uncaching of {} completed. usedBytes = {},debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>
Uncaching {} now that it is no longer in use by any clients.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
Replica {} still can\'t be uncached because some clients continue to use it.  Will wait for {},info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
Forcibly uncaching {} after {} because client(s) {} refused to stop using it.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Successfully cached {}.  We are now caching {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of {} was aborted.  We are now caching only {} bytes in total.,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> : failed to find backing  files. ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> : could not reserve  <*>  more bytes in the cache:  dfs.datanode.max.locked.memory  of  <*>  exceeded. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> : Underlying blocks are not backed by files. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> : failed to open file ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> : checksum verification failed. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Failed to cache  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Caching of  <*>  was cancelled. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
Shutting down all async disk service threads,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
All async disk service threads have been shut down,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
AsyncDiskService has already shut down.,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void shutdown()>
Scheduling  <*>  file  blockFile  for deletion ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: void deleteAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl,java.io.File,java.io.File,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
Deleted  <*>   <*>  file  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>
Unexpected error trying to  <*>  block  <*>   <*>  at file  <*> . Ignored. ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: void run()>
Moving files  <*>  and  <*>  to trash. ,debug,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
Failed to create trash directory  <*> ,error,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
sync_file_range error,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2: void run()>
Failed to delete old dfsUsed file in  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>
Failed to write dfsUsed to  $u ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void saveDfsUsed()>
Recovered  <*>  replicas from  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)>"
Failed to delete block file  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
Failed to delete meta file  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
Failed to delete restart meta file:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: void addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean)>"
resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: org.apache.hadoop.hdfs.server.datanode.ReplicaInfo selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>"
Cached dfsUsed found for  <*> :  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: long loadDfsUsed()>
Moved  blockFile  to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
Moved  metaFile  to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: java.io.File activateSavedReplica(org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>"
Failed to mkdirs  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
Failed to move meta file from  file  to  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
Failed to move block file from  <*>  to  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
Failed to move  <*>  to  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: int moveLazyPersistReplicasToFinalized(java.io.File)>
"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold =  <*> ,  dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction  =  <*> ",info,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is greater than . but should be in the range . - .,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction is less than . so volumes with less available disk space will receive more block allocations,warn,<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: void setConf(org.apache.hadoop.conf.Configuration)>
All volumes are within the configured free space balance threshold. Selecting  <*>  for write of block size  replicaSize ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
Volumes are imbalanced. Selecting  <*>  from high available space volumes for write of block size  replicaSize ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
Volumes are imbalanced. Selecting  <*>  from low available space volumes for write of block size  replicaSize ,debug,"<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi chooseVolume(java.util.List,long)>"
dfs.blockreport.initialDelay is greater than dfs.blockreport.intervalMsec. Setting initial delay to  msec:,info,<org.apache.hadoop.hdfs.server.datanode.DNConf: void <init>(org.apache.hadoop.conf.Configuration)>
Periodic Directory Tree Verification scan starting at  firstScanTime  with interval  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void start()>
interrupted while waiting for masterThread to terminate,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
interrupted while waiting for reportCompileThreadPool to terminate,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
"DirectoryScanner: shutdown has been called, but periodic scanner not started",warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
DirectoryScanner: shutdown has been called,warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void shutdown()>
Exception during DirectoryScanner execution - will continue next cycle,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
System Error during DirectoryScanner execution - permanently terminating periodic scanner,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
this cycle terminating immediately because \'shouldRun\' has been deactivated,warn,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: void run()>
Error compiling report,error,<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: java.util.Map getDiskReport()>
Exception occured while compiling report: ,warn,"<org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler: java.util.LinkedList compileReport(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,java.util.LinkedList)>"
Got error when sending OOB message.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>
Interrupted when sending OOB message.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void sendOOBToPeers()>
<*> :DataXceiverServer: Exiting due to:  ,error,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
Shutting down DataXceiverServer before restart,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
<*> :DataXceiverServer:  ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
<*> :DataXceiverServer:  ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
DataNode is out of memory. Will retry in  seconds.,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
<*>  :DataXceiverServer: close exception ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void run()>
<*> :DataXceiverServer.kill():  ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void kill()>
Closing all peers.,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer: void closeAllPeers()>
Balancing bandwith is  bandwidth  bytes/s ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>"
Number threads for balancing is  maxThreads ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler: void <init>(long,int)>"
"opWriteBlock: stage= stage , clientname= clientname \n  block  = block , newGs= latestGenerationStamp , bytesRcvd=[ minBytesRcvd ,  maxBytesRcvd ] \n  targets= <*> ; pipelineSize= pipelineSize , srcDataNode= srcDataNode ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"isDatanode= isDatanode , isClient= isClient , isTransfer= isTransfer ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
writeBlock receive buf size  <*>  tcp no delay  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Connecting to datanode  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
<*> :Exception transfering block  block  to mirror  <*> :  <*> ,error,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Receiving  block  src:  <*>  dest:  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Datanode  <*>  got response for connect ack   from downstream datanode with firstbadlink as  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
<*> :Exception transfering  block  to mirror  <*> - continuing without the mirror ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Datanode  <*>  forwarding connect ack to upstream firstbadlink is  firstBadLink ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Received  block  src:  <*>  dest:  <*>  of size  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
opWriteBlock  block  received exception  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
TRANSFER: send close-ack,trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Sending OOB to peer:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void sendOOB()>
<*> :Number of active connections is:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
Cached  <*>  closing after  opsProcessed  ops ,debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> :Number of active connections is:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> :Number of active connections is:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> :Number of active connections is:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> :DataXceiver error processing  <*>  operation   src:  <*>  dst:  <*> ,error,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
Failed to read expected encryption handshake from client at  <*> . Perhaps the client  is running an older version of Hadoop which does not support  encryption ,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> ;  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<*> :DataXceiver error processing  <*>  operation   src:  <*>  dst:  <*> ,trace,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to send success response back to the client.  Shutting down socket for  <*> . ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to send success response back to the client.  Shutting down socket for  <*> . ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to send success response back to the client.  Shutting down socket for  <*> . ,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Failed to shut down socket in error handler,warn,<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitShm(java.lang.String)>
Unregistering  registeredSlotId  because the  requestShortCircuitFdsForRead operation failed. ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
Unregistering  registeredSlotId  because the  requestShortCircuitFdsForRead operation failed. ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
Reading receipt verification byte for  slotId ,trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
Receipt verification is not enabled on the DataNode.  Not verifying  slotId ,trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>"
Connecting to datanode  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
"Moved  block  from  <*> , delHint= delHint ",info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
opReplaceBlock  block  received exception  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
Not able to receive block  <*>  from  <*>  because threads  quota is exceeded. ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
Error writing reply back to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
Error writing reply back to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
Error reading client status response. Will close connection.,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
opReadBlock  block  received exception  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
<*> :Ignoring exception while serving  block  to  <*> ,trace,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
Client  <*>  did not send a valid status code after reading.  Will close connection. ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
<*> :Got exception while serving  block  to  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
Not able to copy block  <*>   to  <*>  because threads  quota is exceeded. ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
Copied  block  to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
opCopyBlock  block  received exception  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
Checking block access token for block \' <*> \' with mode \' mode \' ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
"Block token verification failed: op= op , remoteAddress= <*> , message= <*> ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
"block= block , bytesPerCRC= <*> , crcPerBlock= crcPerBlock , md= <*> ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
Number of active connections is:  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void <init>(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataXceiverServer)>"
Restored trash for bpid  bpid ,info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void restoreTrash(java.lang.String)>
Unexpectedly low genstamp on  <*> . ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
Unexpectedly short length on  <*> . ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
Unexpectedly short length on  <*> . ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
Discarding  <*> . ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList)>"
DataNode version:  <*>  and NameNode layout version:  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Invalid directory in:  <*> :  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void makeBlockPoolDataDir(java.util.Collection,org.apache.hadoop.conf.Configuration)>"
There are  <*>  duplicate block  entries within the same volume. ,error,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void linkBlocks(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.io.File,int,org.apache.hadoop.fs.HardLink)>"
Enabled trash for bpid  bpid ,info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void enableTrash(java.lang.String)>
Updating layout version from  <*>  to  <*>  for storage  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Upgrading storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Upgrade of  <*>  is complete ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Layout version rolled back to  <*>  for storage  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Rollback of  <*>  is complete ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Finalizing upgrade for storage directory  <*> .\n   cur LV =  <*> ; cur CTime =  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataStorage: void doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
Storage directory is in use.,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.datanode.DataStorage$VolumeBuilder prepareVolume(org.apache.hadoop.hdfs.server.datanode.DataNode,java.io.File,java.util.List)>"
Storage directory  dataDir  does not exist ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Storage directory  dataDir  is not formatted for  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Storage directory  dataDir  has already been used. ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Failed to add storage for block pool:  <*>  :  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: java.util.List addStorageLocations(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Generated new storageID  <*>  for directory  <*> <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataStorage: boolean createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,boolean)>"
Finalize upgrade for  <*>  failed ,error,<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>
Finalize upgrade for  <*>  is complete ,info,<org.apache.hadoop.hdfs.server.datanode.DataStorage$1: void run()>
Failed to transfer block  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.hdfs.StorageType[][])>"
Can\'t send invalid block  block ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>"
<*>  Starting thread to transfer  block  to  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[])>"
"block= <*> , (length= <*> ), syncList= syncList ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>"
"Failed to updateBlock (newblock= $u , datanode= <*> ) ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void syncBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock,java.util.List)>"
Started plug-in  p ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>
ServicePlugin  p  could not be started ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void startPlugins(org.apache.hadoop.conf.Configuration)>
Starting DataNode with maxLockedMemory =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
dnUserName =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
supergroup =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void startDataNode(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
shutdownDatanode command received (upgrade= forUpgrade ). Shutting down Datanode... ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdownDatanode(boolean)>
Stopped plug-in  p ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
"Waiting for threadgroup to exit, active threads is  <*> ",info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Shutdown complete.,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
ServicePlugin  p  could not be stopped ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Exception shutting down DataNode,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Received exception in BlockPoolManager#shutDownAll: ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Exception when unlocking storage:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
Exception in secureMain,fatal,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Exiting Datanode,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
"Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void registerBlockPoolWithSecretManager(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String)>"
Adding new volumes:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
Storage directory is loaded:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
Deactivating volumes:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void refreshVolumes(java.lang.String)>
Recovery for replica  <*>  on data-node  id  is already in progress. Recovery id =  <*>  is aborted. ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>
Failed to obtain replica info for block (= <*> ) from datanode (= id ) ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void recoverBlock(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>
Reconfiguring  property  to  newVal ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void reconfigurePropertyImpl(java.lang.String,java.lang.String)>"
Cannot find BPOfferService for reporting block receiving for bpid= <*> ,error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivingBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
Cannot find BPOfferService for reporting block received for bpid= <*> ,error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeReceivedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>"
Cannot find BPOfferService for reporting block deleted for bpid= <*> ,error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void notifyNamenodeDeletedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)>"
"who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void logRecoverBlock(java.lang.String,org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)>"
Received exception in Datanode#join:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void join()>
Setting up storage: nsid= <*> ;bpid= <*> ;lv= <*> ;nsInfo= nsInfo ;dnuuid= <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initStorage(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Opened IPC server at  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initIpcServer(org.apache.hadoop.conf.Configuration)>
Periodic Directory Tree Verification scan is disabled because  reason#_ ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDirectoryScanner(org.apache.hadoop.conf.Configuration)>
Opened streaming server at  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>
Listening on UNIX domain socket:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataXceiver(org.apache.hadoop.conf.Configuration)>
Periodic Block Verification scan disabled because  reason#_ ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void initDataBlockScanner(org.apache.hadoop.conf.Configuration)>
DataNode.handleDiskError: Keep Running:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>
DataNode is shutting down:  errMsgr ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleDiskError(java.lang.String)>
"deleteBlockPool command received for block pool  blockPoolId , force= force ",info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>"
"The block pool  blockPoolId  is still running, cannot be deleted. ",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void deleteBlockPool(java.lang.String,boolean)>"
Cannot find BPOfferService for reporting block received for bpid= <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void closeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String)>"
Got:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkReadAccess(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Starting CheckDiskError Thread,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDiskErrorAsync()>
Generated and persisted new Datanode UUID  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDatanodeUuid()>
Got:  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
File descriptor passing is enabled.,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Configured hostname is  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
File descriptor passing is disabled because  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>"
Connecting to datanode  <*>  addr= <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>"
getBlockLocalPathInfo successful block= block  blockfile  <*>  metafile  <*> ,trace,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
getBlockLocalPathInfo for block= block  returning null ,trace,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>"
"Although short-circuit local reads are configured, they are disabled because you didn\'t configure dfs.domain.socket.path",warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.net.DomainPeerServer getDomainPeerServer(org.apache.hadoop.conf.Configuration,int)>"
Failed to initialize storage directory  locationString . Exception details:  <*> ,error,<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>
Failed to initialize storage directory  locationString . Exception details:  <*> ,error,<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List getStorageLocations(org.apache.hadoop.conf.Configuration)>
Invalid dfs.datanode.data.dir  <*>  :  ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.util.List checkStorageLocations(java.util.Collection,org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker)>"
requestShortCircuitFdsForRead failed,debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.FileInputStream[] requestShortCircuitFdsForRead(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,int)>"
"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.",error,"<org.apache.hadoop.hdfs.server.datanode.DataNode: boolean parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration)>"
Connecting to datanode  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
<*> : close-ack= <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
<*> : Transmitted  <*>  (numBytes= <*> ) to  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
<*> :Failed to transfer  <*>  to  <*>  got  ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
"<*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> ",debug,"<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)>"
InterruptedException in check disk error thread,debug,<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>
Unexpected exception occurred while checking disk error   <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$6: void run()>
recoverBlocks FAILED:  b ,warn,<org.apache.hadoop.hdfs.server.datanode.DataNode$4: void run()>
Block Pool  <*>  is not alive ,warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void run()>
Removed bpid= blockPoolId  from blockPoolScannerMap ,info,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void removeBlockPool(java.lang.String)>
No block pool scanner found for block pool id:  poolId ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
No block pool scanner found for block pool id:  poolId ,warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void deleteBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.Block)>"
"Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> ",info,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlockPool(java.lang.String)>
No block pool scanner found for block pool id:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
"No block pool is up, going to wait",warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>
Received exception:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner getNextBPScanner(java.lang.String)>
Periodic block scanner is not running. Please check the datanode log if this is unexpected.,append,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Periodic block scanner is not running,warn,"<org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Error reporting an error to NameNode  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void trySendErrorReport(int,java.lang.String)>"
<*> : scheduling an incremental block report. ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>
<*> : scheduling a full block report. ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions)>
BPOfferService  this  interrupted while  stateString ,info,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void sleepAndLogInterrupts(int,java.lang.String)>"
Initialization failed for  this   <*> ,error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Exception in BPOfferService for  this ,error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Initialization failed for  this . Exiting.  ,fatal,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
this  starting to offer service ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Ending block pool service for:  this ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Ending block pool service for:  this ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Unexpected exception in block pool  this ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Ending block pool service for:  this ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Ending block pool service for:  this ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void run()>
Failed to report bad block  block  to namenode :   Exception ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void reportBadBlocks(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,org.apache.hadoop.hdfs.StorageType)>"
this  beginning handshake with NN ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
Problem connecting to server:  <*>  : <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
Problem connecting to server:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
Block pool  this  successfully registered with NN ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void register()>
For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
Took  <*> ms to process  <*>  commands from NN ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
BPOfferService for  this  interrupted ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
this  is shutting down ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
RemoteException in offerService,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
IOException in offerService,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void offerService()>
Invalid BlockPoolId  <*>  in HeartbeatResponse. Expected  <*> ,error,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void handleRollingUpgradeStatus(org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse)>
Reported NameNode version \' <*> \' does not match  DataNode version \' <*> \' but is within acceptable  limits. Note: This is normal during a rolling upgrade. ,info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: void checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
this  received versionRequest response:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
Problem connecting to server:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
Problem connecting to server:  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
Sending heartbeat with  <*>  storage reports from service actor:  this ,debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse sendHeartBeat()>
Sending cacheReport from service actor:  this ,debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing ,debug,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
"<*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>
"<*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  brCreateCost  msec to generate and  brSendCost  msecs for RPC and NN processing.  Got back  <*> . ",info,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: java.util.List blockReport()>
Error processing datanode Command,warn,<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: boolean processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[])>
Namenode  actor  trying to claim ACTIVE state with  txid= <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
Acknowledging ACTIVE Namenode  actor ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
Namenode  actor  relinquishing ACTIVE state with  txid= <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
NN  actor  tried to claim ACTIVE state at txid= <*>  but there was already a more recent claim at txid= <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)>"
Couldn\'t report bad block  block  to  actor ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: void reportRemoteBadBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>"
DatanodeCommand action from standby: DNA_ACCESSKEYUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
Got a command from standby NN - ignoring command: <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
Unknown DatanodeCommand action:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_CACHE for  <*>  of [ <*> ] ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_UNCACHE for  <*>  of [ <*> ] ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
Got finalize command for block pool  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_ACCESSKEYUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
Updating balance throttler bandwidth from  <*>  bytes/s  to:  <*>  bytes/s. ,info,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
Unknown DatanodeCommand action:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BPOfferService: boolean processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)>"
 Could not read or failed to veirfy checksum for data at offset  <*>  for block  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void readChecksum(byte[],int,int)>"
Unable to drop cache on file close,warn,<org.apache.hadoop.hdfs.server.datanode.BlockSender: void close()>
Bumping up the client provided block\'s genstamp to latest  <*>  for block  block ,debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
"block= block , replica= <*> ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
replica= <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
Could not find metadata file for  block ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
<*> :sendBlock() :  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>"
BlockSender.sendChunks() exception: ,error,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Failed to send data:  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
Failed to send data:,trace,"<org.apache.hadoop.hdfs.server.datanode.BlockSender: int sendPacket(java.nio.ByteBuffer,int,java.io.OutputStream,boolean,org.apache.hadoop.hdfs.util.DataTransferThrottler)>"
report corrupt  <*>  from datanode  <*>  to namenode ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
Checksum error in block  <*>  from  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
Failed to report bad  <*>  from datanode  <*>  to namenode ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer)>"
Shutting down for restart ( <*> ). ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
Exception for  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
Failed to delete restart meta file:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
<*> \n <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
Failed to delete restart meta file:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
<*> \n <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
Failed to delete restart meta file:  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
<*> \n <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void receiveBlock(java.io.DataOutputStream,java.io.DataInputStream,java.io.DataOutputStream,java.lang.String,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean)>"
Slow manageWriterOsCache took  <*> ms (threshold= <*> ms) ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>
Error managing cache for writer of block  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void manageWriterOsCache(long)>
<*> :Exception writing  <*>  to mirror  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void handleMirrorOutError(java.io.IOException)>
"Slow flushOrSync took  <*> ms (threshold= <*> ms), isSync: isSync , flushTotalNanos= flushTotalNanos ns ",warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void flushOrSync(boolean)>
"<*> :  block \n  isClient  = <*> , clientname= clientname \n  isDatanode= <*> , srcDataNode= srcDataNode \n  inAddr= inAddr , myAddr= myAddr \n  cachingStrategy =  cachingStrategy ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
Could not get file descriptor for outputstream of class  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
IOException in BlockReceiver constructor. Cause is ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean)>"
"computePartialChunkCrc for  <*> : sizePartialChunk= sizePartialChunk , block offset= blkoff# , metafile offset= ckoff ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>"
Read in partial CRC chunk from disk for  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>"
Receiving one packet for block  <*> :  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
Receiving an empty packet or the end of the block  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
receivePacket for  <*> : previous write did not end at the chunk boundary.  onDiskLen= <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
"Writing out partial crc for data len  <*> , skip=  ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
Slow BlockReceiver write packet to mirror took  duration_ ms (threshold= <*> ms) ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
Slow BlockReceiver write data to disk cost: <*> ms (threshold= <*> ms) ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
A packet was last sent  diff  milliseconds ago. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: boolean packetSentInTime()>
Cannot send OOB response  ackStatus . Responder not running. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
Sending an out of band ack of type  ackStatus ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
"<*> , replyAck= $u ",debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
"Slow PacketResponder send ack to upstream took  <*> ms (threshold= <*> ms),  <*> , replyAck= $u ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
<*>  got  $u ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
Calculated invalid ack time:  oobStatus ns. ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
Relaying an out of band ack of type  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
<*> : Thread is interrupted. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
<*>  terminating ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
The downstream error might be due to congestion in upstream including this node. Propagating the error: ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
IOException in BlockReceiver.run(): ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
Received  <*>  size  <*>  from  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void finalizeBlock(long)>
<*> : enqueue  <*> ,debug,"<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
<*> : closing ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void close()>
Created  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>
<*>  already exists. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void setRollingUpgradeMarkers(java.util.List)>
Restoring trash failed for storage directory  sd ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void restoreTrash()>
Analyzing storage directories for bpid  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Formatting block pool  <*>  directory  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Upgrade of block pool  <*>  at  <*>  is complete ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doUpgrade(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Restored  <*>  block files from trash. ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doTransition(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Rollback of  <*>  is complete ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>"
Finalizing upgrade for storage directory  <*> .\n   cur LV =  <*> ; cur CTime =  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void doFinalize(java.io.File)>
Deleting  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>
Failed to delete  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: void clearRollingUpgradeMarkers(java.util.List)>
Block pool storage directory  dataDir  does not exist ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Block pool storage directory  dataDir  is not formatted for  <*> ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Formatting ...,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Failed to analyze storage directories for block pool  <*> ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.util.List loadBpStorageDirectories(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Restoring  blockFile  to  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: java.lang.String getRestoreDirectory(java.io.File)>
Not overwriting  $u  with smaller file from  trash directory. This message can be safely ignored. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: int restoreBlockFilesFromTrash(java.io.File)>
Finalize upgrade for  <*>  failed. ,error,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>
Finalize upgrade for  <*>  is complete. ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1: void run()>
<*> Verification succeeded for  block ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
block  is no longer in the dataset ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Verification failed for  block  - may be due to race with write ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
<*> Verification failed for  block ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void verifyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Starting a new period : work left in prev period :  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void startNewPeriod()>
Starting to scan blockpool:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
"All remaining blocks were processed recently, so this run is complete",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
Done scanning block pool:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
Done scanning block pool:  <*> ,debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
RuntimeException during BlockPoolScanner.scan(),warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void scan()>
Received exception: ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void rollVerificationLogs()>
Reporting bad  block ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Cannot report bad  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void handleScanFailure(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Adding an already existing block  block ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void addBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid ,info,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>"
Could not open verfication log. Verification times are not stored.,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi,org.apache.hadoop.conf.Configuration)>"
"Skipping scan since bytesLeft= <*> , Start= <*> , period= <*> , now= <*>   <*> ",debug,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean workRemainingInCurrentPeriod()>
Failed to read previous verification times.,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: boolean assignInitialVerificationTimes()>
Failed to close the appender of  <*> ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void close()>
"Failed to append to  <*> , m= <*> ",warn,"<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogFileHandler: void append(long,long,long)>"
Cannot parse line:  line ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry: org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner$LogEntry parseEntry(java.lang.String)>
Removed  bpos ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>
Couldn\'t remove BPOS  t  from bpByNameserviceId map ,warn,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService)>
Refresh request received for nameservices:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void refreshNamenodes(org.apache.hadoop.conf.Configuration)>
Starting BPOfferServices for nameservices:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
Stopping BPOfferServices for nameservices:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
Refreshing list of NNs for nameservices:  <*> ,info,<org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: void doRefreshNamenodes(java.util.Map)>
Unexpected meta-file version for  name : version in file is  $i  but expected version is   ,warn,"<org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: org.apache.hadoop.util.DataChecksum readDataChecksum(java.io.DataInputStream,java.lang.Object)>"
Error while processing URI:  name ,error,<org.apache.hadoop.hdfs.server.common.Util: java.util.List stringCollectionAsURIs(java.util.Collection)>
Syntax error in URI  s . Please check hdfs configuration. ,error,<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>
Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. ,warn,<org.apache.hadoop.hdfs.server.common.Util: java.net.URI stringAsURI(java.lang.String)>
Failed to preserve last modified date from\' srcFile \' to \' destFile \' ,debug,"<org.apache.hadoop.hdfs.server.common.Storage: void nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean)>"
*********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ ,error,<org.apache.hadoop.hdfs.server.common.Storage: void checkVersionUpgradable(int)>
Locking is disabled for  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>
Cannot lock storage  <*> . The directory is already locked ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void lock()>
Completing previous upgrade for storage directory  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Recovering storage directory  <*>  from previous upgrade ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Completing previous rollback for storage directory  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Recovering storage directory  <*>  from previous rollback ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Completing previous finalize for storage directory  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Completing previous checkpoint for storage directory  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
Recovering storage directory  <*>  from failed checkpoint ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: void doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)>
<*>  does not exist. Creating ... ,info,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
Storage directory  <*>  does not exist ,warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
<*> is not a directory ,warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
Cannot access storage directory  <*> ,warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
Cannot access storage directory  <*> ,warn,"<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: org.apache.hadoop.hdfs.server.common.Storage$StorageState analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)>"
It appears that another namenode <*>  has already locked the storage directory ,error,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
"Failed to acquire lock on  <*> . If this storage directory is mounted via NFS,  ensure that the appropriate nfs lock services are running. ",error,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
Lock on  <*>  acquired by nodename  <*> ,info,<org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: java.nio.channels.FileLock tryLock()>
getUGI is returning:  <*> ,debug,"<org.apache.hadoop.hdfs.server.common.JspHelper: org.apache.hadoop.security.UserGroupInformation getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean)>"
UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>"
BLOCK* NameSystem.UnderReplicationBlock.update: block  has only  curReplicas  replicas and needs  curExpectedReplicas  replicas so is added to neededReplications  at priority level  <*> ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.protocol.Block,int,int,int,int,int)>"
BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>"
BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.protocol.Block,int)>"
BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean add(org.apache.hadoop.hdfs.protocol.Block,int,int,int)>"
Removing pending replication for  block ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks: void decrement(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
PendingReplicationMonitor thread is interrupted.,debug,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void run()>
PendingReplicationMonitor checking Q,debug,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
PendingReplicationMonitor timed out  block ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
dfs.namenode.startup.delay.block.deletion.sec is set to  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>
The block deletion will start around  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void printBlockDeletionTime(org.apache.commons.logging.Log)>
BLOCK*  <*> : add  block  to  datanode ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)>"
Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: java.util.List invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.conf.Configuration)>"
Exception while checking heartbeat,error,<org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor: void run()>
<*>  interrupted:  <*> ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void run()>
entry= entry ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor: void check()>
<*> .wipeDatanode( node ): storage  <*>  is removed from datanodeMap. ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Stop Decommissioning  node ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Start Decommissioning  node   storage  with  <*>  blocks ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
BLOCK* removeDeadDatanode: lost heartbeat from  d ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDeadDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
remove datanode  nodeInfo ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
BLOCK* removeDatanode:  node  does not exist ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
<*>,add,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
BLOCK* registerDatanode: node restarted.,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
BLOCK* registerDatanode: from  nodeReg  storage  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
BLOCK* registerDatanode:  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
BLOCK* registerDatanode:  <*>  is replaced by  nodeReg  with the same storageID  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
<*>,remove,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
Unresolved datanode registration:  <*> ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
Marking all datandoes as stale,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void markAllDatanodesStale()>
<*> Not checking for mis-replicated blocks because this NN is not yet processing repl queues. ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
"<*> Re-checking all blocks for replication, since they should now be replicated cross-rack ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<*> .addDatanode:  node  node  is added to datanodeMap. ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
error reading hosts files: ,error,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
dfs.block.invalidate.limit= <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
dfs.namenode.datanode.registration.ip-hostname-check= <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void <init>(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.conf.Configuration)>"
Skipped stale nodes for recovery :  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[] handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int)>"
BLOCK* NameSystem.getDatanode:  <*> ,fatal,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor getDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Invalid hostname  hostStr  in hosts file ,warn,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.protocol.DatanodeID parseDNFromHostsEntry(java.lang.String)>
"The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
"The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: long getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long)>"
Unresolved dependency mapping for host  <*> . Continuing with an empty dependency list ,error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependenciesWithDefault(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
The dependency call returned null for host  <*> ,<init>,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
The dependency call returned null for host  <*> ,error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)>
Unresolved topology mapping. Using /default-rack for host  <*> ,error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocationWithFallBackToDefaultLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Unresolved topology mapping for host  <*> ,<init>,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
The resolve call returned null!,error,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.lang.String resolveNetworkLocation(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Decommission complete for  node ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: boolean checkDecommissionState(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Number of failed storage changes from  <*>  to  volFailures ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateHeartbeatState(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int)>"
storageInfo  failed. ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void updateFailedStorage(java.util.Set)>
Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
Deferring removal of stale storage  len$  with  <*>  blocks ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
Removed storage  len$  from DataNode this ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
block  is already in the recovery queue ,info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void addBlockToBeRecovered(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction)>
"<*>  had lastBlockReportId x <*> , but curBlockReportId = x <*> ",info,<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: java.util.List removeZombieStorages()>
BLOCK NameSystem.addToCorruptReplicasMap:  <*>  added as corrupt on  dn  by  <*> reasonText ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>"
BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for  <*>  to add as corrupt  on  dn  by  <*> reasonText ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>"
Interrupted while waiting for CacheReplicationMonitor rescan,warn,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void waitForRescanIfNeeded()>
"Directive {}: not scanning file {} because bytesNeeded for pool {} is {}, but the pool\'s limit is {}",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Directive {}: caching {}: {}/{} bytes,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
"Directive {}: can\'t cache block {} because it is in state {}, not COMPLETE.",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Directive {}: setting replication for block {} to {},trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>"
Directive {}: the directive expired at {} (now = {}),debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
Directive {}: got UnresolvedLinkException while resolving path {},debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
Directive {}: No inode found at {},debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
"Directive {}: ignoring non-directive, non-file inode {} ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
Block {}: removing from PENDING_UNCACHED for node {} because the DataNode uncached it.,trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: can\'t cache block because it is {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: removing from PENDING_CACHED for node {}because we already have {} cached replicas and we only need {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Block {}: removing from PENDING_UNCACHED for node {} because we only have {} cached replicas and we need {},trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
"Block {}: removing from cachedBlocks, since neededCached == , and pendingUncached and pendingCached are empty.",trace,<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
Logic error: we\'re trying to uncache more replicas than actually exist for  cachedBlock ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingUncached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"Block {}: can\'t add new cached replicas, because there is no record of this block on the NameNode.",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"Block {}: can\'t cache this block, because it is not yet complete.",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
Block {}: we only have {} of {} cached replicas. {} DataNodes have insufficient cache capacity.,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
"Block {}: DataNode {} is not a valid possibility because the block has size {}, but the DataNode only has {}bytes of cache remaining ({} pending bytes, {} already cached.",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
Block {}: added to PENDING_CACHED on DataNode {},trace,"<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>"
Not able to find datanode  hostname  which has dependency with datanode  <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup: int addDependentNodesToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)>"
"Failed to choose remote rack (location = ~ <*> ), fallback to local rack ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: void chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
storageTypes= <*> ,trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
"Failed to place enough replicas, still in need of  <*>  to reach  totalReplicasExpected  (unavailableStorages= unavailableStorages , storagePolicy= storagePolicy , newBlock= newBlock ) ",trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
<*>   <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>"
"Failed to choose with favored nodes (= favoredNodes ), disregard favored nodes hint and retry. ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>"
Could not find a target for file  src  with favored node  favoredNode ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy)>"
"Failed to choose from local rack (location =  <*> ), retry with the rack of the next replica (location =  <*> ) ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Failed to choose from local rack (location =  <*> ); the second replica is not found, retry choosing ramdomly ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
"Failed to choose from the next rack (location =  <*> ), retry choosing ramdomly ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseFromNextRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>"
No excess replica can be found. excessTypes:  excessTypes . moreThanOne:  <*> . exactlyOne:  <*> . ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.util.List chooseReplicasToDelete(java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Interrupted while waiting for replicationQueueInitializer. Returning..,warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void stopReplicationInitializer()>
Decreasing replication from  $i  to  $i  for  src ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
Increasing replication from  $i  to  $i  for  src ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void setReplication(short,short,java.lang.String,org.apache.hadoop.hdfs.protocol.Block[])>"
BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block  b  no longer found  in block map. ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block  b , result is  <*> ",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
"processReport x <*> : removing zombie storage  <*> , which no longer exists on the DataNode. ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>"
"processReport x <*> : removed  <*>  replicas from storage  <*> , which no longer exists on the DataNode. ",warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeZombieReplicas(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)>"
BLOCK* removeStoredBlock:  block  from  node ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
BLOCK* removeStoredBlock:  block  has already been removed from node  node ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
BLOCK* removeStoredBlock:  block  is removed from excessBlocks ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Queueing reported block  block  in state  reportedState  from datanode  <*>  for later processing because  reason . ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String)>"
Processing previouly queued message  rbi ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processQueuedMessages(java.lang.Iterable)>
Invalidated  numOverReplicated  over-replicated blocks on  srcNode  during recommissioning ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlocksOnReCommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. ,trace,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlock(org.apache.hadoop.hdfs.protocol.Block,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Total number of blocks            =  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Number of invalid blocks          =  nrInvalid ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Number of under-replicated blocks =  nrUnderReplicated ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Number of  over-replicated blocks =  nrOverReplicated <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Number of blocks being written    =  nrUnderConstruction ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
"STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in  <*>  msec ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
Interrupted while processing replication queues.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
block  block :  <*> ,trace,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
BLOCK* block  <*> :  <*>  is received from  nodeID ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"*BLOCK* NameNode.processIncrementalBlockReport: from  nodeID  receiving:  receiving ,   received:  received ,   deleted:  deleted ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
BLOCK* processIncrementalBlockReport is received from dead or unregistered node  nodeID ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
Unknown block status code reported by  nodeID :  rdbi ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>"
"BLOCK* chooseExcessReplicates: ( chosen ,  b ) is added to invalidated blocks set ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processChosenExcessReplica(java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block)>"
BLOCK* addBlock: logged info for  <*>  of  i$_  reported. ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
BLOCK* addBlock: block  b_#  on  <*>  size  <*>  does not belong to any file ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Processing  <*>  messages from DataNodes  that were previously queued during standby state ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAllPendingDNMessages()>
BLOCK* markBlockReplicasAsCorrupt: mark block replica b  on  <*>  as corrupt because the dn is not in the new committed  storage list. ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockReplicasAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,long,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>"
BLOCK markBlockAsCorrupt:  b  cannot be marked as corrupt as it does not belong to any file ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
"Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void logBlockReplicationInfo(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)>"
invalidateCorruptReplicas error in deleting bad block  blk  on  node ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>
BLOCK* findAndMarkBlockAsCorrupt:  blk  not found ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String)>"
BLOCK* addToInvalidates:  b   <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToInvalidates(org.apache.hadoop.hdfs.protocol.Block)>
"BLOCK* addToExcessReplicate: ( dn ,  block ) is added to excessReplicateMap ",debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToExcessReplicate(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)>"
BLOCK* blockReceived:  block  is expected to be removed from an unrecorded node  delHint ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,java.lang.String)>"
defaultReplication         =  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
maxReplication             =  $i ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
minReplication             =  $i ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
maxReplicationStreams      =  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
replicationRecheckInterval =  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
encryptDataTransfer        =  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
maxNumBlocksToLog          =  <*> ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void <init>(org.apache.hadoop.hdfs.server.namenode.Namesystem,org.apache.hadoop.hdfs.server.namenode.FSClusterStats,org.apache.hadoop.conf.Configuration)>"
BLOCK* getBlocks: Asking for blocks from an unrecorded node  datanode ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations getBlocksWithLocations(org.apache.hadoop.hdfs.protocol.DatanodeID,long)>"
"Received an RBW replica for  storedBlock  on  dn : ignoring it, since it is  complete with the same genstamp ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Unexpected replica state  reportedState  for block:  storedBlock  on  dn  size  <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>"
Reported block  block  on  <*>  size  <*>  replicaState =  reportedState ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>"
In memory blockUCState =  <*> ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>"
Security is enabled but block access tokens (via dfs.block.access.token.enable) aren\'t enabled. This may cause issues when clients attempt to talk to a DataNode.,error,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
dfs.block.access.token.enable= <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
"dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)>
blocks =  <*> ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlocks createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo)>"
Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlock createLocatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long)>"
BLOCK* addStoredBlock:  block  on  <*>  size  <*>  but it does not belong to any file ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
block,logAddStoredBlock,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
BLOCK* addStoredBlock: Redundant addStoredBlock request received for  storedBlock  on  <*>  size  <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
Inconsistent number of corrupt replicas for  storedBlock blockMap has  <*>  but corrupt replicas map has  <*> ,warn,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>"
BLOCK* processReport: logged info for  <*>  of  i$  reported. ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: java.util.Collection processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)>"
"In safemode, not computing replication work",debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
BLOCK*  <*> : ask  dn  to delete  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
"DataNode  dn  cannot be found with UUID  <*> , removing block invalidation work. ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
Block  block  cannot be repl from any node ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
BLOCK* block  <*>  is moved from neededReplications to pendingReplications ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
BLOCK* neededReplications =  <*>  pendingReplications =  <*> ,debug,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
BLOCK* Removing  block  from neededReplications as it has enough replicas ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
BLOCK* Removing  <*>  from neededReplications as it has enough replicas ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
BLOCK* ask  <*>  to replicate  <*>  to  $u ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
processReport x <*> : no zombie storages found. ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
processReport x <*> :  <*>  more RPCs remaining in this report. ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
BLOCK* processReport: discarded non-initial block report from  nodeID  because namenode still in startup phase ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
BLOCK* processReport: Received first block report from  storage  after starting up or becoming active. Its block  contents are no longer considered stale ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
BLOCK* processReport:  staleBefore  on  <*>  size  <*>  does not belong to any file ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"BLOCK* processReport: from storage  <*>  node  nodeID , blocks:  <*> , hasStaleStorages:  <*> , processing time:  <*>  msecs ",info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext,boolean)>"
"srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. ",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean isReplicationInProgress(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
BLOCK* invalidateBlocks:  b  on  dn  listed for deletion. ,debug,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
BLOCK* invalidateBlock:  b  on  dn ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
BLOCK* invalidateBlocks: postponing invalidation of  b  on  dn  because  <*>  replica(s) are located on nodes  with potentially out-of-date block reports ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
BLOCK* invalidateBlocks:  b  on  dn  is the only copy and was not deleted ,info,"<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>"
ReplicationMonitor thread received Runtime exception. ,fatal,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
Stopping ReplicationMonitor.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
ReplicationMonitor received an exception while shutting down.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
Stopping ReplicationMonitor for testing.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor: void run()>
Error while processing replication queues async,error,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>
Interrupted while processing replication queues.,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1: void run()>
BLOCK* Removing stale replica from location:  <*> ,info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void setGenerationStampAndVerifyReplicas(long)>
"BLOCK*  this  recovery started, primary= primary ",info,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>
"BLOCK* BlockInfoUnderConstruction.initLeaseRecovery: No blocks found, lease removed.",warn,<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction: void initializeBlockRecovery(long)>
Failed to delete  <*> ,warn,<org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: void close()>
Exception shutting down access key updater thread,warn,<org.apache.hadoop.hdfs.server.balancer.KeyManager: void close()>
"Block token params received from NN: update interval= <*> , token lifetime= <*> ",info,"<org.apache.hadoop.hdfs.server.balancer.KeyManager: void <init>(java.lang.String,org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol,boolean,org.apache.hadoop.conf.Configuration)>"
InterruptedException in block key updater thread,debug,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
Failed to set keys,error,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
Exception in block key updater thread,error,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
Exception shutting down key updater thread,warn,<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void close()>
Update block keys every  <*> ,info,"<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void <init>(org.apache.hadoop.hdfs.server.balancer.KeyManager,long)>"
Dispatcher thread failed,warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher: long dispatchBlockMoves()>
"Excluding datanode  dn :  <*> ,  <*> ,  <*> ,  notIncluded ",trace,<org.apache.hadoop.hdfs.server.balancer.Dispatcher: boolean shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
Exception while getting block list,warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: void dispatchBlocks()>
Start moving  this ,debug,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
Successfully moved  this ,info,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
Failed to move  this :  <*> ,warn,<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: void dispatch()>
Decided to move  this ,debug,"<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: boolean markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.StorageType)>"
Decided to move  <*>  bytes from  <*>  to  <*> ,info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: void matchSourceWithTargetToMove(org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)>"
Exiting balancer due an exception,error,<org.apache.hadoop.hdfs.server.balancer.Balancer: void main(java.lang.String[])>
over-utilized,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
above-average,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
below-average,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
underutilized,logUtilizationCollection,<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollections()>
<*>   name :  items ,info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: void logUtilizationCollection(java.lang.String,java.util.Collection)>"
namenodes  =  namenodes ,info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>"
parameters =  p ,info,"<org.apache.hadoop.hdfs.server.balancer.Balancer: int run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters,org.apache.hadoop.conf.Configuration)>"
Using a threshold of  <*> ,info,<org.apache.hadoop.hdfs.server.balancer.Balancer$Cli: org.apache.hadoop.hdfs.server.balancer.Balancer$Parameters parse(java.lang.String[])>
Same delegation token being added twice; invalid entry in fsimage or editlogs,<init>,"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
No KEY found for persisted identifier  <*> ,warn,"<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: void addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)>"
"Checking access for user= userId , block= block , access mode= mode  using  <*> ",debug,"<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager$AccessMode)>"
Setting block keys,info,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void addKeys(org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys)>
Exporting access keys,debug,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys exportKeys()>
Generating block token for  <*> ,debug,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: byte[] createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier)>
Updating block keys,info,<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: boolean updateKeys()>
Could not send read status ( statusCode ) to datanode  <*> :  <*> ,info,<org.apache.hadoop.hdfs.RemoteBlockReader2: void sendReadResult(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
Reading empty packet at end of read,trace,<org.apache.hadoop.hdfs.RemoteBlockReader2: void readTrailingEmptyPacket()>
DFSClient readNextPacket got header  <*> ,trace,<org.apache.hadoop.hdfs.RemoteBlockReader2: void readNextPacket()>
Could not send read status ( statusCode ) to datanode  <*> :  <*> ,info,"<org.apache.hadoop.hdfs.RemoteBlockReader: void sendReadResult(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>"
DFSClient readChunk got header  $u ,debug,"<org.apache.hadoop.hdfs.RemoteBlockReader: int readChunk(long,byte[],int,int,byte[])>"
Unable to stop HTTP server for  this ,warn,<org.apache.hadoop.hdfs.qjournal.server.JournalNode: void stop(int)>
Initializing journal in directory  <*> ,info,"<org.apache.hadoop.hdfs.qjournal.server.JournalNode: org.apache.hadoop.hdfs.qjournal.server.Journal getOrCreateJournal(java.lang.String,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)>"
Error reported on file  f ... exiting ,fatal,<org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter: void reportErrorOnFile(java.io.File)>
Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void updateLastPromisedEpoch(long)>
Updating lastWriterEpoch from  <*>  to  <*>  for client  <*> ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>"
Client is requesting a new log segment  txid  though we are already writing  <*> .  Aborting the current segment in order to begin the new one. ,warn,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int)>"
Writing txid  firstTxnId - <*> ,trace,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>"
Sync of transaction range  firstTxnId - <*>  took  <*> ms ,warn,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>"
Formatting  this  with namespace info:  nsInfo ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Validating log segment  <*>  about to be  finalized ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long)>"
Starting upgrade of edits directory: .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void doUpgrade(org.apache.hadoop.hdfs.server.common.StorageInfo)>
Finalizing upgrade for journal  <*> . <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void doFinalize()>
Rolling forward previously half-completed synchronization:  <*>  ->  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: void completeHalfDoneAcceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData)>
<*> ,alwaysAssert,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Synchronizing log  <*> : no current segment in place ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Synchronizing log  <*> : old segment  <*>  is not the right length ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Skipping download of log  <*> : already have up-to-date logs ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Accepted recovery for segment  <*> :  <*> ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: void acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Scanning storage  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
Latest log is  latestLog ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
No files in  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
Latest log  latestLog  has no transactions.  moving it aside and looking for previous log ,warn,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile scanStorageForLatestEdits()>
Edit log file  <*>  appears to be empty.  Moving it aside... ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>
getSegmentInfo( segmentTxId ):  <*>  ->  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto getSegmentInfo(long)>
Prepared recovery for segment  segmentTxId :  <*> ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long)>"
Synchronizing log  <*>  from  url ,info,"<org.apache.hadoop.hdfs.qjournal.server.Journal: java.io.File syncLog(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL)>"
Failed to delete temporary file  <*> ,warn,<org.apache.hadoop.hdfs.qjournal.server.Journal$1: java.lang.Void run()>
Purging no-longer needed file  <*> ,info,"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>"
Unable to delete no-longer-needed data  f ,warn,"<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void purgeMatching(java.io.File,java.util.List,long)>"
Formatting journal  <*>  with nsid:  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)>
Closing journal storage for  <*> ,info,<org.apache.hadoop.hdfs.qjournal.server.JNStorage: void close()>
Validating request made by  <*>  /  <*> . This user is:  <*> ,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
SecondaryNameNode principal could not be added,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
isValidRequestor is comparing to valid requestor:  msg ,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
isValidRequestor is allowing:  <*> ,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
isValidRequestor is allowing other JN principal:  <*> ,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
isValidRequestor is rejecting:  <*> ,debug,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
Received null remoteUser while authorizing access to GetJournalEditServlet,warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>"
Received an invalid request file transfer request from  <*> :  <*> ,warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
Received non-NN/JN request for edits from  <*> ,warn,"<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>"
<*> <*> <*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void flushAndSync(boolean)>
Aborting  this ,warn,<org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream: void abort()>
selectInputStream manifests:\n <*> ,debug,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>"
newEpoch( <*> ) responses:\n <*> ,debug,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
Starting recovery process for unclosed journal segments...,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
Successfully started new epoch  <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
"One of the loggers had a response, but no best logger was found.",<init>,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
Beginning recovery of unclosed segment starting at txid  segmentTxId ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
Recovery prepare phase complete. Responses:\n <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
Using already-accepted recovery for segment starting at txid  segmentTxId :  <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
Using longest log:  <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
None of the responders had a log to recover:  <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
<*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
<*> <*>,waitForWriteQuorum,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnclosedSegment(long)>
Purging remote journals older than txid  minTxIdToKeep ,info,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void purgeLogsOlderThan(long)>
firstTxId lastTxId,waitForWriteQuorum,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void finalizeLogSegment(long,long)>"
loggerFactory,<init>,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void <init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory)>"
txId layoutVersion,waitForWriteQuorum,"<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream startLogSegment(long,int)>"
Quorum journal URI \' uri \' has an even number  of Journal Nodes specified. This is not recommended! ,warn,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: java.util.List getLoggerAddresses(java.net.URI)>
Timed out waiting for response from loggers,<init>,<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: boolean hasSomeData()>
msg . No responses yet. ,info,"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>"
msg . No responses yet. ,warn,"<org.apache.hadoop.hdfs.qjournal.client.QuorumCall: void waitFor(int,int,int,int,java.lang.String)>"
<*> <*>,addWriteEndToEndLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteEndToEndLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteRpcLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
<*> <*>,addWriteRpcLatency,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
Took  <*> ms to send a batch of  <*>  edits ( <*>  bytes) to  remote journal  <*> ,warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
Remote journal  <*>  failed to  write txns  <*> - <*> . Will try to write to this JN again after the next  log roll. ,warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
Took  <*> ms to send a batch of  <*>  edits ( <*>  bytes) to  remote journal  <*> ,warn,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7: java.lang.Void call()>
Restarting previously-stopped writes to  <*>  in segment starting at txid  <*> ,info,<org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10: java.lang.Void call()>
Connecting to datanode  <*>  addr= <*> ,debug,"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: void <init>(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>"
Connecting to datanode  <*>  addr= <*> ,debug,"<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean,org.apache.hadoop.hdfs.protocol.LocatedBlock)>"
Sending DataTransferOp  <*> :  proto ,trace,"<org.apache.hadoop.hdfs.protocol.datatransfer.Sender: void send(java.io.DataOutputStream,org.apache.hadoop.hdfs.protocol.datatransfer.Op,com.google.protobuf.Message)>"
"SASL server doing encrypted handshake for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in unsecured configuration for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in secured configuration for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server doing general handshake for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
Server using encryption algorithm  <*> ,debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream)>"
Server using cipher suite  <*> ,debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(java.io.OutputStream,java.io.InputStream,java.util.Map,javax.security.auth.callback.CallbackHandler)>"
"SASL client doing encrypted handshake for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client doing general handshake for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
Client using encryption algorithm {},debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey)>"
"SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>"
"Verifying QOP, requested QOP = {}, negotiated QOP = {}",debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: void checkSaslComplete(org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant,java.util.Map)>"
"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}",debug,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
"DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}",debug,<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,debug,"<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair createStreamPair(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherOption,java.io.OutputStream,java.io.InputStream,boolean)>"
readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i ,trace,"<org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: void doRead(java.nio.channels.ReadableByteChannel,java.io.InputStream)>"
"Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) ",warn,"<org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: java.util.List chooseStorageTypes(short,java.lang.Iterable,java.util.EnumSet,boolean)>"
SocketCache disabled.,info,"<org.apache.hadoop.hdfs.PeerCache: void <init>(int,long)>"
"got IOException closing stale peer  <*> , which is  ageMs  ms old ",warn,"<org.apache.hadoop.hdfs.PeerCache: org.apache.hadoop.hdfs.net.Peer get(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean)>"
error closing TcpPeerServer: ,error,<org.apache.hadoop.hdfs.net.TcpPeerServer: void close()>
error closing DomainPeerServer: ,error,<org.apache.hadoop.hdfs.net.DomainPeerServer: void close()>
Couldn\'t create proxy provider  failoverProxyProviderClass ,debug,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean)>"
Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup,warn,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean)>"
Unsupported protocol found when creating the proxy connection to NameNode:  <*> ,error,"<org.apache.hadoop.hdfs.NameNodeProxies: org.apache.hadoop.hdfs.NameNodeProxies$ProxyAndInfo createNonHAProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean,java.util.concurrent.atomic.AtomicBoolean)>"
Lease renewer daemon for  <*>  with renew id  id  executed ,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
Lease renewer daemon for  <*>  with renew id  id  is not current ,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
Lease renewer daemon for  <*>  with renew id  id  expired ,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
Failed to renew lease for  <*>  for  <*>  seconds.  Aborting ... ,warn,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
Failed to renew lease for  <*>  for  <*>  seconds.  Will retry shortly ... ,warn,<org.apache.hadoop.hdfs.LeaseRenewer: void run(int)>
Did not renew lease for client  c ,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>
Lease renewed for client  <*> ,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void renew()>
Wait for lease checker to terminate,debug,<org.apache.hadoop.hdfs.LeaseRenewer: void interruptAndJoin()>
Lease renewer daemon for  <*>  with renew id  <*>  started ,debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
Lease renewer daemon for  <*>  with renew id  <*>  exited ,debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
<*>  is interrupted. ,debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
Lease renewer daemon for  <*>  with renew id  <*>  exited ,debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
Lease renewer daemon for  <*>  with renew id  <*>  exited ,debug,<org.apache.hadoop.hdfs.LeaseRenewer$1: void run()>
Mapped HA service delegation token for logical URI  haUri  to namenode  singleNNAddr ,debug,"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>"
No HA service delegation token found for logical URI  haUri ,debug,"<org.apache.hadoop.hdfs.HAUtil: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>"
Starting web server as:  <*> ,info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
Starting Web-server for  name  at:  <*> ,info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
Starting Web-server for  name  at:  <*> ,info,"<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpServer2$Builder httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String)>"
hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.,warn,<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>
dfs.https.enable is deprecated. Please use dfs.http.policy.,warn,<org.apache.hadoop.hdfs.DFSUtil: org.apache.hadoop.http.HttpConfig$Policy getHttpPolicy(org.apache.hadoop.conf.Configuration)>
Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. ,warn,"<org.apache.hadoop.hdfs.DFSUtil: java.util.Map getAddressesForNameserviceId(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String[])>"
Exception in creating socket address  <*> ,warn,"<org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>"
"DFSClient writeChunk allocating new packet seqno= <*> , src= <*> , packetSize= <*> , chunksPerPacket= <*> , bytesCurBlock= <*> ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>"
"DFSClient writeChunk packet full seqno= <*> , src= <*> , bytesCurBlock= <*> , blockSize= <*> , appendChunk= <*> ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>"
Waiting for ack for:  seqno ,debug,<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>
Slow waitForAckedSeqno took  <*> ms (threshold= <*> ms) ,warn,<org.apache.hadoop.hdfs.DFSOutputStream: void waitForAckedSeqno(long)>
Queued packet  <*> ,debug,<org.apache.hadoop.hdfs.DFSOutputStream: void queueCurrentPacket()>
DFSClient flush() : bytesCurBlock  <*>  lastFlushOffset  <*> ,debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
Unable to persist blocks in hflush for  <*> ,warn,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
Error while syncing,warn,"<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>"
"computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> ",debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void computePacketChunkSize(int,int)>"
Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning  <*>  hdfsTimeout  <*> ,info,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Could not complete  <*>  retrying... ,info,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream: void completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Set non-null progress callback on DFSOutputStream  src ,debug,"<org.apache.hadoop.hdfs.DFSOutputStream: void <init>(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.util.Progressable,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.util.DataChecksum)>"
Connecting to datanode  <*> ,debug,"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>"
Send buf size  <*> ,debug,"<org.apache.hadoop.hdfs.DFSOutputStream: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>"
Allocating new block,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Append to block  <*> ,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
DataStreamer block  <*>  sending packet  e_ ,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
DataStreamer Exception,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void run()>
Closing old block  <*> ,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void endBlock()>
Caught exception ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void closeResponder()>
lastAckedSeqno =  <*> ,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: void addDatanode2ExistingPipeline()>
Abandoning  <*> ,info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>
Excluding datanode  <*> ,info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock nextBlockOutputStream()>
Exception while adding a block,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
Waiting for replication for  <*>  seconds ,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
NotReplicatedYetException sleeping  <*>  retries left  retries ,warn,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
Caught exception ,warn,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: org.apache.hadoop.hdfs.protocol.LocatedBlock locateFollowingBlock(long,org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>"
Could not get block locations. Source file \ <*> \ - Aborting... ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
Error Recovery for block  <*>  in pipeline  $u : bad datanode  <*> ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
Failed to replace datanode. Continue with the remaining datanodes since dfs.client.block.write.replace-datanode-on-failure.best-effort is set to true.,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
Datanode did not restart in time:  <*> ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean setupPipelineForAppendOrRecovery()>
Error Recovery for  <*>  waiting for responder to exit.  ,info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>
Error recovering pipeline for writing  <*> . Already retried  times for the same packet. ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean processDatanodeError()>
pipeline =  <*> ,debug,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
nodes are empty for write pipeline of block  <*> ,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
Exception in createBlockOutputStream,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> ",info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
Waiting for the datanode to be restarted:  <*> ,info,"<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.StorageType[],long,boolean)>"
DFSClient  $u ,debug,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
A datanode is restarting:  <*> ,info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
"Slow ReadProcessor read fields took  duration ms (threshold= <*> ms); ack:  $u , targets:  <*> ",warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
DFSOutputStream ResponseProcessor exception  for block  <*> ,warn,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor: void run()>
Removing node  <*>  from the excluded nodes list ,info,<org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$2: void onRemoval(com.google.common.cache.RemovalNotification)>
Exception while seek to  targetPos  from  <*>  of  <*>  from  <*> ,debug,<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>
"BlockReader failed to seek to  targetPos . Instead, it seeked to  <*> . ",warn,<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>
Last block locations not available. Datanodes might not have reported blocks completely. Will retry for  retriesForLastBlockLength  times ,warn,<org.apache.hadoop.hdfs.DFSInputStream: void openInfo()>
error closing blockReader,error,<org.apache.hadoop.hdfs.DFSInputStream: void closeCurrentBlockReader()>
"closing file  <*> , but there are still  unreleased ByteBuffers allocated by read().   Please release  <*> . ",warn,<org.apache.hadoop.hdfs.DFSInputStream: void close()>
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  targetAddr  :  <*> ",info,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
fetchBlockByteRange(). Got a checksum exception for  <*>  at  <*> : <*>  from  chosenNode ,warn,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
Connection failure:  <*> ,warn,"<org.apache.hadoop.hdfs.DFSInputStream: void actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>"
Successfully connected to  <*>  for  <*> ,info,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> ",info,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
"Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> ",warn,<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.protocol.DatanodeInfo blockSeekTo(long)>
Connecting to datanode  <*> ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
No node available for  <*> ,info,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
Could not obtain  <*>  from any node:  <*> <*> . Will get new block locations from namenode and retry... ,info,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
<*> <*> . Throwing a BlockMissingException ,warn,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
"DFS chooseDataNode: got #  <*>  IOException, will wait for  <*>  msec. ",warn,"<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>"
Failed to getReplicaVisibleLength from datanode  datanode  for block  <*> ,debug,<org.apache.hadoop.hdfs.DFSInputStream: long readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock)>
newInfo =  <*> ,debug,<org.apache.hadoop.hdfs.DFSInputStream: long fetchLocatedBlocksAndGetLastBlockLength()>
Unable to perform a zero-copy read from offset  curPos  of  <*> ;  length  bytes left in block.   blockPos= blockPos ; curPos= curPos ; curEnd= curEnd ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
Reducing read length from  maxLength  to  length  to avoid going more than one byte  past the end of the block.  blockPos= blockPos ; curPos= curPos ; curEnd= curEnd ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
"Unable to perform a zero-copy read from offset  curPos  of  <*> ; -bit MappedByteBuffer limit  exceeded.  blockPos= blockPos , curEnd= curEnd ",debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
Reducing read length from  maxLength  to  length  to avoid -bit limit.   blockPos= blockPos ; curPos= curPos ; curEnd= curEnd ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
unable to perform a zero-copy read from offset  curPos  of  <*> ; BlockReader#getClientMmap returned  null. ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
readZeroCopy read  length  bytes from offset  curPos  via the zero-copy read  path.  blockEnd =  <*> ,debug,"<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>"
DFS Read,warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readWithStrategy(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int)>"
Found Checksum error for  <*>  from  <*>  at  <*> ,warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>"
Exception while reading from  <*>  of  <*>  from  <*> ,warn,"<org.apache.hadoop.hdfs.DFSInputStream: int readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy,int,int,java.util.Map)>"
Access token was invalid when connecting to  targetAddr  :  ex ,info,"<org.apache.hadoop.hdfs.DFSInputStream: boolean tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)>"
"take(): poll() returned null, sleeping for {} ms",debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch take()>
timed poll(): timed out,debug,"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>"
"timed poll(): poll() returned null, sleeping for {} ms",debug,"<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>"
"poll(): lastReadTxid is -, reading current txid from NN",debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>
poll(): read no edits from the NN when requesting edits after txid {},debug,<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>
Found corruption while reading  file . Error repairing corrupt blocks. Bad blocks remain. ,info,"<org.apache.hadoop.hdfs.DFSClient: void reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>"
Using hedged reads; pool threads= num ,debug,<org.apache.hadoop.hdfs.DFSClient: void initThreadsNumForHedgedReads(int)>
Clearing encryption key,debug,<org.apache.hadoop.hdfs.DFSClient: void clearDataEncryptionKey()>
Cancelling  <*> ,info,<org.apache.hadoop.hdfs.DFSClient: void cancelDelegationToken(org.apache.hadoop.security.token.Token)>
Using legacy short-circuit local reads.,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
No KeyProvider found.,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
Found KeyProvider:  <*> ,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
Using local interfaces [ <*> ] with addresses [ <*> ] ,debug,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
"dfs.client.test.drop.namenode.response.number is set to  <*> , this hacked client will proactively drop responses ",warn,"<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>"
Created  <*> ,info,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
Cannot get delegation token from  renewer ,info,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.security.token.Token getDelegationToken(org.apache.hadoop.io.Text)>
Getting new encryption token from NN,debug,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey newDataEncryptionKey()>
Connecting to datanode  <*> ,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair connectToDN(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.protocol.LocatedBlock)>"
src : masked= <*> ,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.DFSOutputStream create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[])>"
"write to  <*> :  <*> , block= <*> ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"set bytesPerCRC= bytesPerCRC , crcPerBlock= crcPerBlock ",debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
got reply from  <*> : md= $u ,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
Got access token error in response to OP_BLOCK_CHECKSUM for file  src  for block  <*>  from datanode  <*> . Will retry the block once. ,debug,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
"src= src , datanodes[ j ]= <*> ",warn,"<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>"
metadata returned:  <*> ,trace,<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.BlockStorageLocation[] getBlockStorageLocations(java.util.List)>
Renewing  <*> ,info,<org.apache.hadoop.hdfs.DFSClient: long renewDelegationToken(org.apache.hadoop.security.token.Token)>
Problem getting block size,warn,<org.apache.hadoop.hdfs.DFSClient: long getBlockSize(java.lang.String)>
Using local interface  addr ,debug,<org.apache.hadoop.hdfs.DFSClient: java.net.SocketAddress getRandomLocalInterfaceAddr()>
Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... ,warn,<org.apache.hadoop.hdfs.DFSClient: boolean renewLease()>
src : masked= absPermission ,debug,"<org.apache.hadoop.hdfs.DFSClient: boolean primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>"
Address  targetAddr <*> ,trace,<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>
Address  targetAddr <*> ,trace,<org.apache.hadoop.hdfs.DFSClient: boolean isLocalAddress(java.net.InetSocketAddress)>
Cancelling  <*> ,info,"<org.apache.hadoop.hdfs.DFSClient$Renewer: void cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)>"
dfs.client.use.legacy.blockreader.local =  <*> ,debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
dfs.client.read.shortcircuit =  <*> ,debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
dfs.client.domain.socket.data.traffic =  <*> ,debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
dfs.domain.socket.path =  <*> ,debug,<org.apache.hadoop.hdfs.DFSClient$Conf: void <init>(org.apache.hadoop.conf.Configuration)>
Bad checksum type:  <*> . Using default  CRCC ,warn,<org.apache.hadoop.hdfs.DFSClient$Conf: org.apache.hadoop.util.DataChecksum$Type getChecksumType(org.apache.hadoop.conf.Configuration)>
"Execution rejected, Executing in current thread",info,"<org.apache.hadoop.hdfs.DFSClient$2: void rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)>"
"Existing client context \' <*> \' does not match  requested configuration.  Existing:  <*> , Requested:  <*> ",warn,<org.apache.hadoop.hdfs.ClientContext: void printConfWarningIfNeeded(org.apache.hadoop.hdfs.DFSClient$Conf)>
Is namenode in safemode?  <*> ; uri= uri ,debug,<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
Got an exception for uri= uri ,debug,<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
Could not fetch information from datanode,debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Cancelled while waiting for datanode  <*> :  <*> ,info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Datanode  <*>  does not support  required #getHdfsBlocksMetadata() API ,info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Failed to query block locations on datanode  <*> :  <*> ,info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Interrupted while fetching HdfsBlocksMetadata,info,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
Invalid access token when trying to retrieve information from datanode  <*> ,warn,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean)>"
No data for block  blockId ,debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>"
"Datanode responded with a block volume id we did not request, omitting.",debug,"<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>"
Cached location of block  blk  as  <*> ,debug,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockPathInfo(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.security.token.Token,boolean,org.apache.hadoop.hdfs.StorageType)>"
New BlockReaderLocalLegacy for file  <*>  of size  <*>  startOffset  startOffset  length  length  short circuit checksum  <*> ,debug,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>"
BlockReaderLocalLegacy: Removing  blk  from cache because local file  <*>  could not be opened. ,warn,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.DFSClient$Conf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.hdfs.StorageType)>"
skip  n ,debug,<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: long skip(long)>
read off  off  len  len ,trace,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy: int read(byte[],int,int)>"
encountered exception ,warn,"<org.apache.hadoop.hdfs.BlockReaderLocalLegacy$LocalDatanodeInfo: org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol getDatanodeProxy(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,boolean)>"
"close(filename= <*> , block= <*> ) ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: void close()>
"can\'t get an mmap for  <*>  of  <*>  since SKIP_CHECKSUMS was not given,  we aren\'t skipping checksums, and the block is not mlocked. ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getClientMmap(java.util.EnumSet)>
"skip(n= n , block= <*> , filename= <*> ): discarded  discardedFromBuf  bytes from  dataBuf and advanced dataPos by  remaining ",trace,<org.apache.hadoop.hdfs.BlockReaderLocal: long skip(long)>
<*> : starting ,info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
traceString : I/O error ,info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
traceString : returning  nRead ,info,<org.apache.hadoop.hdfs.BlockReaderLocal: int read(java.nio.ByteBuffer)>
<*> : starting ,trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
traceString : I/O error ,trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
traceString : returning  nRead ,trace,"<org.apache.hadoop.hdfs.BlockReaderLocal: int read(byte[],int,int)>"
loaded  <*>  bytes into bounce  buffer from offset  oldDataPos  of  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderLocal: boolean fillDataBuf(boolean)>
this : <*> ,debug,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
Sending receipt verification byte for slot  slot ,trace,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
this : error creating ShortCircuitReplica. ,warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
short-circuit read access is disabled for DataNode  <*> .  reason:  <*> ,warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
short-circuit read access for the file  <*>  is disabled for DataNode  <*> .  reason:  <*> ,warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
this : unknown response code  <*>  while attempting to set up short-circuit access.  <*> ,warn,"<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>"
this : closing stale domain peer  peer ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
this : trying to create ShortCircuitReplicaInfo. ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
this : allocShmSlot used up our previous socket  <*> .  Allocating a new one... ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
this : I/O error requesting file descriptors.   Disabling domain socket  <*> ,warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
nextTcpPeer: reusing existing peer  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
nextTcpPeer: created newConnectedPeer  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
nextTcpPeer: failed to create newConnectedPeer connected to  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
nextDomainPeer: reusing existing peer  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReaderFactory$BlockReaderPeer nextDomainPeer()>
Closed potentially stale remote peer  peer ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
this : trying to create a remote block reader from a  TCP socket ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
this : got security exception while constructing  a remote block reader from  peer ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
I/O error constructing remote block reader.,warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
Closed potentially stale domain peer  <*> ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
this : trying to create a remote block reader from the  UNIX domain socket at  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
I/O error constructing remote block reader.  Disabling domain socket  <*> ,warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
this : can\'t construct  BlockReaderLocalLegacy because  disableLegacyBlockReaderLocal is set. ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
this : trying to construct BlockReaderLocalLegacy ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
this : can\'t construct BlockReaderLocalLegacy because  the address  <*>  is not local ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
this : error creating legacy BlockReaderLocal.   Disabling legacy local reads. ,warn,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
this : failed to get  ShortCircuitReplica. Cannot construct  BlockReaderLocal via  <*> ,debug,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
this : trying to construct a BlockReaderLocal  for short-circuit reads. ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
this : got InvalidToken exception while trying to  construct BlockReaderLocal via  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
this : returning new legacy block reader local. ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
this : returning new block reader local. ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
this : returning new remote block reader using  UNIX domain socket on  <*> ,trace,<org.apache.hadoop.hdfs.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
"ValidationChain: {}, {}",fine,"<jdk.internal.event.EventHelper: void logX509ValidationEvent(int,int[])>"
"XCertificate: Alg:{}, Serial:{}, Subject:{}, Issuer:{}, Key type:{}, Length:{}, Cert Id:{}, Valid from:{}, Valid until:{}",fine,"<jdk.internal.event.EventHelper: void logX509CertificateEvent(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,long,long,long)>"
"<*>  TLSHandshake: {}:{}, {}, {}, {} ",fine,"<jdk.internal.event.EventHelper: void logTLSHandshakeEvent(java.time.Instant,java.lang.String,int,java.lang.String,java.lang.String,long)>"
"SecurityPropertyModification: key:{}, value:{}",fine,"<jdk.internal.event.EventHelper: void logSecurityPropertyEvent(java.lang.String,java.lang.String)>"