DEBUG org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Going to check the following volumes disk space:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : successfully loaded  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new block reader local. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : found waitable for  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to get  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : successfully loaded  <*> 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got InvalidToken exception while trying to  construct BlockReaderLocal via  <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from a  TCP socket 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: created newConnectedPeer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: created newConnectedPeer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: The threshold value should\'t be greater than , threshold:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Clusterid mismatch - current clusterid:  <*> , Ignoring given clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  sd  contains no VERSION file. Skipping... 
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load edit log stream:  elis 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
<INIT> org.apache.hadoop.hdfs.server.namenode.FSImage: editLog must be initialized
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: Closing log when already closed
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: No edits directories configured!
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Beginning to copy stream  stream  to shared edits 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: ending log segment because of END_LOG_SEGMENT op in  stream 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: ending log segment because of end of stream in  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.ClusterJspHelper: Cluster console encounters a not handled situtation.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Encountered exception during format: 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: Added filter \' <*> \' (class= <*> ) 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Edits URI  i$  listed multiple times in  dfs.namenode.shared.edits.dir  and  dfs.namenode.edits.dir . Ignoring duplicates. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: current cluster id for sd= <*> ;lv= <*> ;cid= <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: couldn\'t find any VERSION file containing valid ClusterId
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Allocated new BlockPoolId:  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Using clusterid:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image?  <*>#_  (staleImage= <*> , haEnabled= <*> , isRollingUpgrade= <*> ) 
<INIT> org.apache.hadoop.hdfs.server.namenode.FSImage: editLog must be initialized
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in  ioe  msecs 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: Service RPC server is binding to  bindHost : <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Setting ADDRESS  address 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: RPC server is binding to  serviceHandlerCount : <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use  <*>  to access  this namenode/service. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: WebHDFS and security are enabled, but configuration property \'dfs.web.authentication.kerberos.principal\' is not set.
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer: Added filter \' <*> \' (class= <*> ) 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: <*>  RPC up at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNode: <*>  service RPC up at:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint Period :  <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Transactions count is  :  <*> , to trigger checkpoint 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file  <*>  ->  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.mkdirs:  srcArg 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous upgrade 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo( segmentTxId ):  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log  <*> : already have up-to-date logs 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* ask  <*>  to replicate  <*>  to  $u 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* neededReplications =  <*>  pendingReplications =  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  block  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* block  <*>  is moved from neededReplications to pendingReplications 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : found waitable for  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : replica  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : starting cache cleaner thread which will run  every  <*>  ms 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from the  UNIX domain socket at  <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: Closed potentially stale domain peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block  block  cannot be repl from any node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* block  <*>  is moved from neededReplications to pendingReplications 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* ask  <*>  to replicate  <*>  to  $u 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
FATAL org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* NameSystem.getDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: Block deletion is delayed during NameNode startup. The deletion will start after  <*>  ms. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK*  <*> : ask  dn  to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block  block  cannot be repl from any node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  block  from neededReplications as it has enough replicas 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* Removing  <*>  from neededReplications as it has enough replicas 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* neededReplications =  <*>  pendingReplications =  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
DEBUG org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: this : cache cleaner running at  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: <*> : released  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: this : freeing empty stale  shm 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Cancelling caching for block with id {}, pool {}.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: Block with id {}, pool {} does not need to be uncached, because it is in state {}.
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> :  b  (numBytes= <*> ) , stage= stage , clientname= clientname , targets= <*> , target storage types= <*> 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: Could not find metadata file for  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: replica= <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : Transmitted  <*>  (numBytes= <*> ) to  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: <*> : close-ack= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: replica= <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: Storage directory  <*>  has been successfully formatted. 
WARN org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: Invalid namespaceID in journal request - expected  <*>  actual  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: NameNode started a new log segment at txid  txid 
WARN org.apache.hadoop.hdfs.server.namenode.BackupImage: NN started new log segment at txid  txid , but BN had only written up to txid  <*> in the log segment starting at  <*> . Aborting this  log segment. 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at  segmentTxId 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Stopped applying edits to prepare for checkpoint.
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Stopped plug-in  p 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: Sending an out of band ack of type  ackStatus 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: <*> , replyAck= $u 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Sending OOB to peer:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: Cannot send OOB response  ackStatus . Responder not running. 
WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: DirectoryScanner: shutdown has been called
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Waiting for threadgroup to exit, active threads is  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Shutdown complete.
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
TRACE org.apache.hadoop.hdfs.RemoteBlockReader2: Reading empty packet at end of read
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: readNextPacket: dataPlusChecksumLen =  dataPlusChecksumLen  headerLen =  $i 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log  latestLog  has no transactions.  moving it aside and looking for previous log 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JNStorage: Purging no-longer needed file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JNStorage: Purging no-longer needed file  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor: NameNode low on available disk space.  Entering safe mode. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: STATE* Safe mode is ON <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Got:  <*> 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* fsync:  src  for  clientName 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: persistBlocks:  path  with  <*>  blocks is persisted to  the file system 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling an incremental block report. 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling an incremental block report. 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> : scheduling a full block report. 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : got stale replica  <*> .  Removing  this replica from the replicaInfoMap and retrying. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to load  key 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : failed to get  ShortCircuitReplica. Cannot construct  BlockReaderLocal via  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct BlockReaderLocalLegacy 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderLocalLegacy: Cached location of block  blk  as  <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderLocalLegacy: New BlockReaderLocalLegacy for file  <*>  of size  <*>  startOffset  startOffset  length  length  short circuit checksum  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : not trying to create a  remote block reader because the UNIX domain socket at  <*>  is not usable. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.DFSClient$Renewer: Cancelling  <*> 
DEBUG org.apache.hadoop.hdfs.NameNodeProxies: Couldn\'t create proxy provider  failoverProxyProviderClass 
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to delete cancelled checkpoint in  sd 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Not going to trigger log rolls on active node because dfs.ha.log-roll.period is negative.
DEBUG org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: logRollPeriodMs= <*>  sleepTime= <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...\nCheckpointing active NN at  <*> \n Serving checkpoints at  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: who  calls recoverBlock( <*> , targets=[ <*> ] , newGenerationStamp= <*> ) 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block= <*> , (length= <*> ), syncList= syncList 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Connecting to datanode  <*>  addr= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block= <*> , (length= <*> ), syncList= syncList 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: mkdirs: created directory  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.startFile: added  src  inode  <*>   holder 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.startFile:  src   <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.DFSClient: src : masked= <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Set non-null progress callback on DFSOutputStream  src 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: modifyDirective of {} successfully applied {}.
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: Removing lazyPersist file  <*>  with no replicas. 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .removeLeaseWithPrefixPath: entry= entry 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: src  not found in lease.paths (= <*> ) 
ERROR org.apache.hadoop.hdfs.server.namenode.LeaseManager: lease  not found in sortedLeases 
ERROR org.apache.hadoop.hdfs.server.namenode.LeaseManager: lease  not found in sortedLeases 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* Namesystem.delete:  <*>  is removed 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: Removing lazyPersist file  <*>  with no replicas. 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.DFSClient: Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... 
WARN org.apache.hadoop.hdfs.DFSClient: Failed to renew lease for  <*>  for  <*>  seconds (>= hard-limit = L  seconds.)  Closing all files being written ... 
DEBUG org.apache.hadoop.hdfs.LeaseRenewer: Lease renewed for client  <*> 
DEBUG org.apache.hadoop.hdfs.LeaseRenewer: Lease renewer daemon for  <*>  with renew id  id  executed 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as:  <*> 
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
WARN org.apache.hadoop.hdfs.DFSUtil: dfs.https.enable is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* NameSystem.abandonBlock:  b of file  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NamenodeFsck: FSCK started by  <*>  from  <*>  for path  <*>  at  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Could not find a target for file  src  with favored node  favoredNode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Failed to choose with favored nodes (= favoredNodes ), disregard favored nodes hint and retry. 
WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: Failed to choose from local rack (location =  <*> ), retry with the rack of the next replica (location =  <*> ) 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: <*>   <*> 
WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is  expectedSize  but only  <*>  storage types can be selected  (replication= $i , selected= <*> , unavailable= unavailables , removed= $u , policy= this ) 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: storageTypes= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: <*>   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Rolling forward previously half-completed synchronization:  <*>  ->  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Purging old image  image 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger: Could not delete  file 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Adding replicas to map for block pool  <*>  on volume  <*> ... 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1: Time to add replicas to map for block pool  <*>  on volume  <*> :  timeTaken ms 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  starting to offer service 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Block Verification scan disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Periodic Directory Tree Verification scan is disabled because  reason#_ 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending heartbeat with  <*>  storage reports from service actor:  this 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  taking over ACTIVE state from  <*>  at higher txid= <*> 
ERROR org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Invalid BlockPoolId  <*>  in HeartbeatResponse. Expected  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block token params received from NN: for block pool  blockPoolId  keyUpdateInterval= <*>  min(s), tokenLifetime= <*>  min(s) 
INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  received versionRequest response:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: DatanodeCommand action : DNA_REGISTER from  <*>  with  <*>  state 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: this  beginning handshake with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Block pool  this  successfully registered with NN 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: <*> uccessfully sent block report x <*> ,  containing  <*>  storage report(s), of which we sent  i$ .  The reports had  totalBlockCount  total blocks and used  kvPair#  RPC(s). This took  <*>  msec to generate and  <*>  msecs for RPC and NN processing.  Got back  <*> . 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending cacheReport from service actor:  this 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Sending cacheReport from service actor:  this 
DEBUG org.apache.hadoop.hdfs.server.datanode.BPServiceActor: CacheReport of  <*>  block(s) took  <*>  msec to generate and  <*>  msecs for RPC and NN processing 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval  hours  hours for block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid= blockPoolId  to blockPoolScannerMap, new size= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: For namenode  <*>  using  DELETEREPORT_INTERVAL of  <*>  msec   BLOCKREPORT_INTERVAL of  <*> msec  CACHEREPORT_INTERVAL of  <*> msec  Initial delay:  <*> msec ; heartBeatInterval= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  trying to claim ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Acknowledging ACTIVE Namenode  actor 
INFO org.apache.hadoop.hdfs.server.datanode.BPOfferService: Namenode  actor  relinquishing ACTIVE state with  txid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Took  <*> ms to process  <*>  commands from NN 
WARN org.apache.hadoop.hdfs.server.datanode.BPServiceActor: Ending block pool service for:  this 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Removed  bpos 
WARN org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Couldn\'t remove BPOS  t  from bpByNameserviceId map 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: Error converting file to URI
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Marking all datandoes as stale
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.update: block  has only  curReplicas  replicas and needs  curExpectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  has already been removed from node  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: UnderReplicationBlocks.update  block  curReplicas  curReplicas  curExpectedReplicas  curExpectedReplicas  oldReplicas  oldReplicas  oldExpectedReplicas   oldExpectedReplicas  curPri   <*>  oldPri   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode metadata after re-processing replication and invalidation queues during failover:\n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: take(): poll() returned null, sleeping for {} ms
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): read no edits from the NN when requesting edits after txid {}
DEBUG org.apache.hadoop.hdfs.DFSInotifyEventInputStream: poll(): lastReadTxid is -, reading current txid from NN
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: removeCachePool of  poolName  successful. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Storage directory is in use.
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Added new volume:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume -  <*> , StorageType:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Start Decommissioning  node   storage  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Start Decommissioning  node   storage  with  <*>  blocks 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
LOGFINALIZEROLLINGUPGRADE org.apache.hadoop.hdfs.server.namenode.FSNamesystem: <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: block= block , bytesPerCRC= <*> , crcPerBlock= crcPerBlock , md= <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: demoteOldEvictable: demoting  <*> :  <*> :  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : trimEvictionMaps is purging  replica <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: srcNode  srcNode  is dead  when decommission is in progress. Continue to mark  it as decommission in progress. In that way, when it rejoins the  cluster it can continue the decommission process. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Decommission complete for  node 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Discard the EditLog files, the given start txid is  startTxId 
INFO org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : closing 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is stale because it\'s  stale#  ms old, and staleThresholdMs =  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: closed  this suffix 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : unref replica  replica :  purgeReason  refCount  <*>  ->  <*> <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: this  is not stale because it\'s only  stale#  ms old, and staleThresholdMs =  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: LazyWriter: Start persisting RamDisk block: block pool Id:  <*>  block id:  <*>  on target volume  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter schedule async task to persist RamDisk block pool id:  bpId  block id:  blockId 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: LazyWriter failed to create  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Cannot find BPOfferService for reporting block received for bpid= <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: Evicting block  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  blockFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Moved  metaFile  to  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reconfiguring  property  to  newVal 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Storage directory is loaded:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.add: block  has only  curReplicas  replicas and need  expectedReplicas  replicas so is added to neededReplications  at priority level  <*> 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: block  block :  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          =  nrInvalid 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks =  nrUnderReplicated 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks =  nrOverReplicated <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    =  nrUnderConstruction 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in  <*>  msec 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from  <*>  to  newEpoch  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastWriterEpoch from  <*>  to  <*>  for client  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Unable to start log segment  txid  at  <*> :  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.ImageServlet: ImageServlet rejecting:  remoteUser 
WARN org.apache.hadoop.hdfs.server.namenode.ImageServlet: Received non-NN/SNN/administrator request for image or edits from  <*>  at  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: concat  <*>  to  target 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Safe mode is OFF
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             =  $i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        =  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:\n <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Upgrading to sequential block IDs. Generation stamp for new blocks set to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Loading image file  curFile  using  stampAtIdSwitch 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Number of files =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Number of files under construction =  <*> 
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addCachePool of {} successful.
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addCachePool of {} successful.
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: Image file  curFile  of size  <*>  bytes loaded in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Saving image file  newFile  using  compression 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver: Image file  newFile  of size  <*>  bytes saved in  <*>  seconds. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Deleting  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Failed to delete image file:  $u 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : no current segment in place 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*>  from  url 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment  <*> :  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer: Invalid clusterId in journal request - expected  <*>  actual  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: data: <*> 
TRACE org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: op= <*> , startOpt= startOpt , numEdits= numEdits , totalEdits= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Upgrade process renamed reserved path  oldPath  to  path 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormat: Renamed root path .reserved to  renameString 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: Adjusting block totals from  <*> / <*>  to  <*> / <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for  blk  blockMap has  <*>  but corrupt replicas map has  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Processing previouly queued message  rbi 
INFO org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: replaying edit log:  deltaTxId / <*>  transactions completed. ( <*> %) 
TRACE org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: replaying edit log finished
APPEND org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Summary of operations loaded from edit log:\n  
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: addDirective of {} successful.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because it does not exist 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: <*> . Note: This is normal during a rolling upgrade. 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Exporting access keys
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode: from  nodeReg  storage  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode:  <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  is removed from excessBlocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* removeStoredBlock:  block  from  node 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: remove datanode  nodeInfo 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*> .wipeDatanode( node ): storage  <*>  is removed from datanodeMap. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: BLOCK* registerDatanode: node restarted.
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The resolve call returned null!
<INIT> org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Unresolved topology mapping for host  <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The dependency call returned null for host  <*> 
ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The dependency call returned null for host  <*> 
REMOVE org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: <*>
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block:  block , Expected Replicas:  <*> , live replicas:  <*> , corrupt replicas:  <*> , decommissioned replicas:  <*> , excess replicas:  <*> , Is Open File:  <*> , Datanodes having this block:  <*> , Current Datanode:  srcNode , Is current datanode decommissioning:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Rollback of  <*>  is complete. 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
WARN org.apache.hadoop.hdfs.util.MD5FileUtils: deleting   <*>  FAILED 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain  <*>  images with txid >=  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
<INIT> org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: Same delegation token being added twice; invalid entry in fsimage or editlogs
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
WARN org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager: No KEY found for persisted identifier  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader: Loaded FSImage in  <*>  seconds. 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: DIR* NameSystem.delete:  src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: DIR* FSDirectory.unprotectedDelete: failed to remove  src  because the root is not allowed to be deleted 
DEBUG org.apache.hadoop.hdfs.server.namenode.LeaseManager: <*> .findLease: prefix= prefix 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: BLOCK*  <*> : add  block  to  datanode 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  priLevel 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* addToInvalidates:  b   <*> 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block  block  from priority queue  i 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Adjusting safe-mode totals for deletion.decreasing safeBlocks by  numRemovedSafe , totalBlocks by  numRemovedComplete 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: Adjusting block totals from  <*> / <*>  to  <*> / <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refresh request received for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Starting BPOfferServices for nameservices:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refreshing list of NNs for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextDomainPeer: reusing existing peer  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: Sending receipt verification byte for slot  slot 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : error creating ShortCircuitReplica. 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: short-circuit read access is disabled for DataNode  <*> .  reason:  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : <*> 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : I/O error requesting file descriptors.   Disabling domain socket  <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.web.TokenAspect: Created new DT for  <*> 
TRACE org.apache.hadoop.hdfs.web.WebHdfsFileSystem: url= <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
WARN org.apache.hadoop.hdfs.DFSClient$Conf: Bad checksum type:  <*> . Using default  CRCC 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.use.legacy.blockreader.local =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.read.shortcircuit =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.client.domain.socket.data.traffic =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient$Conf: dfs.domain.socket.path =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.PeerCache: SocketCache disabled.
DEBUG org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: Both short-circuit local reads and UNIX domain socket are disabled.
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: startFile: recover  <*> , src= src  client  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering  lease , src= src 
WARN org.apache.hadoop.hdfs.server.namenode.LeaseManager: Removing non-existent lease! holder= holder  src= src 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: closeFile:  path  with  <*>  blocks is persisted to the file system 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
TRACE org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processOverReplicatedBlock: Postponing  block  since storage  storage  does not yet have up-to-date information. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* internalReleaseLease: Removed empty last block and closed file.
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
ERROR org.apache.hadoop.hdfs.NameNodeProxies: Unsupported protocol found when creating the proxy connection to NameNode:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: currently disabled dir  <*> ; type= <*> ;canwrite= <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNStorage: restoring dir  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous finalize for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage: Failed to delete temporary edits file:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Found KeyProvider:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Caching file names occuring more than  <*>  times 
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: Using minimum value {} for {}
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: The threshold value should\'t be greater than , threshold:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.threshold-pct =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.min.datanodes =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: dfs.namenode.safemode.extension     =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled?  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr:  <*> <*> 
WARN org.apache.hadoop.hdfs.server.namenode.CheckpointConf: Configuration key  key  is deprecated! Ignoring...  Instead please specify a value for  dfs.namenode.checkpoint.txns 
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for  name  at:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
WARN org.apache.hadoop.hdfs.DFSUtil: hadoop.ssl.enabled is deprecated. Please use dfs.http.policy.
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   : <*>  secs  ( <*>  min) 
INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    : <*>  txns 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: short-circuit read access is disabled for DataNode  <*> .  reason:  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: this : unknown response code  <*>  while attempting to set up short-circuit access.  <*> 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : unregisterSlot  slotIdx 
TRACE org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: this : freeing empty stale  shm 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: this : freed 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this : closing stale domain peer  peer 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Waiting for ack for:  seqno 
WARN org.apache.hadoop.hdfs.DFSOutputStream: Slow waitForAckedSeqno took  <*> ms (threshold= <*> ms) 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage  len$  from DataNode this 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: storageInfo  failed. 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for block  <*>  in pipeline  $u : bad datanode  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: nodes are empty for write pipeline of block  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: pipeline =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: pipeline =  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Closing old block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error Recovery for  <*>  waiting for responder to exit.  
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Allocating new block
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Abandoning  <*> 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Excluding datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: DataStreamer block  <*>  sending packet  e_ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Refresh request received for nameservices:  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolManager: Starting BPOfferServices for nameservices:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool  bpid 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to scan all replicas for block pool  bpid :  arr$# ms 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: Total time to add all replicas to map:  arr$# ms 
TRACE org.apache.hadoop.hdfs.server.namenode.CacheManager: Validating directive {} pool maxRelativeExpiryTime {}
INFO org.apache.hadoop.hdfs.server.namenode.CacheManager: modifyDirective of {} successfully applied {}.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: <*> <*> . Throwing a BlockMissingException 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to construct a BlockReaderLocal  for short-circuit reads. 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
DEBUG org.apache.hadoop.hdfs.BlockReaderFactory: this :  <*>  is not  usable for short circuit; giving up on BlockReaderLocal. 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : trying to create a remote block reader from a  TCP socket 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: nextTcpPeer: failed to create newConnectedPeer connected to  <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
WARN org.apache.hadoop.hdfs.BlockReaderFactory: I/O error constructing remote block reader.
TRACE org.apache.hadoop.hdfs.DFSClient: Address  targetAddr <*> 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to get  key 
TRACE org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : loading  key 
WARN org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: this : failed to load  key 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : got security exception while constructing  a remote block reader from the unix domain socket at  <*> 
TRACE org.apache.hadoop.hdfs.BlockReaderFactory: this : returning new remote block reader using  UNIX domain socket on  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
INFO org.apache.hadoop.hdfs.DFSInputStream: Access token was invalid when connecting to  targetAddr  :  ex 
WARN org.apache.hadoop.hdfs.DFSInputStream: Failed to connect to  <*>  for block , add to deadNodes and continue.  <*> 
WARN org.apache.hadoop.hdfs.DFSInputStream: Found Checksum error for  <*>  from  <*>  at  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Successfully connected to  <*>  for  <*> 
INFO org.apache.hadoop.hdfs.DFSInputStream: Will fetch a new encryption key and retry, encryption key was invalid when connecting to  <*>  :  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
DEBUG org.apache.hadoop.hdfs.DFSClient: Clearing encryption key
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Leaving safe mode after  <*>  secs 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* Network topology has  <*>  racks and  <*>  datanodes 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: STATE* UnderReplicatedBlocks has  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: set restore failed storage to  val 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Enabling async auditlog
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Logj is required to enable async auditlog
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to  <*>  since  dfs.namenode.stale.datanode.interval  is less than  dfs.namenode.heartbeat.recheck-interval 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check= <*> 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  <*> , which is less than    heartbeat intervals. This may cause too frequent changes of  stale states of DataNodes since a heartbeat msg may be missing  due to temporary short-term failures. Reset stale interval to  <*> . 
WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: The given interval for marking stale datanode =  staleInterval , which is larger than heartbeat expire interval  heartbeatExpireInterval . 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: dfs.namenode.startup.delay.block.deletion.sec is set to  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: The block deletion will start around  <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable= <*> 
INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval= <*>  min(s),  dfs.block.access.token.lifetime = <*>  min(s),  dfs.encrypt.data.transfer.algorithm = <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled =  <*> 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
WARN org.apache.hadoop.hdfs.DFSUtil: Namenode for  nsId  remains unresolved for ID  nnId .  Check your hdfs-site.xml file to  ensure namenodes are configured properly. 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured \ propertyName \ in hdfs-site.xml; \n\t\t- use Backup Node as a persistent and up-to-date storage  of the file system meta-data. 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: No class configured for  uriScheme ,  <*>  is empty 
WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Encountered exception during format: 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reconfiguring  property  to  newVal 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
WARN org.apache.hadoop.hdfs.server.common.Util: Path  s  should be specified as a URI  in configuration files. Please update hdfs configuration. 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Storage directory is loaded:  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: No files in  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Edit log file  <*>  appears to be empty.  Moving it aside... 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Synchronizing log  <*> : old segment  <*>  is not the right length 
DEBUG org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Start moving  this 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: Successfully moved  this 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from  <*>  to  volFailures 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of storages reported in heartbeat= <*> ; Number of storages in storageMap= <*> 
DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity= <*> , max load factor=  maxLoadFactor , min load factor=  minLoadFactor 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Deferring removal of stale storage  len$  with  <*>  blocks 
DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes =  <*> , excludedNodes =  <*> , foundNodes =  <*> , nodes =  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: Initializing journal in directory  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*> is not a directory 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage  <*> 
INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is  latestLog 
WARN org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Unable to start log segment  txid  at  <*> :  <*> 
DEBUG org.apache.hadoop.hdfs.server.common.Storage: Failed to preserve last modified date from\' srcFile \' to \' destFile \' 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcMeta  to  <*>  and calculated checksum 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Copied  srcFile  to  <*> 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Storage directory  <*>  does not exist 
WARN org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot access storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataStorage: Storage directory  dataDir  is not formatted for  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Formatting ...
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.DataStorage: Generated new storageID  <*>  for directory  <*> <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Rolling back storage directory  <*> .\n   target LV =  <*> ; target CTime =  <*> 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
ERROR org.apache.hadoop.hdfs.server.datanode.DataStorage: There are  <*>  duplicate block  entries within the same volume. 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Unexpectedly short length on  <*> . 
ERROR org.apache.hadoop.hdfs.server.datanode.DataStorage: There are  <*>  duplicate block  entries within the same volume. 
WARN org.apache.hadoop.hdfs.server.datanode.DataStorage: Unexpectedly low genstamp on  <*> . 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrade of block pool  <*>  at  <*>  is complete 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Locking is disabled for  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Completing previous checkpoint for storage directory  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Not overwriting  $u  with smaller file from  trash directory. This message can be safely ignored. 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Restored  <*>  block files from trash. 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage: Upgrading block pool storage directory  <*> .\n   old LV =  <*> ; old CTime =  <*> .\n   new LV =  <*> ; new CTime =  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: <*>  does not exist. Creating ... 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Lock on  <*>  acquired by nodename  <*> 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Cannot lock storage  <*> . The directory is already locked 
ERROR org.apache.hadoop.hdfs.server.common.Storage: *********** Upgrade is not supported from this  older version  oldVersion  of storage to the current version.  Please upgrade to  Hadoop-.  or a later version and then upgrade to current  version. Old layout version is  <*>  and latest layout version this software version can  upgrade from is  $i . ************ 
INFO org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory: Recovering storage directory  <*>  from previous rollback 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to move  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to move  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to mkdirs  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to mkdirs  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Recovered  <*>  replicas from  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete block file  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica: Failed to delete meta file  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: resolveDuplicateReplicas decide to keep  replicaToKeep .  Will try to delete  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil: Block  blockFile  does not have a metafile! 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Failed to delete restart meta file:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: computePacketChunkSize: src= <*> , chunkSize= chunkSize , chunksPerPacket= <*> , packetSize= <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Queued packet  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: Connecting to datanode  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: lastAckedSeqno =  <*> 
DEBUG org.apache.hadoop.hdfs.DFSClient: Using local interface  addr 
DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}
TRACE org.apache.hadoop.hdfs.protocol.datatransfer.Sender: Sending DataTransferOp  <*> :  proto 
INFO org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: nodes are empty for write pipeline of block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Error recovering pipeline for writing  <*> . Already retried  times for the same packet. 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Append to block  <*> 
WARN org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: Datanode did not restart in time:  <*> 
DEBUG org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer: DataStreamer block  <*>  sending packet  e_ 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Resolved path is  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: ACCESS CHECK:  this , doCheckOwner= doCheckOwner , ancestorAccess= ancestorAccess , parentAccess= parentAccess , access= access , subAccess= subAccess , ignoreEmptyDir= ignoreEmptyDir , resolveLink= resolveLink 
DEBUG org.apache.hadoop.hdfs.server.namenode.INodesInPath: UnresolvedPathException  path:  <*>  preceding:  <*>  count:  count  link:  <*>  target:  <*>  remainder:  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
DEBUG org.apache.hadoop.hdfs.server.datanode.DataXceiver: Checking block access token for block \' <*> \' with mode \' mode \' 
DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Checking access for user= userId , block= block , access mode= mode  using  <*> 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: Bumping up the client provided block\'s genstamp to latest  <*>  for block  block 
DEBUG org.apache.hadoop.hdfs.server.datanode.BlockSender: block= block , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader: Unexpected meta-file version for  name : version in file is  $i  but expected version is   
WARN org.apache.hadoop.hdfs.server.datanode.BlockSender: <*> :sendBlock() :  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.DataXceiver: Copied  block  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Unable to roll forward using only logs. Downloading image with txid  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD  digestString  to  <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.util.AtomicFileOutputStream: Unable to delete tmp file  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming   <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.NNStorage: Error reported on storage directory  sd 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: current list of storage dirs: <*> 
WARN org.apache.hadoop.hdfs.server.namenode.NNStorage: About to remove corresponding storage:  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NNStorage: at the end current list of storage dirs: <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open AuthenticatedURL connection url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Overwriting existing file  f  with file downloaded from  url 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Unable to rename edits file from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file:  f 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to  <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file  <*>  size  <*>  bytes. 
DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Renaming  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Namespace quota violation in image for  <*>  quota =  <*>  < consumed =  child# 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading  editIn  expecting start txid # <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Connection closed by client
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.JournalSet: Skipping jas  jas  since it\'s disabled 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Disabling journal  j 
ERROR org.apache.hadoop.hdfs.server.namenode.JournalSet: Error:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
CLOSEALLSTREAMS org.apache.hadoop.hdfs.server.namenode.BackupImage:  <*> <*>
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Successfully synced BackupNode with NameNode at txnid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.BackupImage: State transition  <*>  ->  newState 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Loading edits into backupnode to try to catch up from txid  <*>  to  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Going to finish converging with remaining  editStreamsAll  txns from in-progress stream  stream 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
INFO org.apache.hadoop.hdfs.server.namenode.BackupImage: BackupNode namespace frozen.
DEBUG org.apache.hadoop.hdfs.server.namenode.Checkpointer: Doing checkpoint. Last applied:  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Loading image with txid  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from  file 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from  <*>  stream(s). 
DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:\n   <*> 
WARN org.apache.hadoop.hdfs.server.namenode.FSDirectory: Could not get full path. Corresponding file might have deleted already.
ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: BUG: Diskspace quota violation in image for  <*>  quota =  <*>  < consumed =  <*> 
INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with  <*>  entries  <*>  lookups 
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo: msg  \n <*> 
DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to  <*>  milliseconds 
WARN org.apache.hadoop.hdfs.server.namenode.TransferFsImage: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid  txid  to namenode at  fsName  in  <*>  seconds 
INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpoint completed in  <*>  seconds.  New Image Size:  <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery:  block , recoveryId= recoveryId , replica= <*> 
WARN org.apache.hadoop.hdfs.server.datanode.BlockReceiver: <*> \n <*> 
INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for  block  from  <*>  to  <*> 
DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to  <*> 